diff --git a/stblinux-2.6.31/Documentation/immediate.txt b/stblinux-2.6.31/Documentation/immediate.txt
new file mode 100644
index 0000000..281e073
--- /dev/null
+++ b/stblinux-2.6.31/Documentation/immediate.txt
@@ -0,0 +1,221 @@
+		        Using the Immediate Values
+
+			    Mathieu Desnoyers
+
+
+This document introduces Immediate Values and their use.
+
+
+* Purpose of immediate values
+
+An immediate value is used to compile into the kernel variables that sit within
+the instruction stream. They are meant to be rarely updated but read often.
+Using immediate values for these variables will save cache lines.
+
+This infrastructure is specialized in supporting dynamic patching of the values
+in the instruction stream when multiple CPUs are running without disturbing the
+normal system behavior.
+
+Compiling code meant to be rarely enabled at runtime can be done using
+if (unlikely(imv_read(var))) as condition surrounding the code. The
+smallest data type required for the test (an 8 bits char) is preferred, since
+some architectures, such as powerpc, only allow up to 16 bits immediate values.
+
+
+* Usage
+
+In order to use the "immediate" macros, you should include linux/immediate.h.
+
+#include <linux/immediate.h>
+
+DEFINE_IMV(char, this_immediate);
+EXPORT_IMV_SYMBOL(this_immediate);
+
+
+And use, in the body of a function:
+
+Use imv_set(this_immediate) to set the immediate value.
+
+Use imv_read(this_immediate) to read the immediate value.
+
+The immediate mechanism supports inserting multiple instances of the same
+immediate. Immediate values can be put in inline functions, inlined static
+functions, and unrolled loops.
+
+If you have to read the immediate values from a function declared as __exit, you
+should explicitly use _imv_read(), which will fall back on a global variable
+read. Failing to do so will leave a reference to the __exit section in kernel
+without module unload support. imv_read() in the __init section is supported.
+
+You can choose to set an initial static value to the immediate by using, for
+instance:
+
+DEFINE_IMV(long, myptr) = 10;
+
+
+* Optimization for a given architecture
+
+One can implement optimized immediate values for a given architecture by
+replacing asm-$ARCH/immediate.h.
+
+
+* Performance improvement
+
+
+  * Memory hit for a data-based branch
+
+Here are the results on a 3GHz Pentium 4:
+
+number of tests: 100
+number of branches per test: 100000
+memory hit cycles per iteration (mean): 636.611
+L1 cache hit cycles per iteration (mean): 89.6413
+instruction stream based test, cycles per iteration (mean): 85.3438
+Just getting the pointer from a modulo on a pseudo-random value, doing
+  nothing with it, cycles per iteration (mean): 77.5044
+
+So:
+Base case:                      77.50 cycles
+instruction stream based test:  +7.8394 cycles
+L1 cache hit based test:        +12.1369 cycles
+Memory load based test:         +559.1066 cycles
+
+So let's say we have a ping flood coming at
+(14014 packets transmitted, 14014 received, 0% packet loss, time 1826ms)
+7674 packets per second. If we put 2 markers for irq entry/exit, it
+brings us to 15348 markers sites executed per second.
+
+(15348 exec/s) * (559 cycles/exec) / (3G cycles/s) = 0.0029
+We therefore have a 0.29% slowdown just on this case.
+
+Compared to this, the instruction stream based test will cause a
+slowdown of:
+
+(15348 exec/s) * (7.84 cycles/exec) / (3G cycles/s) = 0.00004
+For a 0.004% slowdown.
+
+If we plan to use this for memory allocation, spinlock, and all sorts of
+very high event rate tracing, we can assume it will execute 10 to 100
+times more sites per second, which brings us to 0.4% slowdown with the
+instruction stream based test compared to 29% slowdown with the memory
+load based test on a system with high memory pressure.
+
+
+
+  * Markers impact under heavy memory load
+
+Running a kernel with my LTTng instrumentation set, in a test that
+generates memory pressure (from userspace) by trashing L1 and L2 caches
+between calls to getppid() (note: syscall_trace is active and calls
+a marker upon syscall entry and syscall exit; markers are disarmed).
+This test is done in user-space, so there are some delays due to IRQs
+coming and to the scheduler. (UP 2.6.22-rc6-mm1 kernel, task with -20
+nice level)
+
+My first set of results: Linear cache trashing, turned out not to be
+very interesting, because it seems like the linearity of the memset on a
+full array is somehow detected and it does not "really" trash the
+caches.
+
+Now the most interesting result: Random walk L1 and L2 trashing
+surrounding a getppid() call.
+
+- Markers compiled out (but syscall_trace execution forced)
+number of tests: 10000
+No memory pressure
+Reading timestamps takes 108.033 cycles
+getppid: 1681.4 cycles
+With memory pressure
+Reading timestamps takes 102.938 cycles
+getppid: 15691.6 cycles
+
+
+- With the immediate values based markers:
+number of tests: 10000
+No memory pressure
+Reading timestamps takes 108.006 cycles
+getppid: 1681.84 cycles
+With memory pressure
+Reading timestamps takes 100.291 cycles
+getppid: 11793 cycles
+
+
+- With global variables based markers:
+number of tests: 10000
+No memory pressure
+Reading timestamps takes 107.999 cycles
+getppid: 1669.06 cycles
+With memory pressure
+Reading timestamps takes 102.839 cycles
+getppid: 12535 cycles
+
+The result is quite interesting in that the kernel is slower without
+markers than with markers. I explain it by the fact that the data
+accessed is not laid out in the same manner in the cache lines when the
+markers are compiled in or out. It seems that it aligns the function's
+data better to compile-in the markers in this case.
+
+But since the interesting comparison is between the immediate values and
+global variables based markers, and because they share the same memory
+layout, except for the movl being replaced by a movz, we see that the
+global variable based markers (2 markers) adds 742 cycles to each system
+call (syscall entry and exit are traced and memory locations for both
+global variables lie on the same cache line).
+
+
+- Test redone with less iterations, but with error estimates
+
+10 runs of 100 iterations each: Tests done on a 3GHz P4. Here I run getppid with
+syscall trace inactive, comparing the case with memory pressure and without
+memory pressure. (sorry, my system is not setup to execute syscall_trace this
+time, but it will make the point anyway).
+
+No memory pressure
+Reading timestamps:     150.92 cycles,     std dev.    1.01 cycles
+getppid:               1462.09 cycles,     std dev.   18.87 cycles
+
+With memory pressure
+Reading timestamps:     578.22 cycles,     std dev.  269.51 cycles
+getppid:              17113.33 cycles,     std dev. 1655.92 cycles
+
+
+Now for memory read timing: (10 runs, branches per test: 100000)
+Memory read based branch:
+                       644.09 cycles,      std dev.   11.39 cycles
+L1 cache hit based branch:
+                        88.16 cycles,      std dev.    1.35 cycles
+
+
+So, now that we have the raw results, let's calculate:
+
+Memory read:
+644.09 +/- 11.39 - 88.16 +/- 1.35 = 555.93 +/- 11.46 cycles
+
+Getppid without memory pressure:
+1462.09 +/- 18.87 - 150.92 +/- 1.01 = 1311.17 +/- 18.90 cycles
+
+Getppid with memory pressure:
+17113.33 +/- 1655.92 - 578.22 +/- 269.51 = 16535.11 +/- 1677.71 cycles
+
+Therefore, if we add 2 markers not based on immediate values to the getppid
+code, which would add 2 memory reads, we would add
+2 * 555.93 +/- 12.74 = 1111.86 +/- 25.48 cycles
+
+Therefore,
+
+1111.86 +/- 25.48 / 16535.11 +/- 1677.71 = 0.0672
+ relative error: sqrt(((25.48/1111.86)^2)+((1677.71/16535.11)^2))
+                     = 0.1040
+ absolute error: 0.1040 * 0.0672 = 0.0070
+
+Therefore: 0.0672 +/- 0.0070 * 100% = 6.72 +/- 0.70 %
+
+We can therefore affirm that adding 2 markers to getppid, on a system with high
+memory pressure, would have a performance hit of at least 6.0% on the system
+call time, all within the uncertainty limits of these tests. The same applies to
+other kernel code paths. The smaller those code paths are, the highest the
+impact ratio will be.
+
+Therefore, not only is it interesting to use the immediate values to dynamically
+activate dormant code such as the markers, but I think it should also be
+considered as a replacement for many of the "read-mostly" static variables.
diff --git a/stblinux-2.6.31/Documentation/ioctl/ioctl-number.txt b/stblinux-2.6.31/Documentation/ioctl/ioctl-number.txt
index dbea4f9..cd1c998 100644
--- a/stblinux-2.6.31/Documentation/ioctl/ioctl-number.txt
+++ b/stblinux-2.6.31/Documentation/ioctl/ioctl-number.txt
@@ -204,3 +204,5 @@ Code	Seq#	Include File		Comments
 					<mailto:thomas@winischhofer.net>
 0xF4	00-1F	video/mbxfb.h		mbxfb
 					<mailto:raph@8d.com>
+0xF5	00-3F	linux/ltt-tracer.h	LTTng
+					<mailto:mathieu.desnoyers@polymtl.ca>
diff --git a/stblinux-2.6.31/Documentation/kernel-parameters.txt b/stblinux-2.6.31/Documentation/kernel-parameters.txt
index 3d5a958..f4750b2 100644
--- a/stblinux-2.6.31/Documentation/kernel-parameters.txt
+++ b/stblinux-2.6.31/Documentation/kernel-parameters.txt
@@ -777,6 +777,10 @@ and is between 256 and 4096 characters. It is defined in the file
 			by the set_ftrace_notrace file in the debugfs
 			tracing directory.
 
+	force_tsc_sync=1
+			Force TSC resynchronization when SMP CPUs go online.
+			See also idle=poll and disable frequency scaling.
+
 	gamecon.map[2|3]=
 			[HW,JOY] Multisystem joystick and NES/SNES/PSX pad
 			support via parallel port (up to 5 devices per port)
diff --git a/stblinux-2.6.31/Documentation/lttng.txt b/stblinux-2.6.31/Documentation/lttng.txt
new file mode 100644
index 0000000..364ac92
--- /dev/null
+++ b/stblinux-2.6.31/Documentation/lttng.txt
@@ -0,0 +1,35 @@
+
+Linux Text Toolkit Next Generation
+Mathieu Desnoyers, July 2009
+
+This guide contains quickstart information on how to to use LTTng. It currently
+only contains information about ascii text dump, but (TODO) we should bring the
+LTTng manual here.
+
+ * Quick and dirty startup ascii output script:
+
+#!/bin/sh
+
+LTT_DIR=/mnt/debugfs/ltt
+ltt-armall
+echo trace > ${LTT_DIR}/setup_trace
+cd control/
+cd trace/
+echo relay > ${LTT_DIR}/control/trace/trans
+cd channel/
+for a in ${LTT_DIR}/control/trace/channel/*/switch_timer;
+	do echo 100 > $a; done
+#note: error for metadata channel for the following line is OK
+#it is not permitted to set metadata channel in overwrite mode
+for a in ${LTT_DIR}/control/trace/channel/*/overwrite;
+	do echo 1 > $a; done
+for a in ${LTT_DIR}/control/trace/channel/*/enable;
+	do echo 1 > $a; done
+cd ..
+echo 1 > ${LTT_DIR}/control/trace/alloc
+echo 1 > ${LTT_DIR}/control/trace/enabled
+
+To see the events for a given channel (e.g. kernel events):
+
+cat ${LTT_DIR}/ascii/trace/kernel
+(hit CTRL-C to stop)
diff --git a/stblinux-2.6.31/Documentation/markers.txt b/stblinux-2.6.31/Documentation/markers.txt
index d2b3d0e..e25df7c 100644
--- a/stblinux-2.6.31/Documentation/markers.txt
+++ b/stblinux-2.6.31/Documentation/markers.txt
@@ -15,10 +15,12 @@ provide at runtime. A marker can be "on" (a probe is connected to it) or "off"
 (no probe is attached). When a marker is "off" it has no effect, except for
 adding a tiny time penalty (checking a condition for a branch) and space
 penalty (adding a few bytes for the function call at the end of the
-instrumented function and adds a data structure in a separate section).  When a
-marker is "on", the function you provide is called each time the marker is
-executed, in the execution context of the caller. When the function provided
-ends its execution, it returns to the caller (continuing from the marker site).
+instrumented function and adds a data structure in a separate section). The
+immediate values are used to minimize the impact on data cache, encoding the
+condition in the instruction stream. When a marker is "on", the function you
+provide is called each time the marker is executed, in the execution context of
+the caller. When the function provided ends its execution, it returns to the
+caller (continuing from the marker site).
 
 You can put markers at important locations in the code. Markers are
 lightweight hooks that can pass an arbitrary number of parameters,
@@ -81,15 +83,22 @@ with tracepoint probes in a scheme like this :
 
 void probe_tracepoint_name(unsigned int arg1, struct task_struct *tsk);
 
-DEFINE_MARKER_TP(marker_eventname, tracepoint_name, probe_tracepoint_name,
-	"arg1 %u pid %d");
+DEFINE_MARKER_TP(marker_channel, marker_eventname, tracepoint_name,
+	probe_tracepoint_name, "arg1 %u pid %d");
 
 notrace void probe_tracepoint_name(unsigned int arg1, struct task_struct *tsk)
 {
-	struct marker *marker = &GET_MARKER(kernel_irq_entry);
+	struct marker *marker = &GET_MARKER(marker_channel, marker_eventname);
 	/* write data to trace buffers ... */
 }
 
+* Optimization for a given architecture
+
+To force use of a non-optimized version of the markers, _trace_mark() should be
+used. It takes the same parameters as the normal markers, but it does not use
+the immediate values based on code patching.
+
+
 * Probe / marker example
 
 See the example provided in samples/markers/src
diff --git a/stblinux-2.6.31/Documentation/psrwlock.txt b/stblinux-2.6.31/Documentation/psrwlock.txt
new file mode 100644
index 0000000..5309aeb
--- /dev/null
+++ b/stblinux-2.6.31/Documentation/psrwlock.txt
@@ -0,0 +1,440 @@
+                 Priority Sifting Reader-Writer Locks
+                     Design and Performance tests
+                       Mathieu Desnoyers, 2008
+
+
+****** Design Goal ******
+
+The main design goal is to lessen the rwlock impact on the irq and softirq
+latency of the system.
+
+A typical case leading to long interrupt latencies :
+
+- rwlock shared between
+  - Rare update in thread context
+  - Frequent slow read in thread context (task list iteration)
+  - Fast interrupt handler read
+
+The slow write must therefore disable interrupts around the write lock, but will
+therefore add up to the global interrupt latency; worse case being the duration
+of the slow read.
+
+
+****** Description of the psrwlock algorithm ******
+
+The writer fast path uses a single bit to indicate that a fast path writer is
+active (UC_WRITER). The uncontended case is done by upgrading the priority to
+the priority of the highest priority reader (e.g. by disabling interrupts) and
+by doing a cmpxchg which sets the UC_WRITER bit atomically only if there is no
+other writer nor reader in their critical section. If there is contention caused
+by either a reader or a writer, the writer falls in the slow path.
+
+The writer slow path first sets the UC_SLOW_WRITER bit and increments the WS
+(writers in slow path) counter (the two operations are made atomic by using the
+WS_COUNT_MUTEX bit as a mutex) and then subscribes to the preemptable lock,
+which locks out the preemptable reader threads. It waits for all the preemptable
+reader threads in the slow path to exit their critical section. It then waits
+for all the "in-flight" fast path readers to exit their critical section. Then,
+it upgrades its priority by disabling preemption and does the same
+(subscription, wait for slow path readers, wait for fast path readers) for
+preemptable readers. The same is then done for bottom halves and interrupt
+contexts. One all the reader contexts has been excluded, the writer takes the
+slow path mutex, accesses the data structure.
+
+In its unlock, the writer detects if it was a fast or slow path writer by
+checking the UC_WRITER bit. A fast path writer has to clear the UC_WRITER bit
+and bring its priority back to its original state (e.g. reenabling interrupts).
+The slow path writer must unlock the mutex, lower its priority stage by stage
+(reenabling interrupt, bh, preemption). At each stage, it unsubscribes from the
+specific context. Then, it checks if it is the last writer in the slow path by
+decrementing and testing the WS counter (writers in slow path) bit. If it is the
+last writer, it clears the UC_SLOW_WRITER bit (count and bit are made atomic by
+the WS_COUNT_MUTEX bit used as a mutex).
+
+The reader does an atomic cmpxchg to check if there is any contention on the
+lock (other readers or writers in their critical section). If not, it increments
+the reader count. If there are other active readers, the first cmpxchg will
+fail, but a second cmpxchg will be taken at the beginning of the slow path if
+there are no writers contending the lock. A cmpxchg is used to take the reader
+lock instead of a simple addition because we cannot afford to take the read
+fast-path lock, even for a short period of time, while we are in a lower
+execution context while there is a writer in an higher priority execution
+context waiting for the lock. Failure to do so would result in priority
+inversion.
+
+The reader slow path waits for the slow path writer subscription count in its
+particular context to become 0. When it does, the reader atomically increments
+the reader count for this context. Then, it waits for all the fast path writers
+to exit their critical section and increments the fast path reader count. Before
+returning from the reader lock primitive, it decrements the slow path reader
+count for the context it subscribed to, behaving exactly as a fast path reader.
+
+The unlock primitive is then exactly the same for the fast and slow path
+readers: They only have to decrement the fastpath reader count.
+
+WS_WQ_MUTEX protects the waitqueue and UC_WQ_ACTIVE changes. WS_WQ_MUTEX must be
+taken with interrupts off. The ordering of operations dealing with preemptable
+threads involves that any code path which can cause a thread to be added to the
+wait queue must end with a check for UC_WQ_ACTIVE flag which leads to a wake up
+if there are pending threads in the wait queue. Also, any point where a thread
+can be woken up from the wait queue must be followed by a UC_WQ_ACTIVE check.
+Given that the UC_WQ_ACTIVE flag is tested without taking the WS_WQ_MUTEX, we
+must make sure that threads added to the waitqueue first set the UC_WQ_ACTIVE
+flag and then re-test for the condition which led them to be put to sleep.
+
+Upon unlock, the following sequence is done :
+- atomically unlock and return the lock value, which contains the UC_WQ_ACTIVE
+  bit status at the moment the unlock is done.
+- if UC_WQ_ACTIVE is set :
+  - take WS_WQ_MUTEX
+  - wake a thread
+  - release WS_WQ_MUTEX
+
+When a thread is ready to be added to the wait queue :
+- the last busy-looping iteration fails.
+- take the WS_WQ_MUTEX
+- set the UC_WQ_ACTIVE bit if the list about to pass from inactive to active.
+- check again for the failed condition, since its status may have changed
+  since the busy-loop failed. If the condition now succeeds, return to
+  busy-looping after putting the UC_WQ_ACTIVE bit back to its original state and
+  releasing the WS_WQ_MUTEX.
+- add the current thread to the wait queue, change state.
+- release WS_WQ_MUTEX.
+
+Upon wakeup :
+- take WS_WQ_MUTEX
+- set state to running, remove from the wait queue.
+- clear UC_WQ_ACTIVE if the list passed to inactive.
+- release WS_WQ_MUTEX.
+
+A sequence similar to the unlock must be done when a trylock fails.
+
+
+****** Performance tests ******
+
+The test module is available at :
+
+http://ltt.polymtl.ca/svn/trunk/tests/kernel/test-psrwlock.c
+
+Dual quad-core Xeon 2.0GHz E5405
+
+
+**** Latency ****
+
+This section presents the detailed breakdown of latency preemption, softirq and
+interrupt latency generated by the psrwlock. In the "High contention" section,
+is compares the "irqoff latency tracer" results between standard Linux kernel
+rwlocks and the psrwlocks (tests done on wbias-rwlock v8).
+
+get_cycles takes [min,avg,max] 72,75,78 cycles, results calibrated on avg
+
+** Single writer test, no contention **
+SINGLE_WRITER_TEST_DURATION 10s
+
+IRQ latency for cpu 6 disabled 99490 times, [min,avg,max] 471,485,1527 cycles
+SoftIRQ latency for cpu 6 disabled 99490 times, [min,avg,max] 693,704,3969 cycles
+Preemption latency for cpu 6 disabled 99490 times, [min,avg,max] 909,917,4593 cycles
+
+
+** Single trylock writer test, no contention **
+SINGLE_WRITER_TEST_DURATION 10s
+
+IRQ latency for cpu 2 disabled 10036 times, [min,avg,max] 393,396,849 cycles
+SoftIRQ latency for cpu 2 disabled 10036 times, [min,avg,max] 609,614,1317 cycles
+Preemption latency for cpu 2 disabled 10036 times, [min,avg,max] 825,826,1971 cycles
+
+
+** Single reader test, no contention **
+SINGLE_READER_TEST_DURATION 10s
+
+Preemption latency for cpu 2 disabled 31596702 times, [min,avg,max] 502,508,54256 cycles
+
+
+** Multiple readers test, no contention (4 readers, busy-loop) **
+MULTIPLE_READERS_TEST_DURATION 10s
+NR_READERS 4
+
+Preemption latency for cpu 1 disabled 9302974 times, [min,avg,max] 502,2039,88060 cycles
+Preemption latency for cpu 3 disabled 9270742 times, [min,avg,max] 508,2045,61342 cycles
+Preemption latency for cpu 6 disabled 13331943 times, [min,avg,max] 508,1387,309088 cycles
+Preemption latency for cpu 7 disabled 4781453 times, [min,avg,max] 508,4092,230752 cycles
+
+
+** High contention test **
+TEST_DURATION 60s
+NR_WRITERS 2
+NR_TRYLOCK_WRITERS 1
+NR_READERS 4
+NR_TRYLOCK_READERS 1
+WRITER_DELAY 100us
+TRYLOCK_WRITER_DELAY 1000us
+TRYLOCK_WRITERS_FAIL_ITER 100
+THREAD_READER_DELAY 0   /* busy loop */
+INTERRUPT_READER_DELAY 100ms
+
+Standard Linux rwlock
+
+irqsoff latency trace v1.1.5 on 2.6.27-rc3-trace
+--------------------------------------------------------------------
+ latency: 2902 us, #3/3, CPU#5 | (M:preempt VP:0, KP:0, SP:0 HP:0 #P:8)
+    -----------------
+    | task: wbiasrwlock_wri-4984 (uid:0 nice:-5 policy:0 rt_prio:0)
+    -----------------
+ => started at: _write_lock_irq
+ => ended at:   _write_unlock_irq
+
+#                _------=> CPU#
+#               / _-----=> irqs-off
+#              | / _----=> need-resched
+#              || / _---=> hardirq/softirq
+#              ||| / _--=> preempt-depth
+#              |||| /
+#              |||||     delay
+#  cmd     pid ||||| time  |   caller
+#     \   /    |||||   \   |   /
+wbiasrwl-4984  5d..1    0us!: _write_lock_irq (0)
+wbiasrwl-4984  5d..2 2902us : _write_unlock_irq (0)
+wbiasrwl-4984  5d..3 2903us : trace_hardirqs_on (_write_unlock_irq)
+
+
+Writer-biased rwlock, same test routine
+
+irqsoff latency trace v1.1.5 on 2.6.27-rc3-trace
+--------------------------------------------------------------------
+ latency: 33 us, #3/3, CPU#7 | (M:preempt VP:0, KP:0, SP:0 HP:0 #P:8)
+    -----------------
+    | task: events/7-27 (uid:0 nice:-5 policy:0 rt_prio:0)
+    -----------------
+ => started at: _spin_lock_irqsave
+ => ended at:   _spin_unlock_irqrestore
+
+#                _------=> CPU#
+#               / _-----=> irqs-off
+#              | / _----=> need-resched
+#              || / _---=> hardirq/softirq
+#              ||| / _--=> preempt-depth
+#              |||| /
+#              |||||     delay
+#  cmd     pid ||||| time  |   caller
+#     \   /    |||||   \   |   /
+events/7-27    7d...    0us+: _spin_lock_irqsave (0)
+events/7-27    7d..1   33us : _spin_unlock_irqrestore (0)
+events/7-27    7d..2   33us : trace_hardirqs_on (_spin_unlock_irqrestore)
+
+(latency unrelated to the tests, therefore irq latency <= 33us)
+
+wbias rwlock instrumentation (below) shows that interrupt latency has been 14176
+cycles, for a total of 7us.
+
+Detailed psrwlock latency breakdown :
+
+IRQ latency for cpu 0 disabled 1086419 times, [min,avg,max] 316,2833,14176 cycles
+IRQ latency for cpu 1 disabled 1099517 times, [min,avg,max] 316,1820,8254 cycles
+IRQ latency for cpu 3 disabled 159088 times, [min,avg,max] 316,1409,5632 cycles
+IRQ latency for cpu 4 disabled 161 times, [min,avg,max] 340,1882,5206 cycles
+SoftIRQ latency for cpu 0 disabled 1086419 times, [min,avg,max] 2212,5350,166402 cycles
+SoftIRQ latency for cpu 1 disabled 1099517 times, [min,avg,max] 2230,4265,138988 cycles
+SoftIRQ latency for cpu 3 disabled 159088 times, [min,avg,max] 2212,3319,14992 cycles
+SoftIRQ latency for cpu 4 disabled 161 times, [min,avg,max] 2266,3802,7138 cycles
+Preemption latency for cpu 3 disabled 59855 times, [min,avg,max] 5266,15706,53494 cycles
+Preemption latency for cpu 4 disabled 72 times, [min,avg,max] 5728,14132,28042 cycles
+Preemption latency for cpu 5 disabled 55586612 times, [min,avg,max] 196,2080,126526 cycles
+
+Note : preemptable critical sections has been implemented after the previous
+latency tests. It should be noted that the worse latency obtained in wbias
+rwlock comes from the busy-loop for the wait queue protection mutex (100us) :
+
+IRQ latency for cpu 3 disabled 2822178 times, [min,avg,max] 256,8892,209926 cycles
+disable : [<ffffffff803acff5>] rwlock_wait+0x265/0x2c0
+
+
+
+**** Lock contention delays ****
+
+Number of cycles required to take the lock are benchmarked for each context.
+
+
+** get_cycles calibration **
+get_cycles takes [min,avg,max] 72,75,78 cycles, results calibrated on avg
+
+
+** Single writer test, no contention **
+
+* Writer-biased rwlocks v13
+
+writer_thread/0 iterations : 100274, lock delay [min,avg,max] 27,33,249 cycles
+writer_thread/0 iterations : 100274, unlock delay [min,avg,max] 27,30,10407 cycles
+
+* Standard rwlock, kernel 2.6.27-rc3
+
+writer_thread/0 iterations : 100322, lock delay [min,avg,max] 37,40,4537 cycles
+writer_thread/0 iterations : 100322, unlock delay [min,avg,max] 37,40,25435 cycles
+
+
+** Single preemptable reader test, no contention **
+
+Writer-biased rwlock has a twice faster lock and unlock uncontended fast path.
+Note that wbias rwlock support preemptable readers. Standard rwlocks disables
+preemption.
+
+* Writer-biased rwlocks v13
+
+preader_thread/0 iterations : 33856510, lock delay [min,avg,max] 27,29,34035 cycles
+preader_thread/0 iterations : 33856510, unlock delay [min,avg,max] 15,20,34701 cycles
+
+* Standard rwlock, kernel 2.6.27-rc3
+
+N/A : preemption must be disabled with standard rwlocks.
+
+
+** Single non-preemptable reader test, no contention **
+wbias rwlock read is still twice faster than standard rwlock even for read done
+in non-preemptable context.
+
+* Writer-biased rwlocks v13
+
+npreader_thread/0 iterations : 33461225, lock delay [min,avg,max] 27,30,16329 cycles
+npreader_thread/0 iterations : 33461225, unlock delay [min,avg,max] 15,19,21657 cycles
+
+* Standard rwlock, kernel 2.6.27-rc3
+
+npreader_thread/0 iterations : 31639225, lock delay [min,avg,max] 37,39,127111 cycles
+npreader_thread/0 iterations : 31639225, unlock delay [min,avg,max] 37,42,215587 cycles
+
+
+** Multiple p(reemptable)/n(on-)p(reemptable) readers test, no contention **
+This contended case where multiple readers try to access the data structure in
+loop, without any writer, shows that standard rwlock average is a little bit
+better than wbias rwlock. It could be explained by the fact that wbias rwlock
+cmpxchg operation, which is used to keep the count of active readers, may fail
+if there is contention and must therefore be retried. The fastpath actually
+expects the number of readers to be 0, which isn't the case here.
+
+* Writer-biased rwlocks v13
+
+npreader_thread/0 iterations : 16885001, lock delay [min,avg,max] 27,425,40239 cycles
+npreader_thread/0 iterations : 16885001, unlock delay [min,avg,max] 15,220,18153 cycles
+npreader_thread/1 iterations : 16832690, lock delay [min,avg,max] 33,433,26841 cycles
+npreader_thread/1 iterations : 16832690, unlock delay [min,avg,max] 15,219,22329 cycles
+preader_thread/0 iterations : 17185174, lock delay [min,avg,max] 27,438,31437 cycles
+preader_thread/0 iterations : 17185174, unlock delay [min,avg,max] 15,211,30465 cycles
+preader_thread/1 iterations : 17293655, lock delay [min,avg,max] 27,435,53301 cycles
+preader_thread/1 iterations : 17293655, unlock delay [min,avg,max] 15,209,63921 cycles
+
+* Standard rwlock, kernel 2.6.27-rc3
+
+npreader_thread/0 iterations : 19248438, lock delay [min,avg,max] 37,273,364459 cycles
+npreader_thread/0 iterations : 19248438, unlock delay [min,avg,max] 43,216,272539 cycles
+npreader_thread/1 iterations : 19251717, lock delay [min,avg,max] 37,242,365719 cycles
+npreader_thread/1 iterations : 19251717, unlock delay [min,avg,max] 43,249,162847 cycles
+preader_thread/0 iterations : 19557931, lock delay [min,avg,max] 37,250,334921 cycles
+preader_thread/0 iterations : 19557931, unlock delay [min,avg,max] 37,245,266377 cycles
+preader_thread/1 iterations : 19671318, lock delay [min,avg,max] 37,258,390913 cycles
+preader_thread/1 iterations : 19671318, unlock delay [min,avg,max] 37,234,604507 cycles
+
+
+
+** High contention test **
+
+In high contention test :
+
+TEST_DURATION 60s
+NR_WRITERS 2
+NR_TRYLOCK_WRITERS 1
+NR_PREEMPTABLE_READERS 2
+NR_NON_PREEMPTABLE_READERS 2
+NR_TRYLOCK_READERS 1
+WRITER_DELAY 100us
+TRYLOCK_WRITER_DELAY 1000us
+TRYLOCK_WRITERS_FAIL_ITER 100
+THREAD_READER_DELAY 0   /* busy loop */
+INTERRUPT_READER_DELAY 100ms
+
+
+* Preemptable writers
+
+* Writer-biased rwlocks v13
+
+writer_thread/0 iterations : 537678, lock delay [min,avg,max] 123,14021,8580813 cycles
+writer_thread/0 iterations : 537678, unlock delay [min,avg,max] 387,9070,1450053 cycles
+writer_thread/1 iterations : 536944, lock delay [min,avg,max] 123,13179,8687331 cycles
+writer_thread/1 iterations : 536944, unlock delay [min,avg,max] 363,10430,1400835 cycles
+
+* Standard rwlock, kernel 2.6.27-rc3
+
+writer_thread/0 iterations : 222797, lock delay [min,avg,max] 127,336611,4710367 cycles
+writer_thread/0 iterations : 222797, unlock delay [min,avg,max] 151,2009,714115 cycles
+writer_thread/1 iterations : 6845, lock delay [min,avg,max] 139,17271138,352848961 cycles
+writer_thread/1 iterations : 6845, unlock delay [min,avg,max] 217,93935,1991509 cycles
+
+
+* Non-preemptable readers
+
+* Writer-biased rwlocks v13
+
+npreader_thread/0 iterations : 64652609, lock delay [min,avg,max] 27,828,67497 cycles
+npreader_thread/0 iterations : 64652609, unlock delay [min,avg,max] 15,485,202773 cycles
+npreader_thread/1 iterations : 65143310, lock delay [min,avg,max] 27,817,64569 cycles
+npreader_thread/1 iterations : 65143310, unlock delay [min,avg,max] 15,484,133611 cycles
+
+* Standard rwlock, kernel 2.6.27-rc3
+
+npreader_thread/0 iterations : 68298472, lock delay [min,avg,max] 37,640,733423 cycles
+npreader_thread/0 iterations : 68298472, unlock delay [min,avg,max] 37,565,672241 cycles
+npreader_thread/1 iterations : 70331311, lock delay [min,avg,max] 37,603,393925 cycles
+npreader_thread/1 iterations : 70331311, unlock delay [min,avg,max] 37,558,373477 cycles
+
+
+* Preemptable readers
+
+* Writer-biased rwlocks v13
+
+preader_thread/0 iterations : 38484022, lock delay [min,avg,max] 27,2207,89363619 cycles
+preader_thread/0 iterations : 38484022, unlock delay [min,avg,max] 15,392,1965315 cycles
+preader_thread/1 iterations : 44661191, lock delay [min,avg,max] 27,1672,8708253 cycles
+preader_thread/1 iterations : 44661191, unlock delay [min,avg,max] 15,495,142119 cycles
+
+* Standard rwlock, kernel 2.6.27-rc3
+
+N/A : preemption must be disabled with standard rwlocks.
+
+
+* Interrupt context readers
+
+* Writer-biased rwlocks v13 (note : the highest unlock delays (32us) are
+  caused by the wakeup of the wait queue done at the exit of the critical section
+  if the waitqueue is active)
+
+interrupt readers on CPU 0, lock delay [min,avg,max] 135,1603,28119 cycles
+interrupt readers on CPU 0, unlock delay [min,avg,max] 9,1712,35355 cycles
+interrupt readers on CPU 1, lock delay [min,avg,max] 39,1756,18285 cycles
+interrupt readers on CPU 1, unlock delay [min,avg,max] 9,2628,58257 cycles
+interrupt readers on CPU 2, lock delay [min,avg,max] 129,1450,16533 cycles
+interrupt readers on CPU 2, unlock delay [min,avg,max] 27,2354,49647 cycles
+interrupt readers on CPU 3, lock delay [min,avg,max] 75,1758,27051 cycles
+interrupt readers on CPU 3, unlock delay [min,avg,max] 9,2446,63603 cycles
+interrupt readers on CPU 4, lock delay [min,avg,max] 159,1707,27903 cycles
+interrupt readers on CPU 4, unlock delay [min,avg,max] 9,1822,39957 cycles
+interrupt readers on CPU 6, lock delay [min,avg,max] 105,1635,24489 cycles
+interrupt readers on CPU 6, unlock delay [min,avg,max] 9,2390,36771 cycles
+interrupt readers on CPU 7, lock delay [min,avg,max] 135,1614,22995 cycles
+interrupt readers on CPU 7, unlock delay [min,avg,max] 9,2052,43479 cycles
+
+* Standard rwlock, kernel 2.6.27-rc3 (note : these numbers seems good, but
+  they do not take interrupt latency in account. See interrupt latency tests
+  above for discussion of this issue)
+
+interrupt readers on CPU 0, lock delay [min,avg,max] 55,573,4417 cycles
+interrupt readers on CPU 0, unlock delay [min,avg,max] 43,529,1591 cycles
+interrupt readers on CPU 1, lock delay [min,avg,max] 139,591,5731 cycles
+interrupt readers on CPU 1, unlock delay [min,avg,max] 31,534,2395 cycles
+interrupt readers on CPU 2, lock delay [min,avg,max] 127,671,6043 cycles
+interrupt readers on CPU 2, unlock delay [min,avg,max] 37,401,1567 cycles
+interrupt readers on CPU 3, lock delay [min,avg,max] 151,676,5569 cycles
+interrupt readers on CPU 3, unlock delay [min,avg,max] 127,536,2797 cycles
+interrupt readers on CPU 5, lock delay [min,avg,max] 127,531,15397 cycles
+interrupt readers on CPU 5, unlock delay [min,avg,max] 31,323,1747 cycles
+interrupt readers on CPU 6, lock delay [min,avg,max] 121,548,29125 cycles
+interrupt readers on CPU 6, unlock delay [min,avg,max] 31,435,2089 cycles
+interrupt readers on CPU 7, lock delay [min,avg,max] 37,613,5485 cycles
+interrupt readers on CPU 7, unlock delay [min,avg,max] 49,541,1645 cycles
diff --git a/stblinux-2.6.31/Documentation/trace/tracepoints.txt b/stblinux-2.6.31/Documentation/trace/tracepoints.txt
index c0e1cee..d380250 100644
--- a/stblinux-2.6.31/Documentation/trace/tracepoints.txt
+++ b/stblinux-2.6.31/Documentation/trace/tracepoints.txt
@@ -106,7 +106,7 @@ used to export the defined tracepoints.
 See the example provided in samples/tracepoints
 
 Compile them with your kernel.  They are built during 'make' (not
-'make modules') when CONFIG_SAMPLE_TRACEPOINTS=m.
+'make modules') when CONFIG_SAMPLE=y and CONFIG_SAMPLE_TRACEPOINTS=m.
 
 Run, as root :
 modprobe tracepoint-sample (insmod order is not important)
diff --git a/stblinux-2.6.31/MAINTAINERS b/stblinux-2.6.31/MAINTAINERS
index 94138c4..cdea3d4 100644
--- a/stblinux-2.6.31/MAINTAINERS
+++ b/stblinux-2.6.31/MAINTAINERS
@@ -3154,6 +3154,13 @@ S:	Maintained
 F:	Documentation/hwmon/lis3lv02d
 F:	drivers/hwmon/lis3lv02d.*
 
+LINUX TRACE TOOLKIT NEXT GENERATION
+P:	Mathieu Desnoyers
+M:	mathieu.desnoyers@polymtl.ca
+L:	ltt-dev@lttng.org
+W:	http://ltt.polymtl.ca
+S:	Maintained
+
 LM83 HARDWARE MONITOR DRIVER
 M:	Jean Delvare <khali@linux-fr.org>
 L:	lm-sensors@lm-sensors.org
diff --git a/stblinux-2.6.31/Makefile b/stblinux-2.6.31/Makefile
index 5ede3e0..1f64c72 100644
--- a/stblinux-2.6.31/Makefile
+++ b/stblinux-2.6.31/Makefile
@@ -552,6 +552,11 @@ ifdef CONFIG_FUNCTION_TRACER
 KBUILD_CFLAGS	+= -pg
 endif
 
+# arch Makefile detects if the compiler permits use of immediate values
+ifdef USE_IMMEDIATE
+KBUILD_CFLAGS	+= -DUSE_IMMEDIATE
+endif
+
 # We trigger additional mismatches with less inlining
 ifdef CONFIG_DEBUG_SECTION_MISMATCH
 KBUILD_CFLAGS += $(call cc-option, -fno-inline-functions-called-once)
@@ -641,7 +646,7 @@ export mod_strip_cmd
 
 
 ifeq ($(KBUILD_EXTMOD),)
-core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/
+core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/ ltt/
 
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
diff --git a/stblinux-2.6.31/arch/alpha/include/asm/thread_info.h b/stblinux-2.6.31/arch/alpha/include/asm/thread_info.h
index 60c83ab..498b378 100644
--- a/stblinux-2.6.31/arch/alpha/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/alpha/include/asm/thread_info.h
@@ -56,7 +56,7 @@ register struct thread_info *__current_thread_info __asm__("$8");
 
 #endif /* __ASSEMBLY__ */
 
-#define PREEMPT_ACTIVE		0x40000000
+#define PREEMPT_ACTIVE		0x10000000
 
 /*
  * Thread information flags:
@@ -75,6 +75,7 @@ register struct thread_info *__current_thread_info __asm__("$8");
 #define TIF_UAC_SIGBUS		7
 #define TIF_MEMDIE		8
 #define TIF_RESTORE_SIGMASK	9	/* restore signal mask in do_signal */
+#define TIF_KERNEL_TRACE	10	/* Kernel tracing of syscalls */
 #define TIF_FREEZE		16	/* is freezing for suspend */
 
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
@@ -82,6 +83,7 @@ register struct thread_info *__current_thread_info __asm__("$8");
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
 
 /* Work to do on interrupt/exception return.  */
@@ -89,7 +91,7 @@ register struct thread_info *__current_thread_info __asm__("$8");
 
 /* Work to do on any return to userspace.  */
 #define _TIF_ALLWORK_MASK	(_TIF_WORK_MASK		\
-				 | _TIF_SYSCALL_TRACE)
+				 | _TIF_SYSCALL_TRACE | _TIF_KERNEL_TRACE)
 
 #define ALPHA_UAC_SHIFT		6
 #define ALPHA_UAC_MASK		(1 << TIF_UAC_NOPRINT | 1 << TIF_UAC_NOFIX | \
diff --git a/stblinux-2.6.31/arch/arm/Kconfig b/stblinux-2.6.31/arch/arm/Kconfig
index aef63c8..853edae 100644
--- a/stblinux-2.6.31/arch/arm/Kconfig
+++ b/stblinux-2.6.31/arch/arm/Kconfig
@@ -16,6 +16,7 @@ config ARM
 	select HAVE_ARCH_KGDB
 	select HAVE_KPROBES if (!XIP_KERNEL)
 	select HAVE_KRETPROBES if (HAVE_KPROBES)
+	select HAVE_LTT_DUMP_TABLES
 	select HAVE_FUNCTION_TRACER if (!XIP_KERNEL)
 	select HAVE_GENERIC_DMA_COHERENT
 	help
diff --git a/stblinux-2.6.31/arch/arm/include/asm/thread_info.h b/stblinux-2.6.31/arch/arm/include/asm/thread_info.h
index 73394e5..2316243 100644
--- a/stblinux-2.6.31/arch/arm/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/arm/include/asm/thread_info.h
@@ -128,6 +128,7 @@ extern void vfp_sync_state(struct thread_info *thread);
 /*
  * thread information flags:
  *  TIF_SYSCALL_TRACE	- syscall trace active
+ *  TIF_KERNEL_TRACE	- kernel trace active
  *  TIF_SIGPENDING	- signal pending
  *  TIF_NEED_RESCHED	- rescheduling necessary
  *  TIF_USEDFPU		- FPU was used by this task this quantum (SMP)
@@ -135,6 +136,7 @@ extern void vfp_sync_state(struct thread_info *thread);
  */
 #define TIF_SIGPENDING		0
 #define TIF_NEED_RESCHED	1
+#define TIF_KERNEL_TRACE	7
 #define TIF_SYSCALL_TRACE	8
 #define TIF_POLLING_NRFLAG	16
 #define TIF_USING_IWMMXT	17
@@ -143,6 +145,7 @@ extern void vfp_sync_state(struct thread_info *thread);
 
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
+#define _TIF_KERNEL_TRACE	(1 << TIF_KERNEL_TRACE)
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_USING_IWMMXT	(1 << TIF_USING_IWMMXT)
diff --git a/stblinux-2.6.31/arch/arm/include/asm/trace-clock.h b/stblinux-2.6.31/arch/arm/include/asm/trace-clock.h
new file mode 100644
index 0000000..f88f132
--- /dev/null
+++ b/stblinux-2.6.31/arch/arm/include/asm/trace-clock.h
@@ -0,0 +1 @@
+#include <mach/trace-clock.h>
diff --git a/stblinux-2.6.31/arch/arm/include/asm/unistd.h b/stblinux-2.6.31/arch/arm/include/asm/unistd.h
index 0e97b8c..630c170 100644
--- a/stblinux-2.6.31/arch/arm/include/asm/unistd.h
+++ b/stblinux-2.6.31/arch/arm/include/asm/unistd.h
@@ -392,6 +392,8 @@
 #define __NR_rt_tgsigqueueinfo		(__NR_SYSCALL_BASE+363)
 #define __NR_perf_counter_open		(__NR_SYSCALL_BASE+364)
 
+#define __NR_syscall_max 365
+
 /*
  * The following SWIs are ARM private.
  */
diff --git a/stblinux-2.6.31/arch/arm/kernel/entry-common.S b/stblinux-2.6.31/arch/arm/kernel/entry-common.S
index 8c3de1a..71d6e59 100644
--- a/stblinux-2.6.31/arch/arm/kernel/entry-common.S
+++ b/stblinux-2.6.31/arch/arm/kernel/entry-common.S
@@ -47,6 +47,8 @@ ret_fast_syscall:
  * Ok, we need to do extra processing, enter the slow path.
  */
 fast_work_pending:
+	tst	r1, #_TIF_KERNEL_TRACE		@ flag can be set asynchronously
+	bne	__sys_trace_return
 	str	r0, [sp, #S_R0+S_OFF]!		@ returned r0
 work_pending:
 	tst	r1, #_TIF_NEED_RESCHED
@@ -91,8 +93,8 @@ ENTRY(ret_from_fork)
 	get_thread_info tsk
 	ldr	r1, [tsk, #TI_FLAGS]		@ check for syscall tracing
 	mov	why, #1
-	tst	r1, #_TIF_SYSCALL_TRACE		@ are we tracing syscalls?
-	beq	ret_slow_syscall
+	tst	r1, #_TIF_SYSCALL_TRACE | _TIF_KERNEL_TRACE
+	beq	ret_slow_syscall		@ are we tracing syscalls?
 	mov	r1, sp
 	mov	r0, #1				@ trace exit [IP = 1]
 	bl	syscall_trace
@@ -268,8 +270,8 @@ ENTRY(vector_swi)
 #endif
 
 	stmdb	sp!, {r4, r5}			@ push fifth and sixth args
-	tst	ip, #_TIF_SYSCALL_TRACE		@ are we tracing syscalls?
-	bne	__sys_trace
+	tst	ip, #_TIF_SYSCALL_TRACE | _TIF_KERNEL_TRACE
+	bne	__sys_trace			@ are we tracing syscalls?
 
 	cmp	scno, #NR_syscalls		@ check upper syscall limit
 	adr	lr, ret_fast_syscall		@ return address
diff --git a/stblinux-2.6.31/arch/arm/kernel/process.c b/stblinux-2.6.31/arch/arm/kernel/process.c
index 39196df..aeb0a27 100644
--- a/stblinux-2.6.31/arch/arm/kernel/process.c
+++ b/stblinux-2.6.31/arch/arm/kernel/process.c
@@ -29,6 +29,7 @@
 #include <linux/tick.h>
 #include <linux/utsname.h>
 #include <linux/uaccess.h>
+#include <trace/sched.h>
 
 #include <asm/leds.h>
 #include <asm/processor.h>
@@ -37,6 +38,8 @@
 #include <asm/stacktrace.h>
 #include <asm/mach/time.h>
 
+DEFINE_TRACE(sched_kthread_create);
+
 static const char *processor_modes[] = {
   "USER_26", "FIQ_26" , "IRQ_26" , "SVC_26" , "UK4_26" , "UK5_26" , "UK6_26" , "UK7_26" ,
   "UK8_26" , "UK9_26" , "UK10_26", "UK11_26", "UK12_26", "UK13_26", "UK14_26", "UK15_26",
@@ -381,6 +384,7 @@ asm(	".section .text\n"
 pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 {
 	struct pt_regs regs;
+	long pid;
 
 	memset(&regs, 0, sizeof(regs));
 
@@ -390,7 +394,10 @@ pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 	regs.ARM_pc = (unsigned long)kernel_thread_helper;
 	regs.ARM_cpsr = SVC_MODE | PSR_ENDSTATE;
 
-	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+	pid = do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+
+	trace_sched_kthread_create(fn, pid);
+	return pid;
 }
 EXPORT_SYMBOL(kernel_thread);
 
diff --git a/stblinux-2.6.31/arch/arm/kernel/ptrace.c b/stblinux-2.6.31/arch/arm/kernel/ptrace.c
index 89882a1..c46d749 100644
--- a/stblinux-2.6.31/arch/arm/kernel/ptrace.c
+++ b/stblinux-2.6.31/arch/arm/kernel/ptrace.c
@@ -17,12 +17,17 @@
 #include <linux/user.h>
 #include <linux/security.h>
 #include <linux/init.h>
+#include <linux/module.h>
+#include <linux/marker.h>
+#include <linux/kallsyms.h>
 #include <linux/signal.h>
 #include <linux/uaccess.h>
+#include <trace/syscall.h>
 
 #include <asm/pgtable.h>
 #include <asm/system.h>
 #include <asm/traps.h>
+#include <asm/unistd.h>
 
 #include "ptrace.h"
 
@@ -52,6 +57,30 @@
 #define BREAKINST_THUMB	0xde01
 #endif
 
+DEFINE_TRACE(syscall_entry);
+DEFINE_TRACE(syscall_exit);
+
+extern unsigned long sys_call_table[];
+
+void ltt_dump_sys_call_table(void *call_data)
+{
+	int i;
+	char namebuf[KSYM_NAME_LEN];
+
+	for (i = 0; i < __NR_syscall_max + 1; i++) {
+		sprint_symbol(namebuf, sys_call_table[i]);
+		__trace_mark(0, syscall_state, sys_call_table, call_data,
+			"id %d address %p symbol %s",
+			i, (void*)sys_call_table[i], namebuf);
+	}
+}
+EXPORT_SYMBOL_GPL(ltt_dump_sys_call_table);
+
+void ltt_dump_idt_table(void *call_data)
+{
+}
+EXPORT_SYMBOL_GPL(ltt_dump_idt_table);
+
 /*
  * this routine will get a word off of the processes privileged stack.
  * the offset is how far from the base addr as stored in the THREAD.
@@ -845,6 +874,11 @@ asmlinkage int syscall_trace(int why, struct pt_regs *regs, int scno)
 {
 	unsigned long ip;
 
+	if (!why)
+		trace_syscall_entry(regs, scno);
+	else
+		trace_syscall_exit(regs->ARM_r0);
+
 	if (!test_thread_flag(TIF_SYSCALL_TRACE))
 		return scno;
 	if (!(current->ptrace & PT_PTRACED))
diff --git a/stblinux-2.6.31/arch/arm/kernel/sys_arm.c b/stblinux-2.6.31/arch/arm/kernel/sys_arm.c
index b3ec641..0880c9f 100644
--- a/stblinux-2.6.31/arch/arm/kernel/sys_arm.c
+++ b/stblinux-2.6.31/arch/arm/kernel/sys_arm.c
@@ -28,6 +28,9 @@
 #include <linux/utsname.h>
 #include <linux/ipc.h>
 #include <linux/uaccess.h>
+#include <trace/ipc.h>
+
+DEFINE_TRACE(ipc_call);
 
 extern unsigned long do_mremap(unsigned long addr, unsigned long old_len,
 			       unsigned long new_len, unsigned long flags,
@@ -143,6 +146,8 @@ asmlinkage int sys_ipc(uint call, int first, int second, int third,
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_ipc_call(call, first);
+
 	switch (call) {
 	case SEMOP:
 		return sys_semtimedop (first, (struct sembuf __user *)ptr, second, NULL);
diff --git a/stblinux-2.6.31/arch/arm/kernel/traps.c b/stblinux-2.6.31/arch/arm/kernel/traps.c
index 57eb0f6..c957f74 100644
--- a/stblinux-2.6.31/arch/arm/kernel/traps.c
+++ b/stblinux-2.6.31/arch/arm/kernel/traps.c
@@ -21,6 +21,7 @@
 #include <linux/hardirq.h>
 #include <linux/init.h>
 #include <linux/uaccess.h>
+#include <trace/trap.h>
 
 #include <asm/atomic.h>
 #include <asm/cacheflush.h>
@@ -32,6 +33,9 @@
 #include "ptrace.h"
 #include "signal.h"
 
+DEFINE_TRACE(trap_entry);
+DEFINE_TRACE(trap_exit);
+
 static const char *handler[]= { "prefetch abort", "data abort", "address exception", "interrupt" };
 
 #ifdef CONFIG_DEBUG_USER
@@ -275,7 +279,11 @@ void arm_notify_die(const char *str, struct pt_regs *regs,
 		current->thread.error_code = err;
 		current->thread.trap_no = trap;
 
+		trace_trap_entry(regs, current->thread.trap_no);
+
 		force_sig_info(info->si_signo, info, current);
+
+		trace_trap_exit();
 	} else {
 		die(str, regs, err);
 	}
diff --git a/stblinux-2.6.31/arch/arm/mach-omap2/Makefile b/stblinux-2.6.31/arch/arm/mach-omap2/Makefile
index 735bae5..f7f9514 100644
--- a/stblinux-2.6.31/arch/arm/mach-omap2/Makefile
+++ b/stblinux-2.6.31/arch/arm/mach-omap2/Makefile
@@ -62,6 +62,7 @@ obj-$(CONFIG_MACH_OMAP3_PANDORA)	+= board-omap3pandora.o \
 					   mmc-twl4030.o
 obj-$(CONFIG_MACH_OMAP_3430SDP)		+= board-3430sdp.o \
 					   mmc-twl4030.o
+obj-$(CONFIG_HAVE_TRACE_CLOCK)		+= trace-clock.o
 
 obj-$(CONFIG_MACH_NOKIA_RX51)		+= board-rx51.o \
 					   board-rx51-peripherals.o \
diff --git a/stblinux-2.6.31/arch/arm/mach-omap2/clock34xx.c b/stblinux-2.6.31/arch/arm/mach-omap2/clock34xx.c
index cd7819c..7d40050 100644
--- a/stblinux-2.6.31/arch/arm/mach-omap2/clock34xx.c
+++ b/stblinux-2.6.31/arch/arm/mach-omap2/clock34xx.c
@@ -337,6 +337,9 @@ static struct omap_clk omap34xx_clks[] = {
  */
 #define SDRC_MPURATE_LOOPS		96
 
+unsigned long long cpu_hz;
+EXPORT_SYMBOL(cpu_hz);
+
 /**
  * omap3430es2_clk_ssi_find_idlest - return CM_IDLEST info for SSI
  * @clk: struct clk * being enabled
@@ -1141,6 +1144,8 @@ int __init omap2_clk_init(void)
 	       (osc_sys_ck.rate / 1000000), (osc_sys_ck.rate / 100000) % 10,
 	       (core_ck.rate / 1000000), (arm_fck.rate / 1000000));
 
+	cpu_hz = arm_fck.rate;
+
 	/*
 	 * Only enable those clocks we will need, let the drivers
 	 * enable other clocks as necessary
diff --git a/stblinux-2.6.31/arch/arm/mach-omap2/trace-clock.c b/stblinux-2.6.31/arch/arm/mach-omap2/trace-clock.c
new file mode 100644
index 0000000..14a332a
--- /dev/null
+++ b/stblinux-2.6.31/arch/arm/mach-omap2/trace-clock.c
@@ -0,0 +1,253 @@
+/*
+ * arch/arm/mach-omap2/trace-clock.c
+ *
+ * Trace clock for ARM OMAP3
+ * Currently uniprocessor-only.
+ *
+ * Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>, February 2009
+ */
+
+#include <linux/module.h>
+#include <linux/clocksource.h>
+#include <linux/timer.h>
+#include <linux/spinlock.h>
+#include <linux/init.h>
+#include <mach/dmtimer.h>
+#include <mach/trace-clock.h>
+
+/* Need direct access to the clock from kernel/time/timekeeping.c */
+extern struct clocksource *clock;
+
+/* 32KHz counter count save upon PM sleep */
+static u32 saved_32k_count;
+static u64 saved_trace_clock;
+
+static void clear_ccnt_ms(unsigned long data);
+
+static DEFINE_TIMER(clear_ccnt_ms_timer, clear_ccnt_ms, 0, 0);
+
+/* According to timer32k.c, this is a 32768Hz clock, not a 32000Hz clock. */
+#define TIMER_32K_FREQ	32768
+#define TIMER_32K_SHIFT	15
+
+/*
+ * Clear ccnt twice per 31-bit overflow, or 4 times per 32-bits period.
+ */
+static u32 clear_ccnt_interval;
+
+static DEFINE_SPINLOCK(trace_clock_lock);
+static int trace_clock_refcount;
+
+static int print_info_done;
+
+/*
+ * Cycle counter management.
+ */
+
+static inline void write_pmnc(u32 val)
+{
+	__asm__ __volatile__ ("mcr p15, 0, %0, c9, c12, 0" : : "r" (val));
+}
+
+static inline u32 read_pmnc(void)
+{
+	u32 val;
+	__asm__ __volatile__ ("mrc p15, 0, %0, c9, c12, 0" : "=r" (val));
+        return val;
+}
+
+static inline void write_ctens(u32 val)
+{
+	__asm__ __volatile__ ("mcr p15, 0, %0, c9, c12, 1" : : "r" (val));
+}
+
+static inline u32 read_ctens(void)
+{
+	u32 val;
+	__asm__ __volatile__ ("mrc p15, 0, %0, c9, c12, 1" : "=r" (val));
+	return val;
+}
+
+static inline void write_intenc(u32 val)
+{
+	__asm__ __volatile__ ("mcr p15, 0, %0, c9, c14, 2" : : "r" (val));
+}
+
+static inline u32 read_intenc(void)
+{
+	u32 val;
+        __asm__ __volatile__ ("mrc p15, 0, %0, c9, c14, 2" : "=r" (val));
+	return val;
+}
+
+static inline void write_useren(u32 val)
+{
+	__asm__ __volatile__ ("mcr p15, 0, %0, c9, c14, 0" : : "r" (val));
+}
+
+static inline u32 read_useren(void)
+{
+	u32 val;
+        __asm__ __volatile__ ("mrc p15, 0, %0, c9, c14, 0" : "=r" (val));
+	return val;
+}
+
+/*
+ * Must disable counter before writing to it.
+ */
+static inline void write_ccnt(u32 val)
+{
+	__asm__ __volatile__ ("mcr p15, 0, %0, c9, c13, 0" : : "r" (val));
+}
+
+/*
+ * Periodical timer handler, clears ccnt most significant bit each half-period
+ * of 31-bit overflow. Makes sure the ccnt never overflows.
+ */
+static void clear_ccnt_ms(unsigned long data)
+{
+	unsigned int cycles;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	isb();	/* clear the pipeline so we can execute ASAP */
+	write_ctens(read_ctens() & ~(1 << 31));	/* disable counter */
+	cycles = read_ccnt();
+	write_ccnt(cycles & ~(1 << 31));
+	isb();
+	write_ctens(read_ctens() |  (1 << 31));	/* enable counter */
+	isb();
+	local_irq_restore(flags);
+
+	mod_timer(&clear_ccnt_ms_timer, jiffies + clear_ccnt_interval);
+}
+
+void _start_trace_clock(void)
+{
+	unsigned long flags;
+	unsigned int count_32k, count_trace_clock;
+	u32 regval;
+	u64 ref_time, prev_time;
+
+	/* Let userspace access performance counter registers */
+	regval = read_useren();
+	regval |=  (1 << 0);	/* User mode enable */
+	write_useren(regval);
+
+	regval = read_intenc();
+	regval |=  (1 << 31);	/* CCNT overflow interrupt disable */
+	write_intenc(regval);
+
+	regval = read_pmnc();
+	regval |=  (1 << 0);	/* Enable all counters */
+	regval &= ~(1 << 3);	/* count every cycles */
+	regval &= ~(1 << 5);	/* Enable even in non-invasive debug prohib. */
+	write_pmnc(regval);
+
+	/*
+	 * Set the timer's value MSBs to the same as current 32K timer.
+	 */
+	ref_time = saved_trace_clock;
+	local_irq_save(flags);
+	count_32k = clocksource_read(clock);
+	prev_time = trace_clock_read64();
+	/*
+	 * Delta done on 32-bits, then casted to u64. Must guarantee
+	 * that we are called often enough so the difference does not
+	 * overflow 32 bits anyway.
+	 */
+	ref_time += (u64)(count_32k - saved_32k_count)
+			* (cpu_hz >> TIMER_32K_SHIFT);
+	/* Make sure we never _ever_ decrement the clock value */
+	if (ref_time < prev_time)
+		ref_time = prev_time;
+	write_ctens(read_ctens() & ~(1 << 31));	/* disable counter */
+	write_ccnt((u32)ref_time & ~(1 << 31));
+	write_ctens(read_ctens() |  (1 << 31));	/* enable counter */
+	count_trace_clock = trace_clock_read32();
+	_trace_clock_write_synthetic_tsc(ref_time);
+	local_irq_restore(flags);
+
+	get_synthetic_tsc();
+
+	/* mod_timer generates a trace event. Must run after time-base update */
+	mod_timer(&clear_ccnt_ms_timer, jiffies + clear_ccnt_interval);
+
+	if (unlikely(!print_info_done || saved_trace_clock > ref_time)) {
+		printk(KERN_INFO "Trace clock using cycle counter at %llu HZ\n"
+			"32k clk value 0x%08X, cycle counter value 0x%08X\n"
+			"saved 32k clk value 0x%08X, "
+			"saved cycle counter value 0x%016llX\n"
+			"synthetic value (write, read) 0x%016llX, 0x%016llX\n",
+			cpu_hz,
+			count_32k, count_trace_clock,
+			saved_32k_count, saved_trace_clock,
+			ref_time, trace_clock_read64());
+		printk(KERN_INFO "Reference clock used : %s\n", clock->name);
+		print_info_done = 1;
+	}
+}
+
+void _stop_trace_clock(void)
+{
+	saved_32k_count = clocksource_read(clock);
+	saved_trace_clock = trace_clock_read64();
+	del_timer_sync(&clear_ccnt_ms_timer);
+	put_synthetic_tsc();
+}
+
+void start_trace_clock(void)
+{
+	spin_lock(&trace_clock_lock);
+	if (!trace_clock_refcount)
+		goto end;
+	_start_trace_clock();
+end:
+	spin_unlock(&trace_clock_lock);
+}
+
+void stop_trace_clock(void)
+{
+	spin_lock(&trace_clock_lock);
+	if (!trace_clock_refcount)
+		goto end;
+	_stop_trace_clock();
+end:
+	spin_unlock(&trace_clock_lock);
+}
+
+void get_trace_clock(void)
+{
+	spin_lock(&trace_clock_lock);
+	if (trace_clock_refcount++)
+		goto end;
+	_start_trace_clock();
+end:
+	spin_unlock(&trace_clock_lock);
+}
+EXPORT_SYMBOL_GPL(get_trace_clock);
+
+void put_trace_clock(void)
+{
+	spin_lock(&trace_clock_lock);
+	WARN_ON(trace_clock_refcount <= 0);
+	if (trace_clock_refcount != 1)
+		goto end;
+	_stop_trace_clock();
+end:
+	trace_clock_refcount--;
+	spin_unlock(&trace_clock_lock);
+}
+EXPORT_SYMBOL_GPL(put_trace_clock);
+
+static __init int init_trace_clock(void)
+{
+	u64 rem;
+
+	clear_ccnt_interval = __iter_div_u64_rem(HZ * (1ULL << 30),
+				cpu_hz, &rem);
+	printk(KERN_INFO "LTTng will clear ccnt top bit every %u jiffies.\n",
+		clear_ccnt_interval);
+	return 0;
+}
+__initcall(init_trace_clock);
diff --git a/stblinux-2.6.31/arch/arm/plat-omap/Kconfig b/stblinux-2.6.31/arch/arm/plat-omap/Kconfig
index efe85d0..0c15ba4 100644
--- a/stblinux-2.6.31/arch/arm/plat-omap/Kconfig
+++ b/stblinux-2.6.31/arch/arm/plat-omap/Kconfig
@@ -22,6 +22,9 @@ config ARCH_OMAP3
 	bool "TI OMAP3"
 	select CPU_V7
 	select COMMON_CLKDEV
+	select HAVE_TRACE_CLOCK
+	select HAVE_TRACE_CLOCK_32_TO_64
+	select OMAP_32K_TIMER
 
 config ARCH_OMAP4
 	bool "TI OMAP4"
diff --git a/stblinux-2.6.31/arch/arm/plat-omap/include/mach/trace-clock.h b/stblinux-2.6.31/arch/arm/plat-omap/include/mach/trace-clock.h
new file mode 100644
index 0000000..8396bd0
--- /dev/null
+++ b/stblinux-2.6.31/arch/arm/plat-omap/include/mach/trace-clock.h
@@ -0,0 +1,96 @@
+/*
+ * Copyright (C) 2009 Mathieu Desnoyers
+ *
+ * Trace clock ARM OMAP3 definitions.
+ */
+
+#ifndef _ASM_ARM_TRACE_CLOCK_OMAP3_H
+#define _ASM_ARM_TRACE_CLOCK_OMAP3_H
+
+#include <linux/clk.h>
+#include <asm/system.h>
+#include <mach/dmtimer.h>
+
+/*
+ * Number of hardware clock bits. The higher order bits are expected to be 0.
+ * If the hardware clock source has more than 32 bits, the bits higher than the
+ * 32nd will be truncated by a cast to a 32 bits unsigned. Range : 1 - 32.
+ * (too few bits would be unrealistic though, since we depend on the timer to
+ * detect the overflows).
+ * OMAP3-specific : we clear bit 31 periodically so it never overflows. There is
+ * a hardware bug with CP14 and CP15 being executed at the same time a ccnt overflow
+ * occurs.
+ *
+ * Siarhei Siamashka <siarhei.siamashka@nokia.com> :
+ * Performance monitoring unit breaks if somebody is accessing CP14/CP15
+ * coprocessor register exactly at the same time as CCNT overflows (regardless
+ * of the fact if generation of interrupts is enabled or not). A workaround
+ * suggested by ARM was to never allow it to overflow and reset it
+ * periodically.
+ */
+#define TC_HW_BITS			31
+
+/* Expected maximum interrupt latency in ms : 15ms, *2 for security */
+#define TC_EXPECTED_INTERRUPT_LATENCY	30
+
+extern u64 trace_clock_read_synthetic_tsc(void);
+extern void _trace_clock_write_synthetic_tsc(u64 value);
+extern struct omap_dm_timer *trace_clock_timer;
+extern unsigned long long cpu_hz;
+
+/*
+ * ARM OMAP3 timers only return 32-bits values. We ened to extend it to a
+ * 64-bit value, which is provided by trace-clock-32-to-64.
+ */
+extern u64 trace_clock_async_tsc_read(void);
+/*
+ * Update done by the architecture upon wakeup.
+ */
+extern void _trace_clock_write_synthetic_tsc(u64 value);
+
+static inline u32 read_ccnt(void)
+{
+	u32 val;
+        __asm__ __volatile__ ("mrc p15, 0, %0, c9, c13, 0" : "=r" (val));
+	return val & ~(1 << TC_HW_BITS);
+}
+
+static inline u32 trace_clock_read32(void)
+{
+	u32 val;
+
+	isb();
+	val = read_ccnt();
+	isb();
+	return val;
+}
+
+static inline u64 trace_clock_read64(void)
+{
+	return trace_clock_read_synthetic_tsc();
+}
+
+static inline u64 trace_clock_frequency(void)
+{
+	return cpu_hz;
+}
+
+static inline u32 trace_clock_freq_scale(void)
+{
+	return 1;
+}
+
+extern void get_trace_clock(void);
+extern void put_trace_clock(void);
+extern void get_synthetic_tsc(void);
+extern void put_synthetic_tsc(void);
+
+/* Used by the architecture upon wakeup from PM idle */
+extern void start_trace_clock(void);
+/* Used by the architecture when going to PM idle */
+extern void stop_trace_clock(void);
+
+static inline void set_trace_clock_is_sync(int state)
+{
+}
+#endif /* _ASM_MIPS_TRACE_CLOCK_OMAP3_H */
diff --git a/stblinux-2.6.31/arch/avr32/include/asm/thread_info.h b/stblinux-2.6.31/arch/avr32/include/asm/thread_info.h
index fc42de5..573fb63 100644
--- a/stblinux-2.6.31/arch/avr32/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/avr32/include/asm/thread_info.h
@@ -66,7 +66,7 @@ static inline struct thread_info *current_thread_info(void)
 
 #endif /* !__ASSEMBLY__ */
 
-#define PREEMPT_ACTIVE		0x40000000
+#define PREEMPT_ACTIVE		0x10000000
 
 /*
  * Thread information flags
@@ -84,6 +84,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_MEMDIE		6
 #define TIF_RESTORE_SIGMASK	7	/* restore signal mask in do_signal */
 #define TIF_CPU_GOING_TO_SLEEP	8	/* CPU is entering sleep 0 mode */
+#define TIF_KERNEL_TRACE	9	/* kernel trace active */
 #define TIF_FREEZE		29
 #define TIF_DEBUG		30	/* debugging enabled */
 #define TIF_USERSPACE		31      /* true if FS sets userspace */
@@ -96,6 +97,7 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_MEMDIE		(1 << TIF_MEMDIE)
 #define _TIF_RESTORE_SIGMASK	(1 << TIF_RESTORE_SIGMASK)
 #define _TIF_CPU_GOING_TO_SLEEP (1 << TIF_CPU_GOING_TO_SLEEP)
+#define _TIF_KERNEL_TRACE	(1 << TIF_KERNEL_TRACE)
 #define _TIF_FREEZE		(1 << TIF_FREEZE)
 
 /* Note: The masks below must never span more than 16 bits! */
@@ -109,7 +111,7 @@ static inline struct thread_info *current_thread_info(void)
 	 | (1 << TIF_RESTORE_SIGMASK))
 
 /* work to do on any return to userspace */
-#define _TIF_ALLWORK_MASK	(_TIF_WORK_MASK | (1 << TIF_SYSCALL_TRACE))
+#define _TIF_ALLWORK_MASK	(_TIF_WORK_MASK | _TIF_SYSCALL_TRACE | _TIF_KERNEL_TRACE)
 /* work to do on return from debug mode */
 #define _TIF_DBGWORK_MASK	(_TIF_WORK_MASK & ~(1 << TIF_BREAKPOINT))
 
diff --git a/stblinux-2.6.31/arch/blackfin/include/asm/thread_info.h b/stblinux-2.6.31/arch/blackfin/include/asm/thread_info.h
index 2bbfdd9..5869cce 100644
--- a/stblinux-2.6.31/arch/blackfin/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/blackfin/include/asm/thread_info.h
@@ -123,6 +123,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_RESTORE_SIGMASK	5	/* restore signal mask in do_signal() */
 #define TIF_FREEZE              6       /* is freezing for suspend */
 #define TIF_IRQ_SYNC            7       /* sync pipeline stage */
+#define TIF_KERNEL_TRACE        8       /* kernel trace active */
 
 /* as above, but as bit values */
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
@@ -132,8 +133,9 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
 #define _TIF_FREEZE             (1<<TIF_FREEZE)
 #define _TIF_IRQ_SYNC           (1<<TIF_IRQ_SYNC)
+#define _TIF_KERNEL_TRACE       (1<<TIF_KERNEL_TRACE)
 
-#define _TIF_WORK_MASK		0x0000FFFE	/* work to do on interrupt/exception return */
+#define _TIF_WORK_MASK		0x0000FEFE	/* work to do on interrupt/exception return */
 
 #endif				/* __KERNEL__ */
 
diff --git a/stblinux-2.6.31/arch/cris/include/asm/thread_info.h b/stblinux-2.6.31/arch/cris/include/asm/thread_info.h
index c3aade3..905f165 100644
--- a/stblinux-2.6.31/arch/cris/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/cris/include/asm/thread_info.h
@@ -83,6 +83,7 @@ struct thread_info {
 #define TIF_NOTIFY_RESUME	1	/* resumption notification requested */
 #define TIF_SIGPENDING		2	/* signal pending */
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
+#define TIF_KERNEL_TRACE	4	/* kernel trace active */
 #define TIF_RESTORE_SIGMASK	9	/* restore signal mask in do_signal() */
 #define TIF_POLLING_NRFLAG	16	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_MEMDIE		17
@@ -92,12 +93,16 @@ struct thread_info {
 #define _TIF_NOTIFY_RESUME	(1<<TIF_NOTIFY_RESUME)
 #define _TIF_SIGPENDING		(1<<TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
 
-#define _TIF_WORK_MASK		0x0000FFFE	/* work to do on interrupt/exception return */
-#define _TIF_ALLWORK_MASK	0x0000FFFF	/* work to do on any return to u-space */
+/* work to do on interrupt/exception return */
+#define _TIF_WORK_MASK		0x0000FFFF & \
+		(~_TIF_SYSCALL_TRACE | ~_TIF_KERNEL_TRACE)
+/* work to do on any return to u-space */
+#define _TIF_ALLWORK_MASK	0x0000FFFF
 
 #endif /* __KERNEL__ */
 
diff --git a/stblinux-2.6.31/arch/frv/include/asm/thread_info.h b/stblinux-2.6.31/arch/frv/include/asm/thread_info.h
index e608e05..4cf10b2 100644
--- a/stblinux-2.6.31/arch/frv/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/frv/include/asm/thread_info.h
@@ -112,6 +112,7 @@ register struct thread_info *__current_thread_info asm("gr15");
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
 #define TIF_SINGLESTEP		4	/* restore singlestep on return to user mode */
 #define TIF_RESTORE_SIGMASK	5	/* restore signal mask in do_signal() */
+#define TIF_KERNEL_TRACE	6	/* kernel trace active */
 #define TIF_POLLING_NRFLAG	16	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_MEMDIE		17	/* OOM killer killed process */
 #define TIF_FREEZE		18	/* freezing for suspend */
@@ -122,10 +123,11 @@ register struct thread_info *__current_thread_info asm("gr15");
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
 #define _TIF_RESTORE_SIGMASK	(1 << TIF_RESTORE_SIGMASK)
+#define _TIF_KERNEL_TRACE	(1 << TIF_KERNEL_TRACE)
 #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_FREEZE		(1 << TIF_FREEZE)
 
-#define _TIF_WORK_MASK		0x0000FFFE	/* work to do on interrupt/exception return */
+#define _TIF_WORK_MASK		0x0000FFBE	/* work to do on interrupt/exception return */
 #define _TIF_ALLWORK_MASK	0x0000FFFF	/* work to do on any return to u-space */
 
 /*
diff --git a/stblinux-2.6.31/arch/h8300/include/asm/thread_info.h b/stblinux-2.6.31/arch/h8300/include/asm/thread_info.h
index 8bbc8b0..4dd120f 100644
--- a/stblinux-2.6.31/arch/h8300/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/h8300/include/asm/thread_info.h
@@ -89,6 +89,7 @@ static inline struct thread_info *current_thread_info(void)
 					   TIF_NEED_RESCHED */
 #define TIF_MEMDIE		4
 #define TIF_RESTORE_SIGMASK	5	/* restore signal mask in do_signal() */
+#define TIF_KERNEL_TRACE	6	/* kernel trace active */
 #define TIF_FREEZE		16	/* is freezing for suspend */
 
 /* as above, but as bit values */
@@ -97,9 +98,10 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
 
-#define _TIF_WORK_MASK		0x0000FFFE	/* work to do on interrupt/exception return */
+#define _TIF_WORK_MASK		0x0000FFBE	/* work to do on interrupt/exception return */
 
 #endif /* __KERNEL__ */
 
diff --git a/stblinux-2.6.31/arch/ia64/include/asm/thread_info.h b/stblinux-2.6.31/arch/ia64/include/asm/thread_info.h
index 8ce2e38..f35caf3 100644
--- a/stblinux-2.6.31/arch/ia64/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/ia64/include/asm/thread_info.h
@@ -100,6 +100,7 @@ struct thread_info {
 #define TIF_SYSCALL_TRACE	2	/* syscall trace active */
 #define TIF_SYSCALL_AUDIT	3	/* syscall auditing active */
 #define TIF_SINGLESTEP		4	/* restore singlestep on return to user mode */
+#define TIF_KERNEL_TRACE	5	/* kernel trace active */
 #define TIF_NOTIFY_RESUME	6	/* resumption notification requested */
 #define TIF_POLLING_NRFLAG	16	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_MEMDIE		17
@@ -111,7 +112,9 @@ struct thread_info {
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
-#define _TIF_SYSCALL_TRACEAUDIT	(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_SINGLESTEP)
+#define _TIF_KERNEL_TRACE	(1 << TIF_KERNEL_TRACE)
+#define _TIF_SYSCALL_TRACEAUDIT	(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|\
+	_TIF_SINGLESTEP|_TIF_KERNEL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
@@ -124,8 +127,9 @@ struct thread_info {
 /* "work to do on user-return" bits */
 #define TIF_ALLWORK_MASK	(_TIF_SIGPENDING|_TIF_NOTIFY_RESUME|_TIF_SYSCALL_AUDIT|\
 				 _TIF_NEED_RESCHED|_TIF_SYSCALL_TRACE)
-/* like TIF_ALLWORK_BITS but sans TIF_SYSCALL_TRACE or TIF_SYSCALL_AUDIT */
-#define TIF_WORK_MASK		(TIF_ALLWORK_MASK&~(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT))
+/* like TIF_ALLWORK_BITS but sans TIF_SYSCALL_TRACE, TIF_KERNEL_TRACE or TIF_SYSCALL_AUDIT */
+#define TIF_WORK_MASK		(TIF_ALLWORK_MASK&~(_TIF_SYSCALL_TRACE|_TIF_KERNEL_TRACE|\
+					_TIF_SYSCALL_AUDIT))
 
 #define TS_POLLING		1 	/* true if in idle loop and not sleeping */
 #define TS_RESTORE_SIGMASK	2	/* restore signal mask in do_signal() */
diff --git a/stblinux-2.6.31/arch/ia64/kernel/entry.S b/stblinux-2.6.31/arch/ia64/kernel/entry.S
index d0e7d37..2aae77d 100644
--- a/stblinux-2.6.31/arch/ia64/kernel/entry.S
+++ b/stblinux-2.6.31/arch/ia64/kernel/entry.S
@@ -635,9 +635,11 @@ GLOBAL_ENTRY(ia64_ret_from_clone)
 	;;
 	ld4 r2=[r2]
 	;;
+	movl r8=_TIF_SYSCALL_TRACEAUDIT
+	;;					// added stop bits to prevent r8 dependency
+	and r2=r8,r2
 	mov r8=0
-	and r2=_TIF_SYSCALL_TRACEAUDIT,r2
-	;;
+	;;					// added stop bits to prevent r2 dependency
 	cmp.ne p6,p0=r2,r0
 (p6)	br.cond.spnt .strace_check_retval
 	;;					// added stop bits to prevent r8 dependency
diff --git a/stblinux-2.6.31/arch/m32r/include/asm/thread_info.h b/stblinux-2.6.31/arch/m32r/include/asm/thread_info.h
index 07bb5bd..60bf12f 100644
--- a/stblinux-2.6.31/arch/m32r/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/m32r/include/asm/thread_info.h
@@ -149,6 +149,7 @@ static inline unsigned int get_thread_fault_code(void)
 #define TIF_NEED_RESCHED	2	/* rescheduling necessary */
 #define TIF_SINGLESTEP		3	/* restore singlestep on return to user mode */
 #define TIF_IRET		4	/* return with iret */
+#define TIF_KERNEL_TRACE	5	/* kernel trace active */
 #define TIF_RESTORE_SIGMASK	8	/* restore signal mask in do_signal() */
 #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
 #define TIF_POLLING_NRFLAG	17	/* true if poll_idle() is polling TIF_NEED_RESCHED */
@@ -160,13 +161,17 @@ static inline unsigned int get_thread_fault_code(void)
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_SINGLESTEP		(1<<TIF_SINGLESTEP)
 #define _TIF_IRET		(1<<TIF_IRET)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
 #define _TIF_USEDFPU		(1<<TIF_USEDFPU)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
 
-#define _TIF_WORK_MASK		0x0000FFFE	/* work to do on interrupt/exception return */
-#define _TIF_ALLWORK_MASK	0x0000FFFF	/* work to do on any return to u-space */
+/* work to do on any return to u-space */
+#define _TIF_ALLWORK_MASK	0x0000FFFF
+/* work to do on interrupt/exception return */
+#define _TIF_WORK_MASK \
+	(0x0000FFFF & ~(_TIF_SYSCALL_TRACE | _TIF_KERNEL_TRACE))
 
 /*
  * Thread-synchronous status.
diff --git a/stblinux-2.6.31/arch/m68k/include/asm/thread_info_mm.h b/stblinux-2.6.31/arch/m68k/include/asm/thread_info_mm.h
index 6ea5c33..27e2628 100644
--- a/stblinux-2.6.31/arch/m68k/include/asm/thread_info_mm.h
+++ b/stblinux-2.6.31/arch/m68k/include/asm/thread_info_mm.h
@@ -50,6 +50,7 @@ struct thread_info {
  */
 #define TIF_SIGPENDING		6	/* signal pending */
 #define TIF_NEED_RESCHED	7	/* rescheduling necessary */
+#define TIF_KERNEL_TRACE	13	/* kernel trace active */
 #define TIF_DELAYED_TRACE	14	/* single step a syscall */
 #define TIF_SYSCALL_TRACE	15	/* syscall trace active */
 #define TIF_MEMDIE		16
diff --git a/stblinux-2.6.31/arch/m68k/include/asm/thread_info_no.h b/stblinux-2.6.31/arch/m68k/include/asm/thread_info_no.h
index c2bde5e..6fe2d55 100644
--- a/stblinux-2.6.31/arch/m68k/include/asm/thread_info_no.h
+++ b/stblinux-2.6.31/arch/m68k/include/asm/thread_info_no.h
@@ -85,6 +85,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_POLLING_NRFLAG	3	/* true if poll_idle() is polling
 					   TIF_NEED_RESCHED */
 #define TIF_MEMDIE		4
+#define TIF_KERNEL_TRACE	5	/* kernel trace active */
 #define TIF_FREEZE		16	/* is freezing for suspend */
 
 /* as above, but as bit values */
@@ -93,8 +94,9 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 
-#define _TIF_WORK_MASK		0x0000FFFE	/* work to do on interrupt/exception return */
+#define _TIF_WORK_MASK		0x0000FFDE	/* work to do on interrupt/exception return */
 
 #endif /* __KERNEL__ */
 
diff --git a/stblinux-2.6.31/arch/mips/Kconfig b/stblinux-2.6.31/arch/mips/Kconfig
index 145955e..08eddd6 100644
--- a/stblinux-2.6.31/arch/mips/Kconfig
+++ b/stblinux-2.6.31/arch/mips/Kconfig
@@ -1763,6 +1763,14 @@ config CPU_R4000_WORKAROUNDS
 config CPU_R4400_WORKAROUNDS
 	bool
 
+config HAVE_GET_CYCLES_32
+	def_bool y
+	depends on !CPU_R4400_WORKAROUNDS
+	select HAVE_TRACE_CLOCK
+	select HAVE_TRACE_CLOCK_32_TO_64
+	select HAVE_UNSYNCHRONIZED_TSC
+	select SYNC_R4K
+
 #
 # Use the generic interrupt handling code in kernel/irq/:
 #
diff --git a/stblinux-2.6.31/arch/mips/include/asm/barrier.h b/stblinux-2.6.31/arch/mips/include/asm/barrier.h
index 8e9ac31..e787634 100644
--- a/stblinux-2.6.31/arch/mips/include/asm/barrier.h
+++ b/stblinux-2.6.31/arch/mips/include/asm/barrier.h
@@ -152,4 +152,10 @@
 #define smp_llsc_rmb()	__asm__ __volatile__(__WEAK_LLSC_MB : : :"memory")
 #define smp_llsc_wmb()	__asm__ __volatile__(__WEAK_LLSC_MB : : :"memory")
 
+/*
+ * MIPS does not have any instruction to serialize instruction execution on the
+ * core.
+ */
+#define sync_core()
+
 #endif /* __ASM_BARRIER_H */
diff --git a/stblinux-2.6.31/arch/mips/include/asm/mipsregs.h b/stblinux-2.6.31/arch/mips/include/asm/mipsregs.h
index 608dc97..121439c 100644
--- a/stblinux-2.6.31/arch/mips/include/asm/mipsregs.h
+++ b/stblinux-2.6.31/arch/mips/include/asm/mipsregs.h
@@ -419,6 +419,7 @@
  */
 #define  CAUSEB_EXCCODE		2
 #define  CAUSEF_EXCCODE		(_ULCAST_(31)  <<  2)
+#define  CAUSE_EXCCODE(cause)	(((cause) & CAUSEF_EXCCODE) >> CAUSEB_EXCCODE)
 #define  CAUSEB_IP		8
 #define  CAUSEF_IP		(_ULCAST_(255) <<  8)
 #define  CAUSEB_IP0		8
diff --git a/stblinux-2.6.31/arch/mips/include/asm/thread_info.h b/stblinux-2.6.31/arch/mips/include/asm/thread_info.h
index f9df720..72756cf 100644
--- a/stblinux-2.6.31/arch/mips/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/mips/include/asm/thread_info.h
@@ -126,6 +126,7 @@ register struct thread_info *__current_thread_info __asm__("$28");
 #define TIF_32BIT_ADDR		23	/* 32-bit address space (o32/n32) */
 #define TIF_FPUBOUND		24	/* thread bound to FPU-full CPU set */
 #define TIF_LOAD_WATCH		25	/* If set, load watch registers */
+#define TIF_KERNEL_TRACE	30	/* kernel trace active */
 #define TIF_SYSCALL_TRACE	31	/* syscall trace active */
 
 #ifdef CONFIG_MIPS32_O32
@@ -135,6 +136,7 @@ register struct thread_info *__current_thread_info __asm__("$28");
 #endif /* CONFIG_MIPS32_O32 */
 
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_SIGPENDING		(1<<TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
@@ -153,7 +155,7 @@ register struct thread_info *__current_thread_info __asm__("$28");
 /* work to do on interrupt/exception return */
 #define _TIF_WORK_MASK		(0x0000ffef & ~_TIF_SECCOMP)
 /* work to do on any return to u-space */
-#define _TIF_ALLWORK_MASK	(0x8000ffff & ~_TIF_SECCOMP)
+#define _TIF_ALLWORK_MASK	(0xc000ffff & ~_TIF_SECCOMP)
 
 #endif /* __KERNEL__ */
 
diff --git a/stblinux-2.6.31/arch/mips/include/asm/timex.h b/stblinux-2.6.31/arch/mips/include/asm/timex.h
index 6529704..10c8dd8 100644
--- a/stblinux-2.6.31/arch/mips/include/asm/timex.h
+++ b/stblinux-2.6.31/arch/mips/include/asm/timex.h
@@ -20,6 +20,8 @@
  */
 #define CLOCK_TICK_RATE 1193182
 
+extern unsigned int mips_hpt_frequency;
+
 /*
  * Standard way to access the cycle counter.
  * Currently only used on SMP for scheduling.
@@ -29,14 +31,80 @@
  * which isn't an evil thing.
  *
  * We know that all SMP capable CPUs have cycle counters.
+ *
+ * Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * HAVE_GET_CYCLES makes sure that this case is handled properly :
+ *
+ * Ralf Baechle <ralf@linux-mips.org> :
+ * This avoids us executing an mfc0 c0_count instruction on processors which
+ * don't have but also on certain R4000 and R4400 versions where reading from
+ * the count register just in the very moment when its value equals c0_compare
+ * will result in the timer interrupt getting lost.
  */
 
 typedef unsigned int cycles_t;
 
+#ifdef CONFIG_HAVE_GET_CYCLES_32
+static inline cycles_t get_cycles(void)
+{
+	return read_c0_count();
+}
+
+static inline void get_cycles_barrier(void)
+{
+}
+
+static inline cycles_t get_cycles_rate(void)
+{
+	return mips_hpt_frequency;
+}
+
+extern int test_tsc_synchronization(void);
+extern int _tsc_is_sync;
+static inline int tsc_is_sync(void)
+{
+	return _tsc_is_sync;
+}
+#else
 static inline cycles_t get_cycles(void)
 {
 	return 0;
 }
+static inline int test_tsc_synchronization(void)
+{
+	return 0;
+}
+static inline int tsc_is_sync(void)
+{
+	return 0;
+}
+#endif
+
+#define DELAY_INTERRUPT 100
+/*
+ * Only updates 32 LSB.
+ */
+static inline void write_tsc(u32 val1, u32 val2)
+{
+	write_c0_count(val1);
+	/* Arrange for an interrupt in a short while */
+	write_c0_compare(read_c0_count() + DELAY_INTERRUPT);
+}
+
+/*
+ * Currently unused, should update internal tsc-related timekeeping sources.
+ */
+static inline void mark_tsc_unstable(char *reason)
+{
+}
+
+/*
+ * Currently simply use the tsc_is_sync value.
+ */
+static inline int unsynchronized_tsc(void)
+{
+	return !tsc_is_sync();
+}
 
 #endif /* __KERNEL__ */
 
diff --git a/stblinux-2.6.31/arch/mips/include/asm/trace-clock.h b/stblinux-2.6.31/arch/mips/include/asm/trace-clock.h
new file mode 100644
index 0000000..722b5ac
--- /dev/null
+++ b/stblinux-2.6.31/arch/mips/include/asm/trace-clock.h
@@ -0,0 +1,70 @@
+/*
+ * Copyright (C) 2005,2008 Mathieu Desnoyers
+ *
+ * Trace clock MIPS definitions.
+ */
+
+#ifndef _ASM_MIPS_TRACE_CLOCK_H
+#define _ASM_MIPS_TRACE_CLOCK_H
+
+#include <linux/timex.h>
+#include <asm/processor.h>
+
+#define TRACE_CLOCK_MIN_PROBE_DURATION 200
+
+/*
+ * Number of hardware clock bits. The higher order bits are expected to be 0.
+ * If the hardware clock source has more than 32 bits, the bits higher than the
+ * 32nd will be truncated by a cast to a 32 bits unsigned. Range : 1 - 32.
+ * (too few bits would be unrealistic though, since we depend on the timer to
+ * detect the overflows).
+ */
+#define TC_HW_BITS			32
+
+/* Expected maximum interrupt latency in ms : 15ms, *2 for security */
+#define TC_EXPECTED_INTERRUPT_LATENCY	30
+
+extern u64 trace_clock_read_synthetic_tsc(void);
+
+/*
+ * MIPS get_cycles only returns a 32 bits TSC (see timex.h). The assumption
+ * there is that the reschedule is done every 8 seconds or so. Given that
+ * tracing needs to detect delays longer than 8 seconds, we need a full 64-bits
+ * TSC, whic is provided by trace-clock-32-to-64.
+*/
+
+static inline u32 trace_clock_read32(void)
+{
+	return (u32)get_cycles(); /* only need the 32 LSB */
+}
+
+static inline u64 trace_clock_read64(void)
+{
+	return trace_clock_read_synthetic_tsc();
+}
+
+static inline u64 trace_clock_frequency(void)
+{
+	return get_cycles_rate();
+}
+
+static inline u32 trace_clock_freq_scale(void)
+{
+	return 1;
+}
+
+extern void get_synthetic_tsc(void);
+extern void put_synthetic_tsc(void);
+
+static inline void get_trace_clock(void) {
+	get_synthetic_tsc();
+}
+
+static inline void put_trace_clock(void) {
+	put_synthetic_tsc();
+}
+
+static inline void set_trace_clock_is_sync(int state)
+{
+}
+#endif /* _ASM_MIPS_TRACE_CLOCK_H */
diff --git a/stblinux-2.6.31/arch/mips/kernel/Makefile b/stblinux-2.6.31/arch/mips/kernel/Makefile
index 1546354..95c5695 100644
--- a/stblinux-2.6.31/arch/mips/kernel/Makefile
+++ b/stblinux-2.6.31/arch/mips/kernel/Makefile
@@ -88,6 +88,8 @@ obj-$(CONFIG_GPIO_TXX9)		+= gpio_txx9.o
 obj-$(CONFIG_KEXEC)		+= machine_kexec.o relocate_kernel.o
 obj-$(CONFIG_EARLY_PRINTK)	+= early_printk.o
 
+# obj-$(CONFIG_HAVE_TRACE_CLOCK)	+= trace-clock.o
+
 CFLAGS_cpu-bugs64.o	= $(shell if $(CC) $(KBUILD_CFLAGS) -Wa,-mdaddi -c -o /dev/null -xc /dev/null >/dev/null 2>&1; then echo "-DHAVE_AS_SET_DADDI"; fi)
 
 obj-$(CONFIG_HAVE_STD_PC_SERIAL_PORT)	+= 8250-platform.o
diff --git a/stblinux-2.6.31/arch/mips/kernel/entry.S b/stblinux-2.6.31/arch/mips/kernel/entry.S
index ffa3310..8c5410f 100644
--- a/stblinux-2.6.31/arch/mips/kernel/entry.S
+++ b/stblinux-2.6.31/arch/mips/kernel/entry.S
@@ -167,7 +167,7 @@ work_notifysig:				# deal with pending signals and
 FEXPORT(syscall_exit_work_partial)
 	SAVE_STATIC
 syscall_exit_work:
-	li	t0, _TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT
+	li	t0, _TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_KERNEL_TRACE
 	and	t0, a2			# a2 is preloaded with TI_FLAGS
 	beqz	t0, work_pending	# trace bit set?
 	local_irq_enable		# could let do_syscall_trace()
diff --git a/stblinux-2.6.31/arch/mips/kernel/process.c b/stblinux-2.6.31/arch/mips/kernel/process.c
index f3d73e1..6aff906 100644
--- a/stblinux-2.6.31/arch/mips/kernel/process.c
+++ b/stblinux-2.6.31/arch/mips/kernel/process.c
@@ -26,6 +26,7 @@
 #include <linux/completion.h>
 #include <linux/kallsyms.h>
 #include <linux/random.h>
+#include <trace/sched.h>
 
 #include <asm/asm.h>
 #include <asm/bootinfo.h>
@@ -43,6 +44,8 @@
 #include <asm/inst.h>
 #include <asm/stacktrace.h>
 
+DEFINE_TRACE(sched_kthread_create);
+
 /*
  * The idle thread. There's no useful work to be done, so just try to conserve
  * power and have a low exit latency (ie sit in a loop waiting for somebody to
@@ -231,6 +234,7 @@ static void __noreturn kernel_thread_helper(void *arg, int (*fn)(void *))
 long kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 {
 	struct pt_regs regs;
+	long pid;
 
 	memset(&regs, 0, sizeof(regs));
 
@@ -246,7 +250,10 @@ long kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 #endif
 
 	/* Ok, create the new process.. */
-	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+	pid = do_fork(flags | CLONE_VM | CLONE_UNTRACED,
+			0, &regs, 0, NULL, NULL);
+	trace_sched_kthread_create(fn, pid);
+	return pid;
 }
 
 /*
diff --git a/stblinux-2.6.31/arch/mips/kernel/ptrace.c b/stblinux-2.6.31/arch/mips/kernel/ptrace.c
index 054861c..6af6424 100644
--- a/stblinux-2.6.31/arch/mips/kernel/ptrace.c
+++ b/stblinux-2.6.31/arch/mips/kernel/ptrace.c
@@ -25,6 +25,7 @@
 #include <linux/security.h>
 #include <linux/audit.h>
 #include <linux/seccomp.h>
+#include <trace/syscall.h>
 
 #include <asm/byteorder.h>
 #include <asm/cpu.h>
@@ -39,6 +40,9 @@
 #include <asm/bootinfo.h>
 #include <asm/reg.h>
 
+DEFINE_TRACE(syscall_entry);
+DEFINE_TRACE(syscall_exit);
+
 /*
  * Called by kernel/ptrace.c when detaching..
  *
@@ -564,6 +568,11 @@ static inline int audit_arch(void)
  */
 asmlinkage void do_syscall_trace(struct pt_regs *regs, int entryexit)
 {
+	if (!entryexit)
+		trace_syscall_entry(regs, regs->regs[2]);
+	else
+		trace_syscall_exit(regs->regs[2]);
+
 	/* do the secure computing check first */
 	if (!entryexit)
 		secure_computing(regs->regs[0]);
diff --git a/stblinux-2.6.31/arch/mips/kernel/smp.c b/stblinux-2.6.31/arch/mips/kernel/smp.c
index 64668a9..4055896 100644
--- a/stblinux-2.6.31/arch/mips/kernel/smp.c
+++ b/stblinux-2.6.31/arch/mips/kernel/smp.c
@@ -173,6 +173,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 {
 	mp_ops->cpus_done();
 	synchronise_count_master();
+	test_tsc_synchronization();
 }
 
 /* called from main before smp_init() */
diff --git a/stblinux-2.6.31/arch/mips/kernel/syscall.c b/stblinux-2.6.31/arch/mips/kernel/syscall.c
index 0d5db6e..2b1873a 100644
--- a/stblinux-2.6.31/arch/mips/kernel/syscall.c
+++ b/stblinux-2.6.31/arch/mips/kernel/syscall.c
@@ -28,6 +28,7 @@
 #include <linux/compiler.h>
 #include <linux/module.h>
 #include <linux/ipc.h>
+#include <trace/ipc.h>
 
 #include <asm/branch.h>
 #include <asm/cachectl.h>
@@ -39,6 +40,8 @@
 #include <asm/sysmips.h>
 #include <asm/uaccess.h>
 
+DEFINE_TRACE(ipc_call);
+
 /*
  * For historic reasons the pipe(2) syscall on MIPS has an unusual calling
  * convention.  It returns results in registers $v0 / $v1 which means there
@@ -334,6 +337,8 @@ SYSCALL_DEFINE6(ipc, unsigned int, call, int, first, int, second,
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_ipc_call(call, first);
+
 	switch (call) {
 	case SEMOP:
 		return sys_semtimedop(first, (struct sembuf __user *)ptr,
diff --git a/stblinux-2.6.31/arch/mips/kernel/time.c b/stblinux-2.6.31/arch/mips/kernel/time.c
index 1f467d5..75e45b5 100644
--- a/stblinux-2.6.31/arch/mips/kernel/time.c
+++ b/stblinux-2.6.31/arch/mips/kernel/time.c
@@ -70,6 +70,7 @@ EXPORT_SYMBOL(perf_irq);
  */
 
 unsigned int mips_hpt_frequency;
+EXPORT_SYMBOL(mips_hpt_frequency);
 
 void __init clocksource_set_clock(struct clocksource *cs, unsigned int clock)
 {
diff --git a/stblinux-2.6.31/arch/mips/kernel/trace-clock.c b/stblinux-2.6.31/arch/mips/kernel/trace-clock.c
new file mode 100644
index 0000000..42e27b8
--- /dev/null
+++ b/stblinux-2.6.31/arch/mips/kernel/trace-clock.c
@@ -0,0 +1,171 @@
+/*
+ * arch/mips/kernel/trace-clock.c
+ *
+ * Trace clock for mips.
+ *
+ * Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>, October 2008
+ */
+
+#include <linux/module.h>
+#include <linux/trace-clock.h>
+#include <linux/jiffies.h>
+#include <linux/timer.h>
+#include <linux/spinlock.h>
+
+static u64 trace_clock_last_tsc;
+static DEFINE_PER_CPU(struct timer_list, update_timer);
+static DEFINE_SPINLOCK(async_tsc_lock);
+static int async_tsc_refcount;	/* Number of readers */
+static int async_tsc_enabled;	/* Async TSC enabled on all online CPUs */
+
+#if (BITS_PER_LONG == 64)
+static inline u64 trace_clock_cmpxchg64(u64 *ptr, u64 old, u64 new)
+{
+	return cmpxchg64(ptr, old, new);
+}
+#else
+/*
+ * Emulate an atomic 64-bits update with a spinlock.
+ * Note : preempt_disable or irq save must be explicit with raw_spinlock_t.
+ * Given we use a spinlock for this time base, we should never be called from
+ * NMI context.
+ */
+static raw_spinlock_t trace_clock_lock =
+	(raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
+
+/*
+ * Must be called under irqoff+spinlock on MIPS32.
+ */
+static inline u64 trace_clock_cmpxchg64(u64 *ptr, u64 old, u64 new)
+{
+	u64 val;
+
+	val = *ptr;
+	if (likely(val == old))
+		*ptr = new;
+	return val;
+}
+#endif
+
+/*
+ * Must be called under irqoff+spinlock on MIPS32.
+ */
+static cycles_t read_last_tsc(void)
+{
+	return trace_clock_last_tsc;
+}
+
+/*
+ * Support for architectures with non-sync TSCs.
+ * When the local TSC is discovered to lag behind the highest TSC counter, we
+ * increment the TSC count of an amount that should be, ideally, lower than the
+ * execution time of this routine, in cycles : this is the granularity we look
+ * for : we must be able to order the events.
+ *
+ * MIPS32 does not have atomic 64-bit updates. Emulate it with irqoff+spinlock.
+ */
+
+notrace u64 trace_clock_async_tsc_read(void)
+{
+	u64 new_tsc, last_tsc;
+#if (BITS_PER_LONG == 32)
+	unsigned long flags;
+
+	local_irq_save(flags);
+	__raw_spin_lock(&trace_clock_lock);
+#endif
+
+	WARN_ON(!async_tsc_refcount || !async_tsc_enabled);
+	new_tsc = trace_clock_read_synthetic_tsc();
+	barrier();
+	last_tsc = read_last_tsc();
+	do {
+		if (new_tsc < last_tsc)
+			new_tsc = last_tsc + TRACE_CLOCK_MIN_PROBE_DURATION;
+		/*
+		 * If cmpxchg fails with a value higher than the new_tsc, don't
+		 * retry : the value has been incremented and the events
+		 * happened almost at the same time.
+		 * We must retry if cmpxchg fails with a lower value :
+		 * it means that we are the CPU with highest frequency and
+		 * therefore MUST update the value.
+		 */
+		last_tsc = trace_clock_cmpxchg64(&trace_clock_last_tsc,
+						 last_tsc, new_tsc);
+	} while (unlikely(last_tsc < new_tsc));
+#if (BITS_PER_LONG == 32)
+	__raw_spin_unlock(&trace_clock_lock);
+	local_irq_restore(flags);
+#endif
+	return new_tsc;
+}
+EXPORT_SYMBOL_GPL(trace_clock_async_tsc_read);
+
+static void update_timer_ipi(void *info)
+{
+	(void)trace_clock_async_tsc_read();
+}
+
+/*
+ * update_timer_fct : - Timer function to resync the clocks
+ * @data: unused
+ *
+ * Fires every jiffy.
+ */
+static void update_timer_fct(unsigned long data)
+{
+	(void)trace_clock_async_tsc_read();
+
+	per_cpu(update_timer, smp_processor_id()).expires = jiffies + 1;
+	add_timer_on(&per_cpu(update_timer, smp_processor_id()),
+		     smp_processor_id());
+}
+
+static void enable_trace_clock(int cpu)
+{
+	init_timer(&per_cpu(update_timer, cpu));
+	per_cpu(update_timer, cpu).function = update_timer_fct;
+	per_cpu(update_timer, cpu).expires = jiffies + 1;
+	smp_call_function_single(cpu, update_timer_ipi, NULL, 1);
+	add_timer_on(&per_cpu(update_timer, cpu), cpu);
+}
+
+static void disable_trace_clock(int cpu)
+{
+	del_timer_sync(&per_cpu(update_timer, cpu));
+}
+
+void get_trace_clock(void)
+{
+	int cpu;
+
+	spin_lock(&async_tsc_lock);
+	if (async_tsc_refcount++ || tsc_is_sync()) {
+		get_synthetic_tsc();
+	} else {
+		async_tsc_enabled = 1;
+		get_synthetic_tsc();
+		for_each_online_cpu(cpu)
+			enable_trace_clock(cpu);
+	}
+	spin_unlock(&async_tsc_lock);
+}
+EXPORT_SYMBOL_GPL(get_trace_clock);
+
+void put_trace_clock(void)
+{
+	int cpu;
+
+	spin_lock(&async_tsc_lock);
+	WARN_ON(async_tsc_refcount <= 0);
+	if (async_tsc_refcount != 1 || !async_tsc_enabled) {
+		put_synthetic_tsc();
+	} else {
+		for_each_online_cpu(cpu)
+			disable_trace_clock(cpu);
+		async_tsc_enabled = 0;
+	}
+	async_tsc_refcount--;
+	spin_unlock(&async_tsc_lock);
+}
+EXPORT_SYMBOL_GPL(put_trace_clock);
diff --git a/stblinux-2.6.31/arch/mips/kernel/traps.c b/stblinux-2.6.31/arch/mips/kernel/traps.c
index e3c4a98..df449bf 100644
--- a/stblinux-2.6.31/arch/mips/kernel/traps.c
+++ b/stblinux-2.6.31/arch/mips/kernel/traps.c
@@ -25,6 +25,7 @@
 #include <linux/ptrace.h>
 #include <linux/kgdb.h>
 #include <linux/kdebug.h>
+#include <trace/trap.h>
 
 #include <asm/bootinfo.h>
 #include <asm/branch.h>
@@ -54,6 +55,12 @@
 #include <asm/brcmstb/brcmapi.h>
 #endif
 
+/*
+ * Also used in unaligned.c and fault.c.
+ */
+DEFINE_TRACE(trap_entry);
+DEFINE_TRACE(trap_exit);
+
 extern void check_wait(void);
 extern asmlinkage void r4k_wait(void);
 extern asmlinkage void rollback_handle_int(void);
@@ -318,7 +325,7 @@ static void __show_regs(const struct pt_regs *regs)
 
 	printk("Cause : %08x\n", cause);
 
-	cause = (cause & CAUSEF_EXCCODE) >> CAUSEB_EXCCODE;
+	cause = CAUSE_EXCCODE(cause);
 	if (1 <= cause && cause <= 5)
 		printk("BadVA : %0*lx\n", field, regs->cp0_badvaddr);
 
@@ -658,6 +665,7 @@ asmlinkage void do_fpe(struct pt_regs *regs, unsigned long fcr31)
 		return;
 	die_if_kernel("FP exception in kernel code", regs);
 
+	trace_trap_entry(regs, CAUSE_EXCCODE(regs->cp0_cause));
 	if (fcr31 & FPU_CSR_UNI_X) {
 		int sig;
 
@@ -689,7 +697,7 @@ asmlinkage void do_fpe(struct pt_regs *regs, unsigned long fcr31)
 		/* If something went wrong, signal */
 		if (sig)
 			force_sig(sig, current);
-
+		trace_trap_exit();
 		return;
 	} else if (fcr31 & FPU_CSR_INV_X)
 		info.si_code = FPE_FLTINV;
@@ -707,6 +715,7 @@ asmlinkage void do_fpe(struct pt_regs *regs, unsigned long fcr31)
 	info.si_errno = 0;
 	info.si_addr = (void __user *) regs->cp0_epc;
 	force_sig_info(SIGFPE, &info, current);
+	trace_trap_exit();
 }
 
 static void do_trap_or_bp(struct pt_regs *regs, unsigned int code,
@@ -881,6 +890,8 @@ asmlinkage void do_cpu(struct pt_regs *regs)
 	int status;
 	unsigned long __maybe_unused flags;
 
+	trace_trap_entry(regs, CAUSE_EXCCODE(regs->cp0_cause));
+
 	die_if_kernel("do_cpu invoked from kernel context!", regs);
 
 	cpid = (regs->cp0_cause >> CAUSEB_CE) & 3;
@@ -892,8 +903,10 @@ asmlinkage void do_cpu(struct pt_regs *regs)
 		opcode = 0;
 		status = -1;
 
-		if (unlikely(compute_return_epc(regs) < 0))
+		if (unlikely(compute_return_epc(regs) < 0)) {
+			trace_trap_exit();
 			return;
+		}
 
 		if (unlikely(get_user(opcode, epc) < 0))
 			status = SIGSEGV;
@@ -911,7 +924,7 @@ asmlinkage void do_cpu(struct pt_regs *regs)
 			regs->cp0_epc = old_epc;	/* Undo skip-over.  */
 			force_sig(status, current);
 		}
-
+		trace_trap_exit();
 		return;
 
 	case 1:
@@ -931,7 +944,7 @@ asmlinkage void do_cpu(struct pt_regs *regs)
 			else
 				mt_ase_fp_affinity();
 		}
-
+		trace_trap_exit();
 		return;
 
 	case 2:
@@ -951,6 +964,7 @@ asmlinkage void do_cpu(struct pt_regs *regs)
 	}
 
 	force_sig(SIGILL, current);
+	trace_trap_exit();
 }
 
 asmlinkage void do_mdmx(struct pt_regs *regs)
diff --git a/stblinux-2.6.31/arch/mips/kernel/unaligned.c b/stblinux-2.6.31/arch/mips/kernel/unaligned.c
index 21abd64..1c749c3 100644
--- a/stblinux-2.6.31/arch/mips/kernel/unaligned.c
+++ b/stblinux-2.6.31/arch/mips/kernel/unaligned.c
@@ -78,6 +78,7 @@
 #include <linux/smp.h>
 #include <linux/sched.h>
 #include <linux/debugfs.h>
+#include <trace/trap.h>
 #include <asm/asm.h>
 #include <asm/branch.h>
 #include <asm/byteorder.h>
@@ -517,6 +518,8 @@ asmlinkage void do_ade(struct pt_regs *regs)
 	unsigned int __user *pc;
 	mm_segment_t seg;
 
+	trace_trap_entry(regs, CAUSE_EXCCODE(regs->cp0_cause));
+
 	/*
 	 * Did we catch a fault trying to load an instruction?
 	 * Or are we running in MIPS16 mode?
@@ -542,6 +545,8 @@ asmlinkage void do_ade(struct pt_regs *regs)
 	emulate_load_store_insn(regs, (void __user *)regs->cp0_badvaddr, pc);
 	set_fs(seg);
 
+	trace_trap_exit();
+
 	return;
 
 sigbus:
@@ -551,6 +556,8 @@ sigbus:
 	/*
 	 * XXX On return from the signal handler we should advance the epc
 	 */
+
+	trace_trap_exit();
 }
 
 #ifdef CONFIG_DEBUG_FS
diff --git a/stblinux-2.6.31/arch/mips/mm/fault.c b/stblinux-2.6.31/arch/mips/mm/fault.c
index f956ecb..3509e47 100644
--- a/stblinux-2.6.31/arch/mips/mm/fault.c
+++ b/stblinux-2.6.31/arch/mips/mm/fault.c
@@ -18,6 +18,7 @@
 #include <linux/smp.h>
 #include <linux/vt_kern.h>		/* For unblank_screen() */
 #include <linux/module.h>
+#include <trace/fault.h>
 
 #include <asm/branch.h>
 #include <asm/mmu_context.h>
@@ -26,6 +27,9 @@
 #include <asm/ptrace.h>
 #include <asm/highmem.h>		/* For VMALLOC_END */
 
+DEFINE_TRACE(page_fault_entry);
+DEFINE_TRACE(page_fault_exit);
+
 /*
  * This routine handles page faults.  It determines the address,
  * and the problem, and then passes it off to one of the appropriate
@@ -102,7 +106,10 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
+	trace_page_fault_entry(regs, CAUSE_EXCCODE(regs->cp0_cause), mm, vma,
+			       address, write);
 	fault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);
+	trace_page_fault_exit(fault);
 	if (unlikely(fault & VM_FAULT_ERROR)) {
 		if (fault & VM_FAULT_OOM)
 			goto out_of_memory;
diff --git a/stblinux-2.6.31/arch/parisc/include/asm/thread_info.h b/stblinux-2.6.31/arch/parisc/include/asm/thread_info.h
index 4ce0edf..0a2f4e6 100644
--- a/stblinux-2.6.31/arch/parisc/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/parisc/include/asm/thread_info.h
@@ -59,6 +59,7 @@ struct thread_info {
 #define TIF_MEMDIE		5
 #define TIF_RESTORE_SIGMASK	6	/* restore saved signal mask */
 #define TIF_FREEZE		7	/* is freezing for suspend */
+#define TIF_KERNEL_TRACE	8	/* kernel trace active */
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
@@ -67,6 +68,7 @@ struct thread_info {
 #define _TIF_32BIT		(1 << TIF_32BIT)
 #define _TIF_RESTORE_SIGMASK	(1 << TIF_RESTORE_SIGMASK)
 #define _TIF_FREEZE		(1 << TIF_FREEZE)
+#define _TIF_KERNEL_TRACE	(1 << TIF_KERNEL_TRACE)
 
 #define _TIF_USER_WORK_MASK     (_TIF_SIGPENDING | \
                                  _TIF_NEED_RESCHED | _TIF_RESTORE_SIGMASK)
diff --git a/stblinux-2.6.31/arch/powerpc/Kconfig b/stblinux-2.6.31/arch/powerpc/Kconfig
index d00131c..e9de4d9 100644
--- a/stblinux-2.6.31/arch/powerpc/Kconfig
+++ b/stblinux-2.6.31/arch/powerpc/Kconfig
@@ -116,6 +116,7 @@ config PPC
 	select HAVE_IOREMAP_PROT
 	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 	select HAVE_KPROBES
+	select HAVE_TRACE_CLOCK
 	select HAVE_ARCH_KGDB
 	select HAVE_KRETPROBES
 	select HAVE_ARCH_TRACEHOOK
@@ -126,6 +127,8 @@ config PPC
 	select HAVE_SYSCALL_WRAPPERS if PPC64
 	select GENERIC_ATOMIC64 if PPC32
 	select HAVE_PERF_COUNTERS
+	select HAVE_GET_CYCLES if PPC64
+	select HAVE_IMMEDIATE
 
 config EARLY_PRINTK
 	bool
diff --git a/stblinux-2.6.31/arch/powerpc/Makefile b/stblinux-2.6.31/arch/powerpc/Makefile
index 39d44f7..ec4be7b 100644
--- a/stblinux-2.6.31/arch/powerpc/Makefile
+++ b/stblinux-2.6.31/arch/powerpc/Makefile
@@ -96,6 +96,8 @@ else
 LDFLAGS_MODULE	+= arch/powerpc/lib/crtsavres.o
 endif
 
+export USE_IMMEDIATE := $(CONFIG_IMMEDIATE)
+
 ifeq ($(CONFIG_TUNE_CELL),y)
 	KBUILD_CFLAGS += $(call cc-option,-mtune=cell)
 endif
diff --git a/stblinux-2.6.31/arch/powerpc/include/asm/cacheflush.h b/stblinux-2.6.31/arch/powerpc/include/asm/cacheflush.h
index ba667a3..a52a565 100644
--- a/stblinux-2.6.31/arch/powerpc/include/asm/cacheflush.h
+++ b/stblinux-2.6.31/arch/powerpc/include/asm/cacheflush.h
@@ -63,7 +63,9 @@ extern void flush_dcache_phys_range(unsigned long start, unsigned long stop);
 #define copy_from_user_page(vma, page, vaddr, dst, src, len) \
 	memcpy(dst, src, len)
 
-
+#define text_poke	memcpy
+#define text_poke_early	text_poke
+#define sync_core()
 
 #ifdef CONFIG_DEBUG_PAGEALLOC
 /* internal debugging function */
diff --git a/stblinux-2.6.31/arch/powerpc/include/asm/immediate.h b/stblinux-2.6.31/arch/powerpc/include/asm/immediate.h
new file mode 100644
index 0000000..3209104
--- /dev/null
+++ b/stblinux-2.6.31/arch/powerpc/include/asm/immediate.h
@@ -0,0 +1,72 @@
+#ifndef _ASM_POWERPC_IMMEDIATE_H
+#define _ASM_POWERPC_IMMEDIATE_H
+
+/*
+ * Immediate values. PowerPC architecture optimizations.
+ *
+ * (C) Copyright 2006 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * Dual BSD/GPL v2 license.
+ */
+
+#include <asm/asm-compat.h>
+
+struct __imv {
+	unsigned long var;	/* Identifier variable of the immediate value */
+	unsigned long imv;	/*
+				 * Pointer to the memory location that holds
+				 * the immediate value within the load immediate
+				 * instruction.
+				 */
+	unsigned char size;	/* Type size. */
+} __attribute__ ((packed));
+
+/**
+ * imv_read - read immediate variable
+ * @name: immediate value name
+ *
+ * Reads the value of @name.
+ * Optimized version of the immediate.
+ * Do not use in __init and __exit functions. Use _imv_read() instead.
+ * Makes sure the 2 bytes update will be atomic by aligning the immediate
+ * value. Use a normal memory read for the 4 bytes immediate because there is no
+ * way to atomically update it without using a seqlock read side, which would
+ * cost more in term of total i-cache and d-cache space than a simple memory
+ * read.
+ */
+#define imv_read(name)							\
+	({								\
+		__typeof__(name##__imv) value;				\
+		BUILD_BUG_ON(sizeof(value) > 8);			\
+		switch (sizeof(value)) {				\
+		case 1:							\
+			asm(".section __imv,\"aw\",@progbits\n\t"	\
+					PPC_LONG "%c1, ((1f)-1)\n\t"	\
+					".byte 1\n\t"			\
+					".previous\n\t"			\
+					"li %0,0\n\t"			\
+					"1:\n\t"			\
+				: "=r" (value)				\
+				: "i" (&name##__imv));			\
+			break;						\
+		case 2:							\
+			asm(".section __imv,\"aw\",@progbits\n\t"	\
+					PPC_LONG "%c1, ((1f)-2)\n\t"	\
+					".byte 2\n\t"			\
+					".previous\n\t"			\
+					".align 2\n\t"			\
+					"li %0,0\n\t"			\
+					"1:\n\t"			\
+				: "=r" (value)				\
+				: "i" (&name##__imv));			\
+			break;						\
+		case 4:							\
+		case 8:	value = name##__imv;				\
+			break;						\
+		};							\
+		value;							\
+	})
+
+extern int arch_imv_update(const struct __imv *imv, int early);
+
+#endif /* _ASM_POWERPC_IMMEDIATE_H */
diff --git a/stblinux-2.6.31/arch/powerpc/include/asm/thread_info.h b/stblinux-2.6.31/arch/powerpc/include/asm/thread_info.h
index aa9d383..3cc0448 100644
--- a/stblinux-2.6.31/arch/powerpc/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/powerpc/include/asm/thread_info.h
@@ -100,7 +100,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_POLLING_NRFLAG	3	/* true if poll_idle() is polling
 					   TIF_NEED_RESCHED */
 #define TIF_32BIT		4	/* 32 bit binary */
-#define TIF_PERFMON_WORK	5	/* work for pfm_handle_work() */
+#define TIF_KERNEL_TRACE	5	/* kernel trace active */
 #define TIF_PERFMON_CTXSW	6	/* perfmon needs ctxsw calls */
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SINGLESTEP		8	/* singlestepping active */
@@ -111,6 +111,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_NOTIFY_RESUME	13	/* callback before returning to user */
 #define TIF_FREEZE		14	/* Freezing for suspend */
 #define TIF_RUNLATCH		15	/* Is the runlatch enabled? */
+#define TIF_PERFMON_WORK	17	/* work for pfm_handle_work() */
 
 /* as above, but as bit values */
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
@@ -118,7 +119,7 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
 #define _TIF_32BIT		(1<<TIF_32BIT)
-#define _TIF_PERFMON_WORK	(1<<TIF_PERFMON_WORK)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_PERFMON_CTXSW	(1<<TIF_PERFMON_CTXSW)
 #define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
 #define _TIF_SINGLESTEP		(1<<TIF_SINGLESTEP)
@@ -128,7 +129,8 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_NOTIFY_RESUME	(1<<TIF_NOTIFY_RESUME)
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
 #define _TIF_RUNLATCH		(1<<TIF_RUNLATCH)
-#define _TIF_SYSCALL_T_OR_A	(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_SECCOMP)
+#define _TIF_PERFMON_WORK	(1<<TIF_PERFMON_WORK)
+#define _TIF_SYSCALL_T_OR_A	(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_SECCOMP|_TIF_KERNEL_TRACE)
 
 #define _TIF_USER_WORK_MASK	(_TIF_SIGPENDING | _TIF_NEED_RESCHED | \
 				 _TIF_NOTIFY_RESUME)
diff --git a/stblinux-2.6.31/arch/powerpc/include/asm/timex.h b/stblinux-2.6.31/arch/powerpc/include/asm/timex.h
index c55e14f..2fe7460 100644
--- a/stblinux-2.6.31/arch/powerpc/include/asm/timex.h
+++ b/stblinux-2.6.31/arch/powerpc/include/asm/timex.h
@@ -14,6 +14,8 @@
 
 typedef unsigned long cycles_t;
 
+extern unsigned long tb_ticks_per_sec;
+
 static inline cycles_t get_cycles(void)
 {
 #ifdef __powerpc64__
@@ -46,5 +48,15 @@ static inline cycles_t get_cycles(void)
 #endif
 }
 
+static inline cycles_t get_cycles_rate(void)
+{
+	return tb_ticks_per_sec;
+}
+
+static inline void get_cycles_barrier(void)
+{
+	isync();
+}
+
 #endif	/* __KERNEL__ */
 #endif	/* _ASM_POWERPC_TIMEX_H */
diff --git a/stblinux-2.6.31/arch/powerpc/include/asm/trace-clock.h b/stblinux-2.6.31/arch/powerpc/include/asm/trace-clock.h
new file mode 100644
index 0000000..b0b4e21
--- /dev/null
+++ b/stblinux-2.6.31/arch/powerpc/include/asm/trace-clock.h
@@ -0,0 +1,47 @@
+/*
+ * Copyright (C) 2005,2008 Mathieu Desnoyers
+ *
+ * Trace clock PowerPC definitions.
+ *
+ * Use get_tb() directly to insure reading a 64-bits value on powerpc 32.
+ */
+
+#ifndef _ASM_TRACE_CLOCK_H
+#define _ASM_TRACE_CLOCK_H
+
+#include <linux/timex.h>
+#include <linux/time.h>
+#include <asm/time.h>
+
+static inline u32 trace_clock_read32(void)
+{
+	return get_tbl();
+}
+
+static inline u64 trace_clock_read64(void)
+{
+	return get_tb();
+}
+
+static inline unsigned int trace_clock_frequency(void)
+{
+	return get_cycles_rate();
+}
+
+static inline u32 trace_clock_freq_scale(void)
+{
+	return 1;
+}
+
+static inline void get_trace_clock(void)
+{
+}
+
+static inline void put_trace_clock(void)
+{
+}
+
+static inline void set_trace_clock_is_sync(int state)
+{
+}
+#endif /* _ASM_TRACE_CLOCK_H */
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/Makefile b/stblinux-2.6.31/arch/powerpc/kernel/Makefile
index b73396b..d7f03d1 100644
--- a/stblinux-2.6.31/arch/powerpc/kernel/Makefile
+++ b/stblinux-2.6.31/arch/powerpc/kernel/Makefile
@@ -62,6 +62,7 @@ obj64-$(CONFIG_HIBERNATION)	+= swsusp_asm64.o
 obj-$(CONFIG_MODULES)		+= module.o module_$(CONFIG_WORD_SIZE).o
 obj-$(CONFIG_44x)		+= cpu_setup_44x.o
 obj-$(CONFIG_FSL_BOOKE)		+= cpu_setup_fsl_booke.o dbell.o
+obj-$(USE_IMMEDIATE)		+= immediate.o
 
 extra-$(CONFIG_PPC_STD_MMU)	:= head_32.o
 extra-$(CONFIG_PPC64)		:= head_64.o
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/immediate.c b/stblinux-2.6.31/arch/powerpc/kernel/immediate.c
new file mode 100644
index 0000000..dfe8d60
--- /dev/null
+++ b/stblinux-2.6.31/arch/powerpc/kernel/immediate.c
@@ -0,0 +1,72 @@
+/*
+ * Powerpc optimized immediate values enabling/disabling.
+ *
+ * Copyright 2007-2009 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * Dual BSD/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/immediate.h>
+#include <linux/string.h>
+#include <linux/kprobes.h>
+#include <asm/cacheflush.h>
+#include <asm/page.h>
+
+#define LI_OPCODE_LEN	2
+
+/**
+ * arch_imv_update - update one immediate value
+ * @imv: pointer of type const struct __imv to update
+ * @early: early boot (1), normal (0)
+ *
+ * Update one immediate value. Must be called with imv_mutex held.
+ */
+int arch_imv_update(const struct __imv *imv, int early)
+{
+#ifdef CONFIG_KPROBES
+	kprobe_opcode_t *insn;
+	/*
+	 * Fail if a kprobe has been set on this instruction.
+	 * (TODO: we could eventually do better and modify all the (possibly
+	 * nested) kprobes for this site if kprobes had an API for this.
+	 */
+	switch (imv->size) {
+	case 1:	/* The uint8_t points to the 3rd byte of the
+		 * instruction */
+		insn = (void *)(imv->imv - 1 - LI_OPCODE_LEN);
+		break;
+	case 2:	insn = (void *)(imv->imv - LI_OPCODE_LEN);
+		break;
+	default:
+	return -EINVAL;
+	}
+
+	if (unlikely(!early && *insn == BREAKPOINT_INSTRUCTION)) {
+		printk(KERN_WARNING "Immediate value in conflict with kprobe. "
+				    "Variable at %p, "
+				    "instruction at %p, size %lu\n",
+				    (void *)imv->imv,
+				    (void *)imv->var, imv->size);
+		return -EBUSY;
+	}
+#endif
+
+	/*
+	 * If the variable and the instruction have the same value, there is
+	 * nothing to do.
+	 */
+	switch (imv->size) {
+	case 1:	if (*(uint8_t *)imv->imv == *(uint8_t *)imv->var)
+			return 0;
+		*(uint8_t *)imv->imv = *(uint8_t *)imv->var;
+		break;
+	case 2:	if (*(uint16_t *)imv->imv == *(uint16_t *)imv->var)
+			return 0;
+		*(uint16_t *)imv->imv = *(uint16_t *)imv->var;
+		break;
+	default:return -EINVAL;
+	}
+	flush_icache_range(imv->imv, imv->imv + imv->size);
+	return 0;
+}
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/irq.c b/stblinux-2.6.31/arch/powerpc/kernel/irq.c
index f7f376e..8e71b69 100644
--- a/stblinux-2.6.31/arch/powerpc/kernel/irq.c
+++ b/stblinux-2.6.31/arch/powerpc/kernel/irq.c
@@ -85,8 +85,6 @@ extern int tau_interrupts(int);
 #endif /* CONFIG_PPC32 */
 
 #ifdef CONFIG_PPC64
-EXPORT_SYMBOL(irq_desc);
-
 int distribute_irqs = 1;
 
 static inline notrace unsigned long get_hard_enabled(void)
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/misc_32.S b/stblinux-2.6.31/arch/powerpc/kernel/misc_32.S
index 15f28e0..fdce70f 100644
--- a/stblinux-2.6.31/arch/powerpc/kernel/misc_32.S
+++ b/stblinux-2.6.31/arch/powerpc/kernel/misc_32.S
@@ -672,7 +672,7 @@ _GLOBAL(abs)
  * Create a kernel thread
  *   kernel_thread(fn, arg, flags)
  */
-_GLOBAL(kernel_thread)
+_GLOBAL(original_kernel_thread)
 	stwu	r1,-16(r1)
 	stw	r30,8(r1)
 	stw	r31,12(r1)
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/misc_64.S b/stblinux-2.6.31/arch/powerpc/kernel/misc_64.S
index a5cf9c1..f8fb067 100644
--- a/stblinux-2.6.31/arch/powerpc/kernel/misc_64.S
+++ b/stblinux-2.6.31/arch/powerpc/kernel/misc_64.S
@@ -415,7 +415,7 @@ _GLOBAL(scom970_write)
  * Create a kernel thread
  *   kernel_thread(fn, arg, flags)
  */
-_GLOBAL(kernel_thread)
+_GLOBAL(original_kernel_thread)
 	std	r29,-24(r1)
 	std	r30,-16(r1)
 	stdu	r1,-STACK_FRAME_OVERHEAD(r1)
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/ppc_ksyms.c b/stblinux-2.6.31/arch/powerpc/kernel/ppc_ksyms.c
index c8b27bb..07115d6 100644
--- a/stblinux-2.6.31/arch/powerpc/kernel/ppc_ksyms.c
+++ b/stblinux-2.6.31/arch/powerpc/kernel/ppc_ksyms.c
@@ -162,7 +162,6 @@ EXPORT_SYMBOL(screen_info);
 
 #ifdef CONFIG_PPC32
 EXPORT_SYMBOL(timer_interrupt);
-EXPORT_SYMBOL(irq_desc);
 EXPORT_SYMBOL(tb_ticks_per_jiffy);
 EXPORT_SYMBOL(cacheable_memcpy);
 EXPORT_SYMBOL(cacheable_memzero);
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/process.c b/stblinux-2.6.31/arch/powerpc/kernel/process.c
index 1ea56fd..4524c00 100644
--- a/stblinux-2.6.31/arch/powerpc/kernel/process.c
+++ b/stblinux-2.6.31/arch/powerpc/kernel/process.c
@@ -37,6 +37,7 @@
 #include <linux/kernel_stat.h>
 #include <linux/personality.h>
 #include <linux/random.h>
+#include <trace/sched.h>
 
 #include <asm/pgtable.h>
 #include <asm/uaccess.h>
@@ -54,6 +55,8 @@
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
 
+DEFINE_TRACE(sched_kthread_create);
+
 extern unsigned long _get_SP(void);
 
 #ifndef CONFIG_SMP
@@ -547,6 +550,17 @@ void show_regs(struct pt_regs * regs)
 		show_instructions(regs);
 }
 
+long original_kernel_thread(int (*fn) (void *), void *arg, unsigned long flags);
+
+long kernel_thread(int (fn) (void *), void *arg, unsigned long flags)
+{
+	long retval;
+
+	retval = original_kernel_thread(fn, arg, flags);
+	trace_sched_kthread_create(fn, retval);
+	return retval;
+}
+
 void exit_thread(void)
 {
 	discard_lazy_cpu_state();
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/ptrace.c b/stblinux-2.6.31/arch/powerpc/kernel/ptrace.c
index ef14988..69f5e6d 100644
--- a/stblinux-2.6.31/arch/powerpc/kernel/ptrace.c
+++ b/stblinux-2.6.31/arch/powerpc/kernel/ptrace.c
@@ -32,12 +32,16 @@
 #ifdef CONFIG_PPC32
 #include <linux/module.h>
 #endif
+#include <trace/syscall.h>
 
 #include <asm/uaccess.h>
 #include <asm/page.h>
 #include <asm/pgtable.h>
 #include <asm/system.h>
 
+DEFINE_TRACE(syscall_entry);
+DEFINE_TRACE(syscall_exit);
+
 /*
  * does not yet catch signals sent when the child dies.
  * in exit.c or in signal.c.
@@ -1041,6 +1045,8 @@ long do_syscall_trace_enter(struct pt_regs *regs)
 {
 	long ret = 0;
 
+	trace_syscall_entry(regs, regs->gpr[0]);
+
 	secure_computing(regs->gpr[0]);
 
 	if (test_thread_flag(TIF_SYSCALL_TRACE) &&
@@ -1076,6 +1082,8 @@ void do_syscall_trace_leave(struct pt_regs *regs)
 {
 	int step;
 
+	trace_syscall_exit(regs->result);
+
 	if (unlikely(current->audit_context))
 		audit_syscall_exit((regs->ccr&0x10000000)?AUDITSC_FAILURE:AUDITSC_SUCCESS,
 				   regs->result);
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/syscalls.c b/stblinux-2.6.31/arch/powerpc/kernel/syscalls.c
index c04832c..7554d8a 100644
--- a/stblinux-2.6.31/arch/powerpc/kernel/syscalls.c
+++ b/stblinux-2.6.31/arch/powerpc/kernel/syscalls.c
@@ -36,12 +36,15 @@
 #include <linux/file.h>
 #include <linux/init.h>
 #include <linux/personality.h>
+#include <trace/ipc.h>
 
 #include <asm/uaccess.h>
 #include <asm/syscalls.h>
 #include <asm/time.h>
 #include <asm/unistd.h>
 
+DEFINE_TRACE(ipc_call);
+
 /*
  * sys_ipc() is the de-multiplexer for the SysV IPC calls..
  *
@@ -55,6 +58,8 @@ int sys_ipc(uint call, int first, unsigned long second, long third,
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_ipc_call(call, first);
+
 	ret = -ENOSYS;
 	switch (call) {
 	case SEMOP:
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/time.c b/stblinux-2.6.31/arch/powerpc/kernel/time.c
index eae4511..e4c2143 100644
--- a/stblinux-2.6.31/arch/powerpc/kernel/time.c
+++ b/stblinux-2.6.31/arch/powerpc/kernel/time.c
@@ -571,6 +571,8 @@ void timer_interrupt(struct pt_regs * regs)
 	 * some CPUs will continuue to take decrementer exceptions */
 	set_dec(DECREMENTER_MAX);
 
+	trace_trap_entry(regs, regs->trap);
+
 #ifdef CONFIG_PPC32
 	if (test_perf_counter_pending()) {
 		clear_perf_counter_pending();
@@ -616,6 +618,8 @@ void timer_interrupt(struct pt_regs * regs)
 
 	irq_exit();
 	set_irq_regs(old_regs);
+
+	trace_trap_exit();
 }
 
 void wakeup_decrementer(void)
diff --git a/stblinux-2.6.31/arch/powerpc/kernel/traps.c b/stblinux-2.6.31/arch/powerpc/kernel/traps.c
index 6f0ae1a..670de77 100644
--- a/stblinux-2.6.31/arch/powerpc/kernel/traps.c
+++ b/stblinux-2.6.31/arch/powerpc/kernel/traps.c
@@ -34,6 +34,8 @@
 #include <linux/bug.h>
 #include <linux/kdebug.h>
 #include <linux/debugfs.h>
+#include <linux/ltt-core.h>
+#include <trace/trap.h>
 
 #include <asm/emulated_ops.h>
 #include <asm/pgtable.h>
@@ -78,6 +80,12 @@ EXPORT_SYMBOL(__debugger_fault_handler);
 #endif
 
 /*
+ * Also used in time.c and fault.c.
+ */
+DEFINE_TRACE(trap_entry);
+DEFINE_TRACE(trap_exit);
+
+/*
  * Trap & Exception support
  */
 
@@ -144,6 +152,10 @@ int die(const char *str, struct pt_regs *regs, long err)
 #ifdef CONFIG_NUMA
 		printk("NUMA ");
 #endif
+#ifdef CONFIG_LTT
+		printk("LTT NESTING LEVEL : %u ", __get_cpu_var(ltt_nesting));
+		printk("\n");
+#endif
 		printk("%s\n", ppc_md.name ? ppc_md.name : "");
 
 		print_modules();
@@ -193,6 +205,8 @@ void _exception(int signr, struct pt_regs *regs, int code, unsigned long addr)
 				addr, regs->nip, regs->link, code);
 		}
 
+	trace_trap_entry(regs, regs->trap);
+
 	memset(&info, 0, sizeof(info));
 	info.si_signo = signr;
 	info.si_code = code;
@@ -220,6 +234,8 @@ void _exception(int signr, struct pt_regs *regs, int code, unsigned long addr)
 			do_exit(signr);
 		}
 	}
+
+	trace_trap_exit();
 }
 
 #ifdef CONFIG_PPC64
@@ -973,7 +989,9 @@ void vsx_unavailable_exception(struct pt_regs *regs)
 
 void performance_monitor_exception(struct pt_regs *regs)
 {
+	trace_trap_entry(regs, regs->trap);
 	perf_irq(regs);
+	trace_trap_exit();
 }
 
 #ifdef CONFIG_8xx
@@ -1141,12 +1159,14 @@ void altivec_assist_exception(struct pt_regs *regs)
 		/* got an error reading the instruction */
 		_exception(SIGSEGV, regs, SEGV_ACCERR, regs->nip);
 	} else {
+		trace_trap_entry(regs, regs->trap);
 		/* didn't recognize the instruction */
 		/* XXX quick hack for now: set the non-Java bit in the VSCR */
 		if (printk_ratelimit())
 			printk(KERN_ERR "Unrecognized altivec instruction "
 			       "in %s at %lx\n", current->comm, regs->nip);
 		current->thread.vscr.u[3] |= 0x10000;
+		trace_trap_exit();
 	}
 }
 #endif /* CONFIG_ALTIVEC */
diff --git a/stblinux-2.6.31/arch/powerpc/mm/fault.c b/stblinux-2.6.31/arch/powerpc/mm/fault.c
index 830bef0..5af4bb7 100644
--- a/stblinux-2.6.31/arch/powerpc/mm/fault.c
+++ b/stblinux-2.6.31/arch/powerpc/mm/fault.c
@@ -30,6 +30,7 @@
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
 #include <linux/perf_counter.h>
+#include <trace/fault.h>
 
 #include <asm/firmware.h>
 #include <asm/page.h>
@@ -41,6 +42,8 @@
 #include <asm/tlbflush.h>
 #include <asm/siginfo.h>
 
+DEFINE_TRACE(page_fault_entry);
+DEFINE_TRACE(page_fault_exit);
 
 #ifdef CONFIG_KPROBES
 static inline int notify_page_fault(struct pt_regs *regs)
@@ -302,7 +305,9 @@ good_area:
 	 * the fault.
 	 */
  survive:
+	trace_page_fault_entry(regs, regs->trap, mm, vma, address, is_write);
 	ret = handle_mm_fault(mm, vma, address, is_write ? FAULT_FLAG_WRITE : 0);
+	trace_page_fault_exit(ret);
 	if (unlikely(ret & VM_FAULT_ERROR)) {
 		if (ret & VM_FAULT_OOM)
 			goto out_of_memory;
diff --git a/stblinux-2.6.31/arch/powerpc/platforms/cell/spufs/spufs.h b/stblinux-2.6.31/arch/powerpc/platforms/cell/spufs/spufs.h
index ae31573..41f6715 100644
--- a/stblinux-2.6.31/arch/powerpc/platforms/cell/spufs/spufs.h
+++ b/stblinux-2.6.31/arch/powerpc/platforms/cell/spufs/spufs.h
@@ -373,9 +373,9 @@ extern void spu_free_lscsa(struct spu_state *csa);
 extern void spuctx_switch_state(struct spu_context *ctx,
 		enum spu_utilization_state new_state);
 
-#define spu_context_trace(name, ctx, spu) \
-	trace_mark(name, "ctx %p spu %p", ctx, spu);
+#define spu_context_trace(name, ctx, _spu) \
+	trace_mark(spu, name, "ctx %p spu %p", ctx, _spu);
 #define spu_context_nospu_trace(name, ctx) \
-	trace_mark(name, "ctx %p", ctx);
+	trace_mark(spu, name, "ctx %p", ctx);
 
 #endif
diff --git a/stblinux-2.6.31/arch/s390/include/asm/thread_info.h b/stblinux-2.6.31/arch/s390/include/asm/thread_info.h
index ba1cab9..0bc04ce 100644
--- a/stblinux-2.6.31/arch/s390/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/s390/include/asm/thread_info.h
@@ -93,6 +93,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_SYSCALL_AUDIT	9	/* syscall auditing active */
 #define TIF_SECCOMP		10	/* secure computing */
 #define TIF_SYSCALL_FTRACE	11	/* ftrace syscall instrumentation */
+#define TIF_KERNEL_TRACE	12	/* kernel trace active */
 #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
 #define TIF_POLLING_NRFLAG	17	/* true if poll_idle() is polling 
 					   TIF_NEED_RESCHED */
@@ -112,6 +113,7 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1<<TIF_SECCOMP)
 #define _TIF_SYSCALL_FTRACE	(1<<TIF_SYSCALL_FTRACE)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_USEDFPU		(1<<TIF_USEDFPU)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
 #define _TIF_31BIT		(1<<TIF_31BIT)
diff --git a/stblinux-2.6.31/arch/s390/kernel/entry.S b/stblinux-2.6.31/arch/s390/kernel/entry.S
index c4c80a2..0802c41 100644
--- a/stblinux-2.6.31/arch/s390/kernel/entry.S
+++ b/stblinux-2.6.31/arch/s390/kernel/entry.S
@@ -54,7 +54,7 @@ _TIF_WORK_SVC = (_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_NEED_RESCHED | \
 _TIF_WORK_INT = (_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_NEED_RESCHED | \
 		 _TIF_MCCK_PENDING)
 _TIF_SYSCALL = (_TIF_SYSCALL_TRACE>>8 | _TIF_SYSCALL_AUDIT>>8 | \
-		_TIF_SECCOMP>>8 | _TIF_SYSCALL_FTRACE>>8)
+		_TIF_SECCOMP>>8 | _TIF_SYSCALL_FTRACE>>8 | _TIF_KERNEL_TRACE>>8)
 
 STACK_SHIFT = PAGE_SHIFT + THREAD_ORDER
 STACK_SIZE  = 1 << STACK_SHIFT
diff --git a/stblinux-2.6.31/arch/s390/kernel/entry64.S b/stblinux-2.6.31/arch/s390/kernel/entry64.S
index f6618e9..829a939 100644
--- a/stblinux-2.6.31/arch/s390/kernel/entry64.S
+++ b/stblinux-2.6.31/arch/s390/kernel/entry64.S
@@ -57,7 +57,7 @@ _TIF_WORK_SVC = (_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_NEED_RESCHED | \
 _TIF_WORK_INT = (_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_NEED_RESCHED | \
 		 _TIF_MCCK_PENDING)
 _TIF_SYSCALL = (_TIF_SYSCALL_TRACE>>8 | _TIF_SYSCALL_AUDIT>>8 | \
-		_TIF_SECCOMP>>8 | _TIF_SYSCALL_FTRACE>>8)
+		_TIF_SECCOMP>>8 | _TIF_SYSCALL_FTRACE>>8 | _TIF_KERNEL_TRACE>>8)
 
 #define BASED(name) name-system_call(%r13)
 
diff --git a/stblinux-2.6.31/arch/s390/kernel/ptrace.c b/stblinux-2.6.31/arch/s390/kernel/ptrace.c
index 43acd73..5e67a06 100644
--- a/stblinux-2.6.31/arch/s390/kernel/ptrace.c
+++ b/stblinux-2.6.31/arch/s390/kernel/ptrace.c
@@ -56,6 +56,9 @@ enum s390_regset {
 	REGSET_FP,
 };
 
+DEFINE_TRACE(syscall_entry);
+DEFINE_TRACE(syscall_exit);
+
 static void
 FixPerRegisters(struct task_struct *task)
 {
@@ -644,6 +647,7 @@ asmlinkage long do_syscall_trace_enter(struct pt_regs *regs)
 	/* Do the secure computing check first. */
 	secure_computing(regs->gprs[2]);
 
+	trace_syscall_entry(regs, regs->gprs[2]);
 	/*
 	 * The sysc_tracesys code in entry.S stored the system
 	 * call number to gprs[2].
@@ -675,6 +679,7 @@ asmlinkage long do_syscall_trace_enter(struct pt_regs *regs)
 
 asmlinkage void do_syscall_trace_exit(struct pt_regs *regs)
 {
+	trace_syscall_exit(regs->gprs[2]);
 	if (unlikely(current->audit_context))
 		audit_syscall_exit(AUDITSC_RESULT(regs->gprs[2]),
 				   regs->gprs[2]);
diff --git a/stblinux-2.6.31/arch/s390/kernel/sys_s390.c b/stblinux-2.6.31/arch/s390/kernel/sys_s390.c
index c7ae4b1..6e5b6de 100644
--- a/stblinux-2.6.31/arch/s390/kernel/sys_s390.c
+++ b/stblinux-2.6.31/arch/s390/kernel/sys_s390.c
@@ -30,9 +30,12 @@
 #include <linux/unistd.h>
 #include <linux/ipc.h>
 #include <linux/syscalls.h>
+#include <trace/ipc.h>
 #include <asm/uaccess.h>
 #include "entry.h"
 
+DEFINE_TRACE(ipc_call);
+
 /* common code for old and new mmaps */
 static inline long do_mmap2(
 	unsigned long addr, unsigned long len,
@@ -115,6 +118,8 @@ SYSCALL_DEFINE5(ipc, uint, call, int, first, unsigned long, second,
         struct ipc_kludge tmp;
 	int ret;
 
+        trace_ipc_call(call, first);
+
         switch (call) {
         case SEMOP:
 		return sys_semtimedop(first, (struct sembuf __user *)ptr,
diff --git a/stblinux-2.6.31/arch/s390/kernel/traps.c b/stblinux-2.6.31/arch/s390/kernel/traps.c
index c2e42cc..289aefa 100644
--- a/stblinux-2.6.31/arch/s390/kernel/traps.c
+++ b/stblinux-2.6.31/arch/s390/kernel/traps.c
@@ -5,6 +5,7 @@
  *    Copyright (C) 1999,2000 IBM Deutschland Entwicklung GmbH, IBM Corporation
  *    Author(s): Martin Schwidefsky (schwidefsky@de.ibm.com),
  *               Denis Joseph Barrow (djbarrow@de.ibm.com,barrow_dj@yahoo.com),
+ *  Portions added by T. Halloran: (C) Copyright 2002 IBM Poughkeepsie, IBM Corporation
  *
  *  Derived from "arch/i386/kernel/traps.c"
  *    Copyright (C) 1991, 1992 Linus Torvalds
@@ -33,6 +34,7 @@
 #include <linux/kprobes.h>
 #include <linux/bug.h>
 #include <linux/utsname.h>
+#include <trace/trap.h>
 #include <asm/system.h>
 #include <asm/uaccess.h>
 #include <asm/io.h>
@@ -71,6 +73,12 @@ static int kstack_depth_to_print = 20;
 #endif /* CONFIG_64BIT */
 
 /*
+ * Also used in fault.c.
+ */
+DEFINE_TRACE(trap_entry);
+DEFINE_TRACE(trap_exit);
+
+/*
  * For show_trace we have tree different stack to consider:
  *   - the panic stack which is used if the kernel stack has overflown
  *   - the asynchronous interrupt stack (cpu related)
@@ -349,6 +357,8 @@ static void __kprobes inline do_trap(long interruption_code, int signr,
 				interruption_code, signr) == NOTIFY_STOP)
 		return;
 
+	trace_trap_entry(regs, interruption_code & 0xffff);
+
         if (regs->psw.mask & PSW_MASK_PSTATE) {
                 struct task_struct *tsk = current;
 
@@ -369,6 +379,7 @@ static void __kprobes inline do_trap(long interruption_code, int signr,
 			die(str, regs, interruption_code);
 		}
         }
+	trace_trap_exit();
 }
 
 static inline void __user *get_check_address(struct pt_regs *regs)
@@ -479,6 +490,8 @@ static void illegal_op(struct pt_regs * regs, long interruption_code)
 	if (regs->psw.mask & PSW_MASK_PSTATE)
 		local_irq_enable();
 
+	trace_trap_entry(regs, interruption_code & 0xffff);
+
 	if (regs->psw.mask & PSW_MASK_PSTATE) {
 		if (get_user(*((__u16 *) opcode), (__u16 __user *) location))
 			return;
@@ -543,6 +556,7 @@ static void illegal_op(struct pt_regs * regs, long interruption_code)
 		do_trap(interruption_code, signal,
 			"illegal operation", regs, &info);
 	}
+	trace_trap_exit();
 }
 
 
@@ -563,6 +577,8 @@ specification_exception(struct pt_regs * regs, long interruption_code)
         if (regs->psw.mask & PSW_MASK_PSTATE)
 		local_irq_enable();
 
+	trace_trap_entry(regs, interruption_code & 0xffff);
+
         if (regs->psw.mask & PSW_MASK_PSTATE) {
 		get_user(*((__u16 *) opcode), location);
 		switch (opcode[0]) {
@@ -607,6 +623,7 @@ specification_exception(struct pt_regs * regs, long interruption_code)
 		do_trap(interruption_code, signal, 
 			"specification exception", regs, &info);
 	}
+	trace_trap_exit();
 }
 #else
 DO_ERROR_INFO(SIGILL, "specification exception", specification_exception,
@@ -627,6 +644,8 @@ static void data_exception(struct pt_regs * regs, long interruption_code)
 	if (regs->psw.mask & PSW_MASK_PSTATE)
 		local_irq_enable();
 
+	trace_trap_entry(regs, interruption_code & 0xffff);
+
 	if (MACHINE_HAS_IEEE)
 		asm volatile("stfpc %0" : "=m" (current->thread.fp_regs.fpc));
 
@@ -701,6 +720,7 @@ static void data_exception(struct pt_regs * regs, long interruption_code)
 		do_trap(interruption_code, signal, 
 			"data exception", regs, &info);
 	}
+	trace_trap_exit();
 }
 
 static void space_switch_exception(struct pt_regs * regs, long int_code)
diff --git a/stblinux-2.6.31/arch/s390/mm/fault.c b/stblinux-2.6.31/arch/s390/mm/fault.c
index e5e119f..5168ef1 100644
--- a/stblinux-2.6.31/arch/s390/mm/fault.c
+++ b/stblinux-2.6.31/arch/s390/mm/fault.c
@@ -5,6 +5,7 @@
  *    Copyright (C) 1999 IBM Deutschland Entwicklung GmbH, IBM Corporation
  *    Author(s): Hartmut Penner (hp@de.ibm.com)
  *               Ulrich Weigand (uweigand@de.ibm.com)
+ *  Portions added by T. Halloran: (C) Copyright 2002 IBM Poughkeepsie, IBM Corporation
  *
  *  Derived from "arch/i386/mm/fault.c"
  *    Copyright (C) 1995  Linus Torvalds
@@ -29,6 +30,7 @@
 #include <linux/kprobes.h>
 #include <linux/uaccess.h>
 #include <linux/hugetlb.h>
+#include <trace/fault.h>
 #include <asm/system.h>
 #include <asm/pgtable.h>
 #include <asm/s390_ext.h>
@@ -51,6 +53,11 @@
 extern int sysctl_userprocess_debug;
 #endif
 
+DEFINE_TRACE(page_fault_entry);
+DEFINE_TRACE(page_fault_exit);
+DEFINE_TRACE(page_fault_nosem_entry);
+DEFINE_TRACE(page_fault_nosem_exit);
+
 #ifdef CONFIG_KPROBES
 static inline int notify_page_fault(struct pt_regs *regs, long err)
 {
@@ -351,7 +358,10 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
+	trace_page_fault_entry(regs, error_code & 0xffff, mm, vma, address,
+			       write);
 	fault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);
+	trace_page_fault_exit(fault);
 	if (unlikely(fault & VM_FAULT_ERROR)) {
 		if (fault & VM_FAULT_OOM) {
 			up_read(&mm->mmap_sem);
@@ -385,9 +395,12 @@ bad_area:
 
 	/* User mode accesses just cause a SIGSEGV */
 	if (regs->psw.mask & PSW_MASK_PSTATE) {
+		trace_page_fault_nosem_entry(regs, error_code & 0xffff,
+					     address);
 		tsk->thread.prot_addr = address;
 		tsk->thread.trap_no = error_code;
 		do_sigsegv(regs, error_code, si_code, address);
+		trace_page_fault_nosem_exit();
 		return;
 	}
 
diff --git a/stblinux-2.6.31/arch/sh/Kconfig b/stblinux-2.6.31/arch/sh/Kconfig
index e2bdd7b..137c1fb 100644
--- a/stblinux-2.6.31/arch/sh/Kconfig
+++ b/stblinux-2.6.31/arch/sh/Kconfig
@@ -11,6 +11,8 @@ config SUPERH
 	select HAVE_CLK
 	select HAVE_IDE
 	select HAVE_OPROFILE
+	select HAVE_TRACE_CLOCK
+	select HAVE_TRACE_CLOCK_32_TO_64
 	select HAVE_GENERIC_DMA_COHERENT
 	select HAVE_IOREMAP_PROT if MMU
 	select HAVE_ARCH_TRACEHOOK
@@ -33,6 +35,7 @@ config SUPERH32
 	select HAVE_DYNAMIC_FTRACE
 	select HAVE_ARCH_KGDB
 	select ARCH_HIBERNATION_POSSIBLE if MMU
+	select HAVE_LTT_DUMP_TABLES
 
 config SUPERH64
 	def_bool ARCH = "sh64"
diff --git a/stblinux-2.6.31/arch/sh/include/asm/thread_info.h b/stblinux-2.6.31/arch/sh/include/asm/thread_info.h
index d570ac2..0c9160f 100644
--- a/stblinux-2.6.31/arch/sh/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/sh/include/asm/thread_info.h
@@ -116,6 +116,7 @@ extern void free_thread_info(struct thread_info *ti);
 #define TIF_SYSCALL_AUDIT	5	/* syscall auditing active */
 #define TIF_SECCOMP		6	/* secure computing */
 #define TIF_NOTIFY_RESUME	7	/* callback before returning to user */
+#define TIF_KERNEL_TRACE	8	/* kernel trace active */
 #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
 #define TIF_POLLING_NRFLAG	17	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_MEMDIE		18
@@ -129,6 +130,7 @@ extern void free_thread_info(struct thread_info *ti);
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
+#define _TIF_KERNEL_TRACE	(1 << TIF_KERNEL_TRACE)
 #define _TIF_USEDFPU		(1 << TIF_USEDFPU)
 #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_FREEZE		(1 << TIF_FREEZE)
@@ -141,17 +143,19 @@ extern void free_thread_info(struct thread_info *ti);
 
 /* work to do in syscall trace */
 #define _TIF_WORK_SYSCALL_MASK	(_TIF_SYSCALL_TRACE | _TIF_SINGLESTEP | \
-				 _TIF_SYSCALL_AUDIT | _TIF_SECCOMP)
+				 _TIF_SYSCALL_AUDIT | _TIF_SECCOMP | \
+				 _TIF_KERNEL_TRACE)
 
 /* work to do on any return to u-space */
 #define _TIF_ALLWORK_MASK	(_TIF_SYSCALL_TRACE | _TIF_SIGPENDING      | \
 				 _TIF_NEED_RESCHED  | _TIF_SYSCALL_AUDIT   | \
 				 _TIF_SINGLESTEP    | _TIF_RESTORE_SIGMASK | \
-				 _TIF_NOTIFY_RESUME)
+				 _TIF_NOTIFY_RESUME | _TIF_KERNEL_TRACE)
 
 /* work to do on interrupt/exception return */
 #define _TIF_WORK_MASK		(_TIF_ALLWORK_MASK & ~(_TIF_SYSCALL_TRACE | \
-				 _TIF_SYSCALL_AUDIT | _TIF_SINGLESTEP))
+				 _TIF_SYSCALL_AUDIT | _TIF_SINGLESTEP | \
+				 _TIF_KERNEL_TRACE))
 
 #endif /* __KERNEL__ */
 
diff --git a/stblinux-2.6.31/arch/sh/include/asm/timex.h b/stblinux-2.6.31/arch/sh/include/asm/timex.h
index b556d49..3694c9a 100644
--- a/stblinux-2.6.31/arch/sh/include/asm/timex.h
+++ b/stblinux-2.6.31/arch/sh/include/asm/timex.h
@@ -6,8 +6,16 @@
 #ifndef __ASM_SH_TIMEX_H
 #define __ASM_SH_TIMEX_H
 
-#define CLOCK_TICK_RATE		(CONFIG_SH_PCLK_FREQ / 4) /* Underlying HZ */
+#include <linux/io.h>
+#include <cpu/timer.h>
 
-#include <asm-generic/timex.h>
+#define CLOCK_TICK_RATE               (HZ * 100000UL)
+
+typedef unsigned long long cycles_t;
+
+static __inline__ cycles_t get_cycles (void)
+{
+	return 0xffffffff - ctrl_inl(TMU1_TCNT);
+}
 
 #endif /* __ASM_SH_TIMEX_H */
diff --git a/stblinux-2.6.31/arch/sh/include/asm/trace-clock.h b/stblinux-2.6.31/arch/sh/include/asm/trace-clock.h
new file mode 100644
index 0000000..0fb1603
--- /dev/null
+++ b/stblinux-2.6.31/arch/sh/include/asm/trace-clock.h
@@ -0,0 +1,70 @@
+/*
+ * Copyright (C) 2007,2008 Giuseppe Cavallaro <peppe.cavallaro@st.com>
+ *                         Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * Trace clock definitions for SuperH.
+ */
+
+#ifndef _ASM_SH_TRACE_CLOCK_H
+#define _ASM_SH_TRACE_CLOCK_H
+
+#include <linux/timer.h>
+#include <asm/clock.h>
+
+/*
+ * Number of hardware clock bits. The higher order bits are expected to be 0.
+ * If the hardware clock source has more than 32 bits, the bits higher than the
+ * 32nd will be truncated by a cast to a 32 bits unsigned. Range : 1 - 32.
+ * (too few bits would be unrealistic though, since we depend on the timer to
+ * detect the overflows).
+ */
+#define TC_HW_BITS			32
+
+/* Expected maximum interrupt latency in ms : 15ms, *2 for security */
+#define TC_EXPECTED_INTERRUPT_LATENCY	30
+
+extern u64 trace_clock_read_synthetic_tsc(void);
+
+static inline u32 trace_clock_get_read32(void)
+{
+	return get_cycles();
+}
+
+static inline u64 trace_clock_get_read64(void)
+{
+	return trace_clock_read_synthetic_tsc();
+}
+
+static inline u64 trace_clock_frequency(void)
+{
+	u64 rate;
+	struct clk *tmu1_clk;
+
+	tmu1_clk = clk_get(NULL, "tmu1_clk");
+	rate = clk_get_rate(tmu1_clk);
+
+	return rate;
+}
+
+static inline u32 trace_clock_freq_scale(void)
+{
+	return 1;
+}
+
+extern void get_synthetic_tsc(void);
+extern void put_synthetic_tsc(void);
+
+static inline void get_trace_clock(void)
+{
+	get_synthetic_tsc();
+}
+
+static inline void put_trace_clock(void)
+{
+	put_synthetic_tsc();
+}
+
+static inline void set_trace_clock_is_sync(int state)
+{
+}
+#endif /* _ASM_SH_TRACE_CLOCK_H */
diff --git a/stblinux-2.6.31/arch/sh/kernel/process_32.c b/stblinux-2.6.31/arch/sh/kernel/process_32.c
index 92d7740..50a4cd0 100644
--- a/stblinux-2.6.31/arch/sh/kernel/process_32.c
+++ b/stblinux-2.6.31/arch/sh/kernel/process_32.c
@@ -24,6 +24,7 @@
 #include <linux/reboot.h>
 #include <linux/fs.h>
 #include <linux/preempt.h>
+#include <trace/sched.h>
 #include <asm/uaccess.h>
 #include <asm/mmu_context.h>
 #include <asm/pgalloc.h>
@@ -32,6 +33,8 @@
 #include <asm/fpu.h>
 #include <asm/syscalls.h>
 
+DEFINE_TRACE(sched_kthread_create);
+
 int ubc_usercnt = 0;
 
 void machine_restart(char * __unused)
@@ -119,6 +122,8 @@ int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
 	pid = do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0,
 		      &regs, 0, NULL, NULL);
 
+	trace_sched_kthread_create(fn, pid);
+
 	return pid;
 }
 
diff --git a/stblinux-2.6.31/arch/sh/kernel/process_64.c b/stblinux-2.6.31/arch/sh/kernel/process_64.c
index 54a5abf..25321cd 100644
--- a/stblinux-2.6.31/arch/sh/kernel/process_64.c
+++ b/stblinux-2.6.31/arch/sh/kernel/process_64.c
@@ -24,12 +24,15 @@
 #include <linux/init.h>
 #include <linux/module.h>
 #include <linux/io.h>
+#include <trace/sched.h>
 #include <asm/syscalls.h>
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
 #include <asm/fpu.h>
 
+DEFINE_TRACE(sched_kthread_create);
+
 struct task_struct *last_task_used_math = NULL;
 
 void machine_restart(char * __unused)
@@ -322,6 +325,7 @@ ATTRIB_NORET void kernel_thread_helper(void *arg, int (*fn)(void *))
  */
 int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
 {
+	int pid;
 	struct pt_regs regs;
 
 	memset(&regs, 0, sizeof(regs));
@@ -332,8 +336,12 @@ int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
 	regs.sr = (1 << 30);
 
 	/* Ok, create the new process.. */
-	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0,
+	pid = do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0,
 		      &regs, 0, NULL, NULL);
+
+	trace_sched_kthread_create(fn, pid);
+
+	return pid;
 }
 
 /*
diff --git a/stblinux-2.6.31/arch/sh/kernel/ptrace_32.c b/stblinux-2.6.31/arch/sh/kernel/ptrace_32.c
index 3392e83..6ec1e6f 100644
--- a/stblinux-2.6.31/arch/sh/kernel/ptrace_32.c
+++ b/stblinux-2.6.31/arch/sh/kernel/ptrace_32.c
@@ -26,6 +26,10 @@
 #include <linux/tracehook.h>
 #include <linux/elf.h>
 #include <linux/regset.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/marker.h>
+#include <trace/syscall.h>
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
 #include <asm/system.h>
@@ -33,6 +37,30 @@
 #include <asm/mmu_context.h>
 #include <asm/syscalls.h>
 #include <asm/fpu.h>
+#include <asm/unistd.h>
+
+DEFINE_TRACE(syscall_entry);
+DEFINE_TRACE(syscall_exit);
+
+extern unsigned long sys_call_table[];
+void ltt_dump_sys_call_table(void *call_data)
+{
+	int i;
+	char namebuf[KSYM_NAME_LEN];
+
+	for (i = 0; i < NR_syscalls; i++) {
+		sprint_symbol(namebuf, sys_call_table[i]);
+		__trace_mark(0, syscall_state, sys_call_table, call_data,
+			"id %d address %p symbol %s",
+			i, (void *)sys_call_table[i], namebuf);
+	}
+}
+EXPORT_SYMBOL_GPL(ltt_dump_sys_call_table);
+
+void ltt_dump_idt_table(void *call_data)
+{
+}
+EXPORT_SYMBOL_GPL(ltt_dump_idt_table);
 
 /*
  * This routine will get a word off of the process kernel stack.
@@ -448,6 +476,8 @@ asmlinkage long do_syscall_trace_enter(struct pt_regs *regs)
 {
 	long ret = 0;
 
+	trace_syscall_entry(regs, regs->regs[3]);
+
 	secure_computing(regs->regs[0]);
 
 	if (test_thread_flag(TIF_SYSCALL_TRACE) &&
@@ -471,6 +501,8 @@ asmlinkage void do_syscall_trace_leave(struct pt_regs *regs)
 {
 	int step;
 
+	trace_syscall_exit(regs->regs[0]);
+
 	if (unlikely(current->audit_context))
 		audit_syscall_exit(AUDITSC_RESULT(regs->regs[0]),
 				   regs->regs[0]);
diff --git a/stblinux-2.6.31/arch/sh/kernel/ptrace_64.c b/stblinux-2.6.31/arch/sh/kernel/ptrace_64.c
index 6950974..6e7816d 100644
--- a/stblinux-2.6.31/arch/sh/kernel/ptrace_64.c
+++ b/stblinux-2.6.31/arch/sh/kernel/ptrace_64.c
@@ -31,6 +31,7 @@
 #include <linux/tracehook.h>
 #include <linux/elf.h>
 #include <linux/regset.h>
+#include <trace/syscall.h>
 #include <asm/io.h>
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -40,6 +41,9 @@
 #include <asm/syscalls.h>
 #include <asm/fpu.h>
 
+DEFINE_TRACE(syscall_entry);
+DEFINE_TRACE(syscall_exit);
+
 /* This mask defines the bits of the SR which the user is not allowed to
    change, which are everything except S, Q, M, PR, SZ, FR. */
 #define SR_MASK      (0xffff8cfd)
diff --git a/stblinux-2.6.31/arch/sh/kernel/sys_sh.c b/stblinux-2.6.31/arch/sh/kernel/sys_sh.c
index 90d00e4..ea33bb3 100644
--- a/stblinux-2.6.31/arch/sh/kernel/sys_sh.c
+++ b/stblinux-2.6.31/arch/sh/kernel/sys_sh.c
@@ -25,6 +25,9 @@
 #include <asm/syscalls.h>
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
+#include <trace/ipc.h>
+
+DEFINE_TRACE(ipc_call);
 
 static inline long
 do_mmap2(unsigned long addr, unsigned long len, unsigned long prot,
@@ -88,6 +91,8 @@ asmlinkage int sys_ipc(uint call, int first, int second,
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_ipc_call(call, first);
+
 	if (call <= SEMTIMEDOP)
 		switch (call) {
 		case SEMOP:
diff --git a/stblinux-2.6.31/arch/sh/kernel/traps_32.c b/stblinux-2.6.31/arch/sh/kernel/traps_32.c
index 2b77277..30a7b89 100644
--- a/stblinux-2.6.31/arch/sh/kernel/traps_32.c
+++ b/stblinux-2.6.31/arch/sh/kernel/traps_32.c
@@ -24,11 +24,14 @@
 #include <linux/kdebug.h>
 #include <linux/kexec.h>
 #include <linux/limits.h>
+#include <trace/trap.h>
 #include <asm/system.h>
 #include <asm/uaccess.h>
 #include <asm/fpu.h>
 #include <asm/kprobes.h>
 
+#include <trace/trap.h>
+
 #ifdef CONFIG_CPU_SH2
 # define TRAP_RESERVED_INST	4
 # define TRAP_ILLEGAL_SLOT_INST	6
@@ -44,6 +47,9 @@
 #define TRAP_ILLEGAL_SLOT_INST	13
 #endif
 
+DEFINE_TRACE(trap_entry);
+DEFINE_TRACE(trap_exit);
+
 static void dump_mem(const char *str, unsigned long bottom, unsigned long top)
 {
 	unsigned long p;
@@ -531,6 +537,8 @@ asmlinkage void do_address_error(struct pt_regs *regs,
 	error_code = lookup_exception_vector();
 #endif
 
+	trace_trap_entry(regs, error_code >> 5);
+
 	oldfs = get_fs();
 
 	if (user_mode(regs)) {
@@ -558,8 +566,10 @@ asmlinkage void do_address_error(struct pt_regs *regs,
 					      &user_mem_access);
 		set_fs(oldfs);
 
-		if (tmp==0)
+		if (!tmp) {
+			trace_trap_exit();
 			return; /* sorted */
+		}
 uspace_segv:
 		printk(KERN_NOTICE "Sending SIGBUS to \"%s\" due to unaligned "
 		       "access (PC %lx PR %lx)\n", current->comm, regs->pc,
@@ -587,6 +597,7 @@ uspace_segv:
 		handle_unaligned_access(instruction, regs, &user_mem_access);
 		set_fs(oldfs);
 	}
+	trace_trap_exit();
 }
 
 #ifdef CONFIG_SH_DSP
diff --git a/stblinux-2.6.31/arch/sh/mm/fault_32.c b/stblinux-2.6.31/arch/sh/mm/fault_32.c
index 7192594..c87d48c 100644
--- a/stblinux-2.6.31/arch/sh/mm/fault_32.c
+++ b/stblinux-2.6.31/arch/sh/mm/fault_32.c
@@ -16,11 +16,17 @@
 #include <linux/hardirq.h>
 #include <linux/kprobes.h>
 #include <linux/perf_counter.h>
+#include <trace/fault.h>
 #include <asm/io_trapped.h>
 #include <asm/system.h>
 #include <asm/mmu_context.h>
 #include <asm/tlbflush.h>
 
+DEFINE_TRACE(page_fault_entry);
+DEFINE_TRACE(page_fault_exit);
+DEFINE_TRACE(page_fault_nosem_entry);
+DEFINE_TRACE(page_fault_nosem_exit);
+
 static inline int notify_page_fault(struct pt_regs *regs, int trap)
 {
 	int ret = 0;
@@ -152,7 +158,14 @@ good_area:
 	 * the fault.
 	 */
 survive:
+	trace_page_fault_entry(regs,
+		({
+			unsigned long trapnr;
+			asm volatile("stc	r2_bank,%0": "=r" (trapnr));
+			trapnr;
+		}) >> 5, mm, vma, address, writeaccess);
 	fault = handle_mm_fault(mm, vma, address, writeaccess ? FAULT_FLAG_WRITE : 0);
+	trace_page_fault_exit(fault);
 	if (unlikely(fault & VM_FAULT_ERROR)) {
 		if (fault & VM_FAULT_OOM)
 			goto out_of_memory;
@@ -182,11 +195,18 @@ bad_area:
 
 bad_area_nosemaphore:
 	if (user_mode(regs)) {
+		trace_page_fault_nosem_entry(regs,
+		({
+			unsigned long trapnr;
+			asm volatile("stc	r2_bank,%0": "=r" (trapnr));
+			trapnr;
+		}) >> 5, address);
 		info.si_signo = SIGSEGV;
 		info.si_errno = 0;
 		info.si_code = si_code;
 		info.si_addr = (void *) address;
 		force_sig_info(SIGSEGV, &info, tsk);
+		trace_page_fault_nosem_exit();
 		return;
 	}
 
@@ -282,7 +302,10 @@ asmlinkage int __kprobes __do_page_fault(struct pt_regs *regs,
 	pte_t *pte;
 	pte_t entry;
 	int ret = 1;
+	int irqvec;
 
+	irqvec = lookup_exception_vector();
+	trace_page_fault_nosem_entry(regs, irqvec, address);
 	/*
 	 * We don't take page faults for P1, P2, and parts of P4, these
 	 * are always mapped, whether it be due to legacy behaviour in
@@ -327,5 +350,6 @@ asmlinkage int __kprobes __do_page_fault(struct pt_regs *regs,
 
 	ret = 0;
 out:
+	trace_page_fault_nosem_exit();
 	return ret;
 }
diff --git a/stblinux-2.6.31/arch/sparc/Kconfig b/stblinux-2.6.31/arch/sparc/Kconfig
index 3f8b6a9..e20125b 100644
--- a/stblinux-2.6.31/arch/sparc/Kconfig
+++ b/stblinux-2.6.31/arch/sparc/Kconfig
@@ -37,6 +37,7 @@ config SPARC64
 	select HAVE_KPROBES
 	select HAVE_LMB
 	select HAVE_SYSCALL_WRAPPERS
+	select HAVE_GET_CYCLES
 	select HAVE_DYNAMIC_FTRACE
 	select HAVE_FTRACE_MCOUNT_RECORD
 	select USE_GENERIC_SMP_HELPERS if SMP
@@ -44,6 +45,8 @@ config SPARC64
 	select RTC_DRV_BQ4802
 	select RTC_DRV_SUN4V
 	select RTC_DRV_STARFIRE
+	select HAVE_TRACE_CLOCK
+	select HAVE_IMMEDIATE
 
 config ARCH_DEFCONFIG
 	string
diff --git a/stblinux-2.6.31/arch/sparc/Makefile b/stblinux-2.6.31/arch/sparc/Makefile
index ef9a6b0..3b86201 100644
--- a/stblinux-2.6.31/arch/sparc/Makefile
+++ b/stblinux-2.6.31/arch/sparc/Makefile
@@ -59,6 +59,10 @@ KBUILD_CFLAGS += -m64 -pipe -mno-fpu -mcpu=ultrasparc -mcmodel=medlow   \
 KBUILD_CFLAGS += $(call cc-option,-mtune=ultrasparc3)
 KBUILD_AFLAGS += -m64 -mcpu=ultrasparc -Wa,--undeclared-regs
 
+# gcc 3.x has problems with passing symbol+offset in
+# asm "i" constraint.
+export USE_IMMEDIATE := $(call cc-ifversion, -ge, 0400, $(CONFIG_IMMEDIATE))
+
 ifeq ($(CONFIG_MCOUNT),y)
   KBUILD_CFLAGS += -pg
 endif
@@ -79,7 +83,7 @@ drivers-$(CONFIG_OPROFILE)	+= arch/sparc/oprofile/
 # Export what is needed by arch/sparc/boot/Makefile
 export VMLINUX_INIT VMLINUX_MAIN
 VMLINUX_INIT := $(head-y) $(init-y)
-VMLINUX_MAIN := $(core-y) kernel/ mm/ fs/ ipc/ security/ crypto/ block/
+VMLINUX_MAIN := $(core-y) kernel/ mm/ fs/ ipc/ security/ crypto/ block/ ltt/
 VMLINUX_MAIN += $(patsubst %/, %/lib.a, $(libs-y)) $(libs-y)
 VMLINUX_MAIN += $(drivers-y) $(net-y)
 
diff --git a/stblinux-2.6.31/arch/sparc/include/asm/asm.h b/stblinux-2.6.31/arch/sparc/include/asm/asm.h
index e8e1d94..23b6ba2 100644
--- a/stblinux-2.6.31/arch/sparc/include/asm/asm.h
+++ b/stblinux-2.6.31/arch/sparc/include/asm/asm.h
@@ -18,6 +18,7 @@
 	brnz,PREDICT	REG, DEST
 #define BRANCH_REG_NOT_ZERO_ANNUL(PREDICT, REG, DEST) \
 	brnz,a,PREDICT	REG, DEST
+#define __ASM_SEL(a,b)	__ASM_FORM(b)
 #else
 #define BRANCH32(TYPE, PREDICT, DEST) \
 	TYPE		DEST
@@ -35,6 +36,16 @@
 #define BRANCH_REG_NOT_ZERO_ANNUL(PREDICT, REG, DEST) \
 	cmp		REG, 0; \
 	bne,a		DEST
+#define __ASM_SEL(a,b)	__ASM_FORM(a)
 #endif
 
+#ifdef __ASSEMBLY__
+#define __ASM_FORM(x)	x
+#else
+#define __ASM_FORM(x)	" " #x " "
+#endif
+
+#define _ASM_PTR	__ASM_SEL(.word, .xword)
+#define _ASM_UAPTR	__ASM_SEL(.uaword, .uaxword)
+
 #endif /* _SPARC_ASM_H */
diff --git a/stblinux-2.6.31/arch/sparc/include/asm/immediate.h b/stblinux-2.6.31/arch/sparc/include/asm/immediate.h
new file mode 100644
index 0000000..ada9e95
--- /dev/null
+++ b/stblinux-2.6.31/arch/sparc/include/asm/immediate.h
@@ -0,0 +1,48 @@
+#ifndef _ASM_SPARC_IMMEDIATE_H
+#define _ASM_SPARC_IMMEDIATE_H
+
+#include <asm/asm.h>
+
+/*
+ * Immediate values. Sparc64 architecture optimizations.
+ *
+ * (C) Copyright 2009 David Miller <davem@davemloft.net>
+ * (C) Copyright 2009 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * Dual BSD/GPL v2 license.
+ */
+
+struct __imv {
+	unsigned long var;
+	unsigned long imv;
+	unsigned char size;
+} __attribute__ ((packed));
+
+#define imv_read(name)							\
+	({								\
+		__typeof__(name##__imv) value;				\
+		BUILD_BUG_ON(sizeof(value) > 8);			\
+		switch (sizeof(value)) {				\
+		case 1:							\
+			asm(".section __imv,\"aw\",@progbits\n\t"	\
+					_ASM_UAPTR " %c1, 1f\n\t"	\
+					".byte 1\n\t"			\
+					".previous\n\t"			\
+					"1: mov 0, %0\n\t"		\
+				: "=r" (value)				\
+				: "i" (&name##__imv));			\
+			break;						\
+		case 2:							\
+		case 4:							\
+		case 8:	value = name##__imv;				\
+			break;						\
+		};							\
+		value;							\
+	})
+
+#define imv_cond(name)	imv_read(name)
+#define imv_cond_end()
+
+extern int arch_imv_update(const struct __imv *imv, int early);
+
+#endif /* _ASM_SPARC_IMMEDIATE_H */
diff --git a/stblinux-2.6.31/arch/sparc/include/asm/thread_info_32.h b/stblinux-2.6.31/arch/sparc/include/asm/thread_info_32.h
index 844d73a..c780458 100644
--- a/stblinux-2.6.31/arch/sparc/include/asm/thread_info_32.h
+++ b/stblinux-2.6.31/arch/sparc/include/asm/thread_info_32.h
@@ -128,6 +128,7 @@ BTFIXUPDEF_CALL(void, free_thread_info, struct thread_info *)
 #define TIF_SIGPENDING		2	/* signal pending */
 #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
 #define TIF_RESTORE_SIGMASK	4	/* restore signal mask in do_signal() */
+#define TIF_KERNEL_TRACE	5	/* kernel trace active */
 #define TIF_USEDFPU		8	/* FPU was used by this task
 					 * this quantum (SMP) */
 #define TIF_POLLING_NRFLAG	9	/* true if poll_idle() is polling
@@ -137,6 +138,7 @@ BTFIXUPDEF_CALL(void, free_thread_info, struct thread_info *)
 
 /* as above, but as bit values */
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_NOTIFY_RESUME	(1<<TIF_NOTIFY_RESUME)
 #define _TIF_SIGPENDING		(1<<TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
diff --git a/stblinux-2.6.31/arch/sparc/include/asm/thread_info_64.h b/stblinux-2.6.31/arch/sparc/include/asm/thread_info_64.h
index f78ad9a..7454362 100644
--- a/stblinux-2.6.31/arch/sparc/include/asm/thread_info_64.h
+++ b/stblinux-2.6.31/arch/sparc/include/asm/thread_info_64.h
@@ -224,7 +224,7 @@ register struct thread_info *current_thread_info_reg asm("g6");
 #define TIF_UNALIGNED		5	/* allowed to do unaligned accesses */
 /* flag bit 6 is available */
 #define TIF_32BIT		7	/* 32-bit binary */
-/* flag bit 8 is available */
+#define TIF_KERNEL_TRACE	8	/* kernel trace active */
 #define TIF_SECCOMP		9	/* secure computing */
 #define TIF_SYSCALL_AUDIT	10	/* syscall auditing active */
 /* NOTE: Thread flags >= 12 should be ones we have no interest
@@ -243,6 +243,7 @@ register struct thread_info *current_thread_info_reg asm("g6");
 #define _TIF_PERFCTR		(1<<TIF_PERFCTR)
 #define _TIF_UNALIGNED		(1<<TIF_UNALIGNED)
 #define _TIF_32BIT		(1<<TIF_32BIT)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_SECCOMP		(1<<TIF_SECCOMP)
 #define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
diff --git a/stblinux-2.6.31/arch/sparc/include/asm/timex_64.h b/stblinux-2.6.31/arch/sparc/include/asm/timex_64.h
index 18b30bc..905443a 100644
--- a/stblinux-2.6.31/arch/sparc/include/asm/timex_64.h
+++ b/stblinux-2.6.31/arch/sparc/include/asm/timex_64.h
@@ -12,7 +12,24 @@
 
 /* Getting on the cycle counter on sparc64. */
 typedef unsigned long cycles_t;
-#define get_cycles()	tick_ops->get_tick()
+
+static inline cycles_t get_cycles(void)
+{
+	return tick_ops->get_tick();
+}
+
+/* get_cycles instruction is synchronized on sparc64 */
+static inline void get_cycles_barrier(void)
+{
+	return;
+}
+
+extern unsigned long tb_ticks_per_usec;
+
+static inline cycles_t get_cycles_rate(void)
+{
+	return (cycles_t)tb_ticks_per_usec * 1000000UL;
+}
 
 #define ARCH_HAS_READ_CURRENT_TIMER
 
diff --git a/stblinux-2.6.31/arch/sparc/include/asm/trace-clock.h b/stblinux-2.6.31/arch/sparc/include/asm/trace-clock.h
new file mode 100644
index 0000000..75ac94e
--- /dev/null
+++ b/stblinux-2.6.31/arch/sparc/include/asm/trace-clock.h
@@ -0,0 +1,43 @@
+/*
+ * Copyright (C) 2008, Mathieu Desnoyers
+ *
+ * Trace clock definitions for Sparc64.
+ */
+
+#ifndef _ASM_SPARC_TRACE_CLOCK_H
+#define _ASM_SPARC_TRACE_CLOCK_H
+
+#include <linux/timex.h>
+
+static inline u32 trace_clock_read32(void)
+{
+	return get_cycles();
+}
+
+static inline u64 trace_clock_read64(void)
+{
+	return get_cycles();
+}
+
+static inline unsigned int trace_clock_frequency(void)
+{
+	return get_cycles_rate();
+}
+
+static inline u32 trace_clock_freq_scale(void)
+{
+	return 1;
+}
+
+static inline void get_trace_clock(void)
+{
+}
+
+static inline void put_trace_clock(void)
+{
+}
+
+static inline void set_trace_clock_is_sync(int state)
+{
+}
+#endif /* _ASM_SPARC_TRACE_CLOCK_H */
diff --git a/stblinux-2.6.31/arch/sparc/kernel/Makefile b/stblinux-2.6.31/arch/sparc/kernel/Makefile
index cbf5d0b..e40f926 100644
--- a/stblinux-2.6.31/arch/sparc/kernel/Makefile
+++ b/stblinux-2.6.31/arch/sparc/kernel/Makefile
@@ -105,3 +105,4 @@ obj-$(CONFIG_SUN_LDOMS) += ldc.o vio.o viohs.o ds.o
 obj-$(CONFIG_AUDIT)     += audit.o
 audit--$(CONFIG_AUDIT)  := compat_audit.o
 obj-$(CONFIG_COMPAT)    += $(audit--y)
+obj-$(USE_IMMEDIATE)	+= immediate.o
diff --git a/stblinux-2.6.31/arch/sparc/kernel/entry.S b/stblinux-2.6.31/arch/sparc/kernel/entry.S
index f41ecc5..f4bec1b 100644
--- a/stblinux-2.6.31/arch/sparc/kernel/entry.S
+++ b/stblinux-2.6.31/arch/sparc/kernel/entry.S
@@ -1118,7 +1118,7 @@ sys_sigreturn:
 	 add	%sp, STACKFRAME_SZ, %o0
 
 	ld	[%curptr + TI_FLAGS], %l5
-	andcc	%l5, _TIF_SYSCALL_TRACE, %g0
+	andcc	%l5, (_TIF_SYSCALL_TRACE|_TIF_KERNEL_TRACE), %g0
 	be	1f
 	 nop
 
@@ -1138,7 +1138,7 @@ sys_rt_sigreturn:
 	 add	%sp, STACKFRAME_SZ, %o0
 
 	ld	[%curptr + TI_FLAGS], %l5
-	andcc	%l5, _TIF_SYSCALL_TRACE, %g0
+	andcc	%l5, (_TIF_SYSCALL_TRACE|_TIF_KERNEL_TRACE), %g0
 	be	1f
 	 nop
 
@@ -1280,7 +1280,7 @@ syscall_is_too_hard:
 
 	ld	[%curptr + TI_FLAGS], %l5
 	mov	%i3, %o3
-	andcc	%l5, _TIF_SYSCALL_TRACE, %g0
+	andcc	%l5, (_TIF_SYSCALL_TRACE|_TIF_KERNEL_TRACE), %g0
 	mov	%i4, %o4
 	bne	linux_syscall_trace
 	 mov	%i0, %l5
@@ -1297,7 +1297,7 @@ ret_sys_call:
 	ld	[%sp + STACKFRAME_SZ + PT_PSR], %g3
 	set	PSR_C, %g2
 	bgeu	1f
-	 andcc	%l6, _TIF_SYSCALL_TRACE, %g0
+	 andcc	%l6, (_TIF_SYSCALL_TRACE|_TIF_KERNEL_TRACE), %g0
 
 	/* System call success, clear Carry condition code. */
 	andn	%g3, %g2, %g3
diff --git a/stblinux-2.6.31/arch/sparc/kernel/immediate.c b/stblinux-2.6.31/arch/sparc/kernel/immediate.c
new file mode 100644
index 0000000..8dfcf2d
--- /dev/null
+++ b/stblinux-2.6.31/arch/sparc/kernel/immediate.c
@@ -0,0 +1,57 @@
+/*
+ * Immediate values. Sparc64 architecture optimizations.
+ *
+ * (C) Copyright 2009 David Miller <davem@davemloft.net>
+ * (C) Copyright 2009 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/immediate.h>
+#include <linux/string.h>
+#include <linux/kprobes.h>
+
+#include <asm/system.h>
+
+int arch_imv_update(const struct __imv *imv, int early)
+{
+	unsigned long imv_vaddr = imv->imv;
+	unsigned long var_vaddr = imv->var;
+	u32 insn, *ip = (u32 *) imv_vaddr;
+
+	insn = *ip;
+
+#ifdef CONFIG_KPROBES
+	switch (imv->size) {
+	case 1:
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	if (unlikely(!early &&
+		     (insn == BREAKPOINT_INSTRUCTION ||
+		      insn == BREAKPOINT_INSTRUCTION_2))) {
+		printk(KERN_WARNING "Immediate value in conflict with kprobe. "
+				    "Variable at %p, "
+				    "instruction at %p, size %u\n",
+				    ip, (void *)var_vaddr, imv->size);
+		return -EBUSY;
+	}
+#endif
+
+	switch (imv->size) {
+	case 1:
+		if ((insn & 0x1fff) == *(uint8_t *)var_vaddr)
+			return 0;
+		insn &= ~0x00001fff;
+		insn |= (u32) (*(uint8_t *)var_vaddr);
+		break;
+	default:
+		return -EINVAL;
+	}
+	*ip = insn;
+	flushi(ip);
+	return 0;
+}
diff --git a/stblinux-2.6.31/arch/sparc/kernel/process_32.c b/stblinux-2.6.31/arch/sparc/kernel/process_32.c
index 2830b41..712b67e 100644
--- a/stblinux-2.6.31/arch/sparc/kernel/process_32.c
+++ b/stblinux-2.6.31/arch/sparc/kernel/process_32.c
@@ -24,6 +24,7 @@
 #include <linux/delay.h>
 #include <linux/pm.h>
 #include <linux/init.h>
+#include <trace/sched.h>
 
 #include <asm/auxio.h>
 #include <asm/oplib.h>
@@ -39,6 +40,8 @@
 #include <asm/prom.h>
 #include <asm/unistd.h>
 
+DEFINE_TRACE(sched_kthread_create);
+
 /* 
  * Power management idle function 
  * Set in pm platform drivers (apc.c and pmc.c)
@@ -672,6 +675,7 @@ pid_t kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
 			     "i" (__NR_clone), "r" (flags | CLONE_VM | CLONE_UNTRACED),
 			     "i" (__NR_exit),  "r" (fn), "r" (arg) :
 			     "g1", "g2", "g3", "o0", "o1", "memory", "cc");
+	trace_sched_kthread_create(fn, retval);
 	return retval;
 }
 EXPORT_SYMBOL(kernel_thread);
diff --git a/stblinux-2.6.31/arch/sparc/kernel/sys_sparc_32.c b/stblinux-2.6.31/arch/sparc/kernel/sys_sparc_32.c
index 03035c8..683b3a9 100644
--- a/stblinux-2.6.31/arch/sparc/kernel/sys_sparc_32.c
+++ b/stblinux-2.6.31/arch/sparc/kernel/sys_sparc_32.c
@@ -21,12 +21,15 @@
 #include <linux/smp.h>
 #include <linux/smp_lock.h>
 #include <linux/ipc.h>
+#include <trace/ipc.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
 
 /* #define DEBUG_UNIMP_SYSCALL */
 
+DEFINE_TRACE(ipc_call);
+
 /* XXX Make this per-binary type, this way we can detect the type of
  * XXX a binary.  Every Sparc executable calls this very early on.
  */
@@ -119,6 +122,8 @@ asmlinkage int sys_ipc (uint call, int first, int second, int third, void __user
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_ipc_call(call, first);
+
 	if (call <= SEMCTL)
 		switch (call) {
 		case SEMOP:
diff --git a/stblinux-2.6.31/arch/sparc/kernel/time_64.c b/stblinux-2.6.31/arch/sparc/kernel/time_64.c
index da1218e..29cb272 100644
--- a/stblinux-2.6.31/arch/sparc/kernel/time_64.c
+++ b/stblinux-2.6.31/arch/sparc/kernel/time_64.c
@@ -794,7 +794,8 @@ static void __init setup_clockevent_multiplier(unsigned long hz)
 	sparc64_clockevent.mult = mult;
 }
 
-static unsigned long tb_ticks_per_usec __read_mostly;
+unsigned long tb_ticks_per_usec __read_mostly;
+EXPORT_SYMBOL_GPL(tb_ticks_per_usec);
 
 void __delay(unsigned long loops)
 {
diff --git a/stblinux-2.6.31/arch/um/include/asm/thread_info.h b/stblinux-2.6.31/arch/um/include/asm/thread_info.h
index fd911f8..dacf076 100644
--- a/stblinux-2.6.31/arch/um/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/um/include/asm/thread_info.h
@@ -69,6 +69,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_MEMDIE	 	5
 #define TIF_SYSCALL_AUDIT	6
 #define TIF_RESTORE_SIGMASK	7
+#define TIF_KERNEL_TRACE	8	/* kernel trace active */
 #define TIF_FREEZE		16	/* is freezing for suspend */
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
@@ -78,6 +79,7 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_MEMDIE		(1 << TIF_MEMDIE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_RESTORE_SIGMASK	(1 << TIF_RESTORE_SIGMASK)
+#define _TIF_KERNEL_TRACE	(1 << TIF_KERNEL_TRACE)
 #define _TIF_FREEZE		(1 << TIF_FREEZE)
 
 #endif
diff --git a/stblinux-2.6.31/arch/x86/Kconfig b/stblinux-2.6.31/arch/x86/Kconfig
index 13ffa5d..fcc7c30 100644
--- a/stblinux-2.6.31/arch/x86/Kconfig
+++ b/stblinux-2.6.31/arch/x86/Kconfig
@@ -22,6 +22,7 @@ config X86
 	select HAVE_READQ
 	select HAVE_WRITEQ
 	select HAVE_UNSTABLE_SCHED_CLOCK
+	select HAVE_GET_CYCLES
 	select HAVE_IDE
 	select HAVE_OPROFILE
 	select HAVE_PERF_COUNTERS if (!M386 && !M486)
@@ -31,8 +32,10 @@ config X86
 	select ARCH_WANT_FRAME_POINTERS
 	select HAVE_DMA_ATTRS
 	select HAVE_KRETPROBES
+	select HAVE_TRACE_CLOCK
 	select HAVE_FTRACE_MCOUNT_RECORD
 	select HAVE_DYNAMIC_FTRACE
+	select HAVE_LTT_DUMP_TABLES
 	select HAVE_FUNCTION_TRACER
 	select HAVE_FUNCTION_GRAPH_TRACER
 	select HAVE_FUNCTION_GRAPH_FP_TEST
@@ -43,8 +46,10 @@ config X86
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_TRACEHOOK
 	select HAVE_GENERIC_DMA_COHERENT if X86_32
+	select HAVE_PSRWLOCK_ASM_CALL if X86_64
 	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 	select USER_STACKTRACE_SUPPORT
+	select HAVE_IMMEDIATE
 	select HAVE_DMA_API_DEBUG
 	select HAVE_KERNEL_GZIP
 	select HAVE_KERNEL_BZIP2
@@ -203,10 +208,12 @@ config USE_GENERIC_SMP_HELPERS
 config X86_32_SMP
 	def_bool y
 	depends on X86_32 && SMP
+	select HAVE_UNSYNCHRONIZED_TSC
 
 config X86_64_SMP
 	def_bool y
 	depends on X86_64 && SMP
+	select HAVE_UNSYNCHRONIZED_TSC
 
 config X86_HT
 	bool
diff --git a/stblinux-2.6.31/arch/x86/Makefile b/stblinux-2.6.31/arch/x86/Makefile
index 1b68659..b6857d7 100644
--- a/stblinux-2.6.31/arch/x86/Makefile
+++ b/stblinux-2.6.31/arch/x86/Makefile
@@ -41,6 +41,7 @@ ifeq ($(CONFIG_X86_32),y)
 
         # temporary until string.h is fixed
         KBUILD_CFLAGS += -ffreestanding
+        export USE_IMMEDIATE := $(CONFIG_IMMEDIATE)
 else
         BITS := 64
         UTS_MACHINE := x86_64
@@ -68,6 +69,10 @@ else
         # this works around some issues with generating unwind tables in older gccs
         # newer gccs do it by default
         KBUILD_CFLAGS += -maccumulate-outgoing-args
+
+        # x86_64 gcc 3.x has problems with passing symbol+offset in
+        # asm "i" constraint.
+        export USE_IMMEDIATE := $(call cc-ifversion, -ge, 0400, $(CONFIG_IMMEDIATE))
 endif
 
 ifdef CONFIG_CC_STACKPROTECTOR
diff --git a/stblinux-2.6.31/arch/x86/ia32/ipc32.c b/stblinux-2.6.31/arch/x86/ia32/ipc32.c
index 29cdcd0..accd6b4 100644
--- a/stblinux-2.6.31/arch/x86/ia32/ipc32.c
+++ b/stblinux-2.6.31/arch/x86/ia32/ipc32.c
@@ -8,8 +8,11 @@
 #include <linux/shm.h>
 #include <linux/ipc.h>
 #include <linux/compat.h>
+#include <trace/ipc.h>
 #include <asm/sys_ia32.h>
 
+DEFINE_TRACE(ipc_call);
+
 asmlinkage long sys32_ipc(u32 call, int first, int second, int third,
 			  compat_uptr_t ptr, u32 fifth)
 {
@@ -18,6 +21,8 @@ asmlinkage long sys32_ipc(u32 call, int first, int second, int third,
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_ipc_call(call, first);
+
 	switch (call) {
 	case SEMOP:
 		/* struct sembuf is the same on 32 and 64bit :)) */
diff --git a/stblinux-2.6.31/arch/x86/include/asm/idle.h b/stblinux-2.6.31/arch/x86/include/asm/idle.h
index 38d8737..c288e3c 100644
--- a/stblinux-2.6.31/arch/x86/include/asm/idle.h
+++ b/stblinux-2.6.31/arch/x86/include/asm/idle.h
@@ -1,20 +1,8 @@
 #ifndef _ASM_X86_IDLE_H
 #define _ASM_X86_IDLE_H
 
-#define IDLE_START 1
-#define IDLE_END 2
-
-struct notifier_block;
-void idle_notifier_register(struct notifier_block *n);
-void idle_notifier_unregister(struct notifier_block *n);
-
-#ifdef CONFIG_X86_64
 void enter_idle(void);
 void exit_idle(void);
-#else /* !CONFIG_X86_64 */
-static inline void enter_idle(void) { }
-static inline void exit_idle(void) { }
-#endif /* CONFIG_X86_64 */
 
 void c1e_remove_cpu(int cpu);
 
diff --git a/stblinux-2.6.31/arch/x86/include/asm/immediate.h b/stblinux-2.6.31/arch/x86/include/asm/immediate.h
new file mode 100644
index 0000000..14f30ec
--- /dev/null
+++ b/stblinux-2.6.31/arch/x86/include/asm/immediate.h
@@ -0,0 +1,112 @@
+#ifndef _ASM_X86_IMMEDIATE_H
+#define _ASM_X86_IMMEDIATE_H
+
+/*
+ * Immediate values. x86 architecture optimizations.
+ *
+ * (C) Copyright 2006 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * Dual BSD/GPL v2 license.
+ */
+
+#include <asm/asm.h>
+
+struct __imv {
+	unsigned long var;	/* Pointer to the identifier variable of the
+				 * immediate value
+				 */
+	unsigned long imv;	/*
+				 * Pointer to the memory location of the
+				 * immediate value within the instruction.
+				 */
+	unsigned char size;	/* Type size. */
+	unsigned char insn_size;/* Instruction size. */
+} __attribute__ ((packed));
+
+/**
+ * imv_read - read immediate variable
+ * @name: immediate value name
+ *
+ * Reads the value of @name.
+ * Optimized version of the immediate.
+ * Do not use in __init and __exit functions. Use _imv_read() instead.
+ * If size is bigger than the architecture long size, fall back on a memory
+ * read.
+ *
+ * Make sure to populate the initial static 64 bits opcode with a value
+ * what will generate an instruction with 8 bytes immediate value (not the REX.W
+ * prefixed one that loads a sign extended 32 bits immediate value in a r64
+ * register).
+ *
+ * Create the instruction in a discarded section to calculate its size. This is
+ * how we can align the beginning of the instruction on an address that will
+ * permit atomic modification of the immediate value without knowing the size of
+ * the opcode used by the compiler. The operand size is known in advance.
+ */
+#define imv_read(name)							\
+	({								\
+		__typeof__(name##__imv) value;				\
+		BUILD_BUG_ON(sizeof(value) > 8);			\
+		switch (sizeof(value)) {				\
+		case 1:							\
+			asm(".section __discard,\"\",@progbits\n\t"	\
+				"1:\n\t"				\
+				"mov $0,%0\n\t"				\
+				"2:\n\t"				\
+				".previous\n\t"				\
+				".section __imv,\"aw\",@progbits\n\t"	\
+				_ASM_PTR "%c1, (3f)-%c2\n\t"		\
+				".byte %c2, (2b-1b)\n\t"		\
+				".previous\n\t"				\
+				"mov $0,%0\n\t"				\
+				"3:\n\t"				\
+				: "=q" (value)				\
+				: "i" (&name##__imv),			\
+				  "i" (sizeof(value)));			\
+			break;						\
+		case 2:							\
+		case 4:							\
+			asm(".section __discard,\"\",@progbits\n\t"	\
+				"1:\n\t"				\
+				"mov $0,%0\n\t"				\
+				"2:\n\t"				\
+				".previous\n\t"				\
+				".section __imv,\"aw\",@progbits\n\t"	\
+				_ASM_PTR "%c1, (3f)-%c2\n\t"		\
+				".byte %c2, (2b-1b)\n\t"		\
+				".previous\n\t"				\
+				".org . + ((-.-(2b-1b)) & (%c2-1)), 0x90\n\t" \
+				"mov $0,%0\n\t"				\
+				"3:\n\t"				\
+				: "=r" (value)				\
+				: "i" (&name##__imv),			\
+				  "i" (sizeof(value)));			\
+			break;						\
+		case 8:							\
+			if (sizeof(long) < 8) {				\
+				value = name##__imv;			\
+				break;					\
+			}						\
+			asm(".section __discard,\"\",@progbits\n\t"	\
+				"1:\n\t"				\
+				"mov $0xFEFEFEFE01010101,%0\n\t"	\
+				"2:\n\t"				\
+				".previous\n\t"				\
+				".section __imv,\"aw\",@progbits\n\t"	\
+				_ASM_PTR "%c1, (3f)-%c2\n\t"		\
+				".byte %c2, (2b-1b)\n\t"		\
+				".previous\n\t"				\
+				".org . + ((-.-(2b-1b)) & (%c2-1)), 0x90\n\t" \
+				"mov $0xFEFEFEFE01010101,%0\n\t" 	\
+				"3:\n\t"				\
+				: "=r" (value)				\
+				: "i" (&name##__imv),			\
+				  "i" (sizeof(value)));			\
+			break;						\
+		};							\
+		value;							\
+	})
+
+extern int arch_imv_update(const struct __imv *imv, int early);
+
+#endif /* _ASM_X86_IMMEDIATE_H */
diff --git a/stblinux-2.6.31/arch/x86/include/asm/irqflags.h b/stblinux-2.6.31/arch/x86/include/asm/irqflags.h
index c6ccbe7..49b2a55 100644
--- a/stblinux-2.6.31/arch/x86/include/asm/irqflags.h
+++ b/stblinux-2.6.31/arch/x86/include/asm/irqflags.h
@@ -57,6 +57,61 @@ static inline void native_halt(void)
 
 #endif
 
+#ifdef CONFIG_X86_64
+/*
+ * Only returns from a trap or exception to a NMI context (intra-privilege
+ * level near return) to the same SS and CS segments. Should be used
+ * upon trap or exception return when nested over a NMI context so no iret is
+ * issued. It takes care of modifying the eflags, rsp and returning to the
+ * previous function.
+ *
+ * The stack, at that point, looks like :
+ *
+ * 0(rsp)  RIP
+ * 8(rsp)  CS
+ * 16(rsp) EFLAGS
+ * 24(rsp) RSP
+ * 32(rsp) SS
+ *
+ * Upon execution :
+ * Copy EIP to the top of the return stack
+ * Update top of return stack address
+ * Pop eflags into the eflags register
+ * Make the return stack current
+ * Near return (popping the return address from the return stack)
+ */
+#define NATIVE_INTERRUPT_RETURN_NMI_SAFE	pushq %rax;		\
+						movq %rsp, %rax;	\
+						movq 24+8(%rax), %rsp;	\
+						pushq 0+8(%rax);	\
+						pushq 16+8(%rax);	\
+						movq (%rax), %rax;	\
+						popfq;			\
+						ret
+#else
+/*
+ * Protected mode only, no V8086. Implies that protected mode must
+ * be entered before NMIs or MCEs are enabled. Only returns from a trap or
+ * exception to a NMI context (intra-privilege level far return). Should be used
+ * upon trap or exception return when nested over a NMI context so no iret is
+ * issued.
+ *
+ * The stack, at that point, looks like :
+ *
+ * 0(esp) EIP
+ * 4(esp) CS
+ * 8(esp) EFLAGS
+ *
+ * Upon execution :
+ * Copy the stack eflags to top of stack
+ * Pop eflags into the eflags register
+ * Far return: pop EIP and CS into their register, and additionally pop EFLAGS.
+ */
+#define NATIVE_INTERRUPT_RETURN_NMI_SAFE	pushl 8(%esp);	\
+						popfl;		\
+						lret $4
+#endif
+
 #ifdef CONFIG_PARAVIRT
 #include <asm/paravirt.h>
 #else
@@ -115,6 +170,7 @@ static inline unsigned long __raw_local_irq_save(void)
 
 #define ENABLE_INTERRUPTS(x)	sti
 #define DISABLE_INTERRUPTS(x)	cli
+#define INTERRUPT_RETURN_NMI_SAFE	NATIVE_INTERRUPT_RETURN_NMI_SAFE
 
 #ifdef CONFIG_X86_64
 #define SWAPGS	swapgs
diff --git a/stblinux-2.6.31/arch/x86/include/asm/paravirt.h b/stblinux-2.6.31/arch/x86/include/asm/paravirt.h
index 43b8adb..043e23d 100644
--- a/stblinux-2.6.31/arch/x86/include/asm/paravirt.h
+++ b/stblinux-2.6.31/arch/x86/include/asm/paravirt.h
@@ -201,6 +201,7 @@ struct pv_cpu_ops {
 	/* Normal iret.  Jump to this with the standard iret stack
 	   frame set up. */
 	void (*iret)(void);
+	void (*nmi_return)(void);
 
 	void (*swapgs)(void);
 
@@ -1677,6 +1678,10 @@ static inline unsigned long __raw_local_irq_save(void)
 	PARA_SITE(PARA_PATCH(pv_cpu_ops, PV_CPU_iret), CLBR_NONE,	\
 		  jmp PARA_INDIRECT(pv_cpu_ops+PV_CPU_iret))
 
+#define INTERRUPT_RETURN_NMI_SAFE					\
+	PARA_SITE(PARA_PATCH(pv_cpu_ops, PV_CPU_nmi_return), CLBR_NONE,	\
+		  jmp *%cs:pv_cpu_ops+PV_CPU_nmi_return)
+
 #define DISABLE_INTERRUPTS(clobbers)					\
 	PARA_SITE(PARA_PATCH(pv_irq_ops, PV_IRQ_irq_disable), clobbers, \
 		  PV_SAVE_REGS(clobbers | CLBR_CALLEE_SAVE);		\
diff --git a/stblinux-2.6.31/arch/x86/include/asm/thread_info.h b/stblinux-2.6.31/arch/x86/include/asm/thread_info.h
index e4bcf0c..98d0f10 100644
--- a/stblinux-2.6.31/arch/x86/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/x86/include/asm/thread_info.h
@@ -82,6 +82,7 @@ struct thread_info {
 #define TIF_SYSCALL_EMU		6	/* syscall emulation active */
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SECCOMP		8	/* secure computing */
+#define TIF_KERNEL_TRACE	9	/* kernel trace active */
 #define TIF_MCE_NOTIFY		10	/* notify userspace of an MCE */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* 32bit process */
@@ -105,6 +106,7 @@ struct thread_info {
 #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
+#define _TIF_KERNEL_TRACE	(1 << TIF_KERNEL_TRACE)
 #define _TIF_MCE_NOTIFY		(1 << TIF_MCE_NOTIFY)
 #define _TIF_NOTSC		(1 << TIF_NOTSC)
 #define _TIF_IA32		(1 << TIF_IA32)
@@ -121,18 +123,19 @@ struct thread_info {
 /* work to do in syscall_trace_enter() */
 #define _TIF_WORK_SYSCALL_ENTRY	\
 	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_EMU | _TIF_SYSCALL_FTRACE |	\
-	 _TIF_SYSCALL_AUDIT | _TIF_SECCOMP | _TIF_SINGLESTEP)
+	 _TIF_SYSCALL_AUDIT | _TIF_SECCOMP | _TIF_SINGLESTEP | \
+	 _TIF_KERNEL_TRACE)
 
 /* work to do in syscall_trace_leave() */
 #define _TIF_WORK_SYSCALL_EXIT	\
 	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SINGLESTEP |	\
-	 _TIF_SYSCALL_FTRACE)
+	 _TIF_SYSCALL_FTRACE | _TIF_KERNEL_TRACE)
 
 /* work to do on interrupt/exception return */
 #define _TIF_WORK_MASK							\
 	(0x0000FFFF &							\
 	 ~(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|			\
-	   _TIF_SINGLESTEP|_TIF_SECCOMP|_TIF_SYSCALL_EMU))
+	   _TIF_SINGLESTEP|_TIF_SECCOMP|_TIF_SYSCALL_EMU|_TIF_KERNEL_TRACE))
 
 /* work to do on any return to user space */
 #define _TIF_ALLWORK_MASK ((0x0000FFFF & ~_TIF_SECCOMP) | _TIF_SYSCALL_FTRACE)
diff --git a/stblinux-2.6.31/arch/x86/include/asm/trace-clock.h b/stblinux-2.6.31/arch/x86/include/asm/trace-clock.h
new file mode 100644
index 0000000..ccdcb67
--- /dev/null
+++ b/stblinux-2.6.31/arch/x86/include/asm/trace-clock.h
@@ -0,0 +1,70 @@
+#ifndef _ASM_X86_TRACE_CLOCK_H
+#define _ASM_X86_TRACE_CLOCK_H
+
+/*
+ * linux/arch/x86/include/asm/trace-clock.h
+ *
+ * Copyright (C) 2005,2006,2008
+ *   Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Trace clock definitions for x86.
+ */
+
+#include <linux/timex.h>
+#include <asm/system.h>
+#include <asm/processor.h>
+#include <asm/atomic.h>
+
+/* Minimum duration of a probe, in cycles */
+#define TRACE_CLOCK_MIN_PROBE_DURATION 200
+
+extern cycles_t trace_clock_async_tsc_read(void);
+
+extern int _trace_clock_is_sync;
+static inline int trace_clock_is_sync(void)
+{
+	return _trace_clock_is_sync;
+}
+
+static inline u32 trace_clock_read32(void)
+{
+	u32 cycles;
+
+	if (likely(trace_clock_is_sync())) {
+		get_cycles_barrier();
+		cycles = (u32)get_cycles(); /* only need the 32 LSB */
+		get_cycles_barrier();
+	} else
+		cycles = (u32)trace_clock_async_tsc_read();
+	return cycles;
+}
+
+static inline u64 trace_clock_read64(void)
+{
+	u64 cycles;
+
+	if (likely(trace_clock_is_sync())) {
+		get_cycles_barrier();
+		cycles = get_cycles();
+		get_cycles_barrier();
+	} else
+		cycles = trace_clock_async_tsc_read();
+	return cycles;
+}
+
+static inline u64 trace_clock_frequency(void)
+{
+	return (u64)cpu_khz * 1000;
+}
+
+static inline u32 trace_clock_freq_scale(void)
+{
+	return 1;
+}
+
+extern void get_trace_clock(void);
+extern void put_trace_clock(void);
+
+extern void set_trace_clock_is_sync(int state);
+
+#endif /* _ASM_X86_TRACE_CLOCK_H */
diff --git a/stblinux-2.6.31/arch/x86/include/asm/tsc.h b/stblinux-2.6.31/arch/x86/include/asm/tsc.h
index 38ae163..4efd2b3 100644
--- a/stblinux-2.6.31/arch/x86/include/asm/tsc.h
+++ b/stblinux-2.6.31/arch/x86/include/asm/tsc.h
@@ -48,7 +48,19 @@ static __always_inline cycles_t vget_cycles(void)
 extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);
-int check_tsc_unstable(void);
+extern int check_tsc_unstable(void);
+
+static inline cycles_t get_cycles_rate(void)
+{
+	if (check_tsc_unstable())
+		return 0;
+	return (cycles_t)tsc_khz * 1000;
+}
+
+static inline void get_cycles_barrier(void)
+{
+	rdtsc_barrier();
+}
 
 /*
  * Boot-time check whether the TSCs are synchronized across
@@ -59,4 +71,10 @@ extern void check_tsc_sync_target(void);
 
 extern int notsc_setup(char *);
 
+extern int test_tsc_synchronization(void);
+extern int _tsc_is_sync;
+static inline int tsc_is_sync(void)
+{
+	return _tsc_is_sync;
+}
 #endif /* _ASM_X86_TSC_H */
diff --git a/stblinux-2.6.31/arch/x86/include/asm/unistd_32.h b/stblinux-2.6.31/arch/x86/include/asm/unistd_32.h
index 732a307..63ca43b 100644
--- a/stblinux-2.6.31/arch/x86/include/asm/unistd_32.h
+++ b/stblinux-2.6.31/arch/x86/include/asm/unistd_32.h
@@ -343,6 +343,8 @@
 #define __NR_rt_tgsigqueueinfo	335
 #define __NR_perf_counter_open	336
 
+#define NR_syscalls		337
+
 #ifdef __KERNEL__
 
 #define __ARCH_WANT_IPC_PARSE_VERSION
diff --git a/stblinux-2.6.31/arch/x86/kernel/Makefile b/stblinux-2.6.31/arch/x86/kernel/Makefile
index 430d5b2..d02d79a 100644
--- a/stblinux-2.6.31/arch/x86/kernel/Makefile
+++ b/stblinux-2.6.31/arch/x86/kernel/Makefile
@@ -42,6 +42,7 @@ obj-y			+= bootflag.o e820.o
 obj-y			+= pci-dma.o quirks.o i8237.o topology.o kdebugfs.o
 obj-y			+= alternative.o i8253.o pci-nommu.o
 obj-y			+= tsc.o io_delay.o rtc.o
+obj-$(CONFIG_HAVE_TRACE_CLOCK)	+= trace-clock.o
 
 obj-$(CONFIG_X86_TRAMPOLINE)	+= trampoline.o
 obj-y				+= process.o
@@ -63,9 +64,8 @@ obj-$(CONFIG_PCI)		+= early-quirks.o
 apm-y				:= apm_32.o
 obj-$(CONFIG_APM)		+= apm.o
 obj-$(CONFIG_SMP)		+= smp.o
-obj-$(CONFIG_SMP)		+= smpboot.o tsc_sync.o
+obj-$(CONFIG_SMP)		+= smpboot.o
 obj-$(CONFIG_SMP)		+= setup_percpu.o
-obj-$(CONFIG_X86_64_SMP)	+= tsc_sync.o
 obj-$(CONFIG_X86_TRAMPOLINE)	+= trampoline_$(BITS).o
 obj-$(CONFIG_X86_MPPARSE)	+= mpparse.o
 obj-y				+= apic/
@@ -78,6 +78,7 @@ obj-$(CONFIG_KEXEC)		+= relocate_kernel_$(BITS).o crash.o
 obj-$(CONFIG_CRASH_DUMP)	+= crash_dump_$(BITS).o
 obj-$(CONFIG_KPROBES)		+= kprobes.o
 obj-$(CONFIG_MODULES)		+= module.o
+obj-$(USE_IMMEDIATE)		+= immediate.o
 obj-$(CONFIG_EFI) 		+= efi.o efi_$(BITS).o efi_stub_$(BITS).o
 obj-$(CONFIG_DOUBLEFAULT) 	+= doublefault_32.o
 obj-$(CONFIG_KGDB)		+= kgdb.o
@@ -114,6 +115,9 @@ obj-$(CONFIG_X86_CHECK_BIOS_CORRUPTION) += check.o
 
 obj-$(CONFIG_SWIOTLB)			+= pci-swiotlb.o
 
+obj-$(CONFIG_HAVE_PSRWLOCK_ASM_CALL)	+= call_64.o
+obj-$(CONFIG_HAVE_PSRWLOCK_ASM_CALL)	+= call_export_64.o
+
 ###
 # 64 bit specific files
 ifeq ($(CONFIG_X86_64),y)
diff --git a/stblinux-2.6.31/arch/x86/kernel/apic/apic.c b/stblinux-2.6.31/arch/x86/kernel/apic/apic.c
index 0a1c283..9a5f3e8 100644
--- a/stblinux-2.6.31/arch/x86/kernel/apic/apic.c
+++ b/stblinux-2.6.31/arch/x86/kernel/apic/apic.c
@@ -34,6 +34,7 @@
 #include <linux/nmi.h>
 #include <linux/smp.h>
 #include <linux/mm.h>
+#include <trace/irq.h>
 
 #include <asm/perf_counter.h>
 #include <asm/pgalloc.h>
@@ -841,7 +842,9 @@ void __irq_entry smp_apic_timer_interrupt(struct pt_regs *regs)
 	 */
 	exit_idle();
 	irq_enter();
+	trace_irq_entry(LOCAL_TIMER_VECTOR, regs, NULL);
 	local_apic_timer_interrupt();
+	trace_irq_exit(IRQ_HANDLED);
 	irq_exit();
 
 	set_irq_regs(old_regs);
@@ -1722,6 +1725,7 @@ void smp_spurious_interrupt(struct pt_regs *regs)
 
 	exit_idle();
 	irq_enter();
+	trace_irq_entry(SPURIOUS_APIC_VECTOR, NULL, NULL);
 	/*
 	 * Check if this really is a spurious interrupt and ACK it
 	 * if it is a vectored one.  Just in case...
@@ -1736,6 +1740,7 @@ void smp_spurious_interrupt(struct pt_regs *regs)
 	/* see sw-dev-man vol 3, chapter 7.4.13.5 */
 	pr_info("spurious APIC interrupt on CPU#%d, "
 		"should never happen.\n", smp_processor_id());
+	trace_irq_exit(IRQ_HANDLED);
 	irq_exit();
 }
 
@@ -1748,6 +1753,7 @@ void smp_error_interrupt(struct pt_regs *regs)
 
 	exit_idle();
 	irq_enter();
+	trace_irq_entry(ERROR_APIC_VECTOR, NULL, NULL);
 	/* First tickle the hardware, only then report what went on. -- REW */
 	v = apic_read(APIC_ESR);
 	apic_write(APIC_ESR, 0);
@@ -1768,6 +1774,7 @@ void smp_error_interrupt(struct pt_regs *regs)
 	 */
 	pr_debug("APIC error on CPU%d: %02x(%02x)\n",
 		smp_processor_id(), v , v1);
+	trace_irq_exit(IRQ_HANDLED);
 	irq_exit();
 }
 
diff --git a/stblinux-2.6.31/arch/x86/kernel/apm_32.c b/stblinux-2.6.31/arch/x86/kernel/apm_32.c
index 442b550..e643abb 100644
--- a/stblinux-2.6.31/arch/x86/kernel/apm_32.c
+++ b/stblinux-2.6.31/arch/x86/kernel/apm_32.c
@@ -228,6 +228,7 @@
 #include <linux/suspend.h>
 #include <linux/kthread.h>
 #include <linux/jiffies.h>
+#include <linux/idle.h>
 
 #include <asm/system.h>
 #include <asm/uaccess.h>
@@ -939,10 +940,15 @@ recalc:
 				break;
 			}
 		}
+		enter_idle();
 		if (original_pm_idle)
 			original_pm_idle();
 		else
 			default_idle();
+		/* In many cases the interrupt that ended idle
+		   has already called exit_idle. But some idle
+		   loops can be woken up without interrupt. */
+		__exit_idle();
 		local_irq_disable();
 		jiffies_since_last_check = jiffies - last_jiffies;
 		if (jiffies_since_last_check > idle_period)
diff --git a/stblinux-2.6.31/arch/x86/kernel/asm-offsets_32.c b/stblinux-2.6.31/arch/x86/kernel/asm-offsets_32.c
index dfdbf64..c486e52 100644
--- a/stblinux-2.6.31/arch/x86/kernel/asm-offsets_32.c
+++ b/stblinux-2.6.31/arch/x86/kernel/asm-offsets_32.c
@@ -113,6 +113,7 @@ void foo(void)
 	OFFSET(PV_IRQ_irq_disable, pv_irq_ops, irq_disable);
 	OFFSET(PV_IRQ_irq_enable, pv_irq_ops, irq_enable);
 	OFFSET(PV_CPU_iret, pv_cpu_ops, iret);
+	OFFSET(PV_CPU_nmi_return, pv_cpu_ops, nmi_return);
 	OFFSET(PV_CPU_irq_enable_sysexit, pv_cpu_ops, irq_enable_sysexit);
 	OFFSET(PV_CPU_read_cr0, pv_cpu_ops, read_cr0);
 #endif
diff --git a/stblinux-2.6.31/arch/x86/kernel/asm-offsets_64.c b/stblinux-2.6.31/arch/x86/kernel/asm-offsets_64.c
index 898ecc4..bf0c55a 100644
--- a/stblinux-2.6.31/arch/x86/kernel/asm-offsets_64.c
+++ b/stblinux-2.6.31/arch/x86/kernel/asm-offsets_64.c
@@ -57,6 +57,7 @@ int main(void)
 	OFFSET(PV_IRQ_irq_enable, pv_irq_ops, irq_enable);
 	OFFSET(PV_IRQ_adjust_exception_frame, pv_irq_ops, adjust_exception_frame);
 	OFFSET(PV_CPU_iret, pv_cpu_ops, iret);
+	OFFSET(PV_CPU_nmi_return, pv_cpu_ops, nmi_return);
 	OFFSET(PV_CPU_usergs_sysret32, pv_cpu_ops, usergs_sysret32);
 	OFFSET(PV_CPU_usergs_sysret64, pv_cpu_ops, usergs_sysret64);
 	OFFSET(PV_CPU_irq_enable_sysexit, pv_cpu_ops, irq_enable_sysexit);
diff --git a/stblinux-2.6.31/arch/x86/kernel/call_64.S b/stblinux-2.6.31/arch/x86/kernel/call_64.S
new file mode 100644
index 0000000..a5e6766
--- /dev/null
+++ b/stblinux-2.6.31/arch/x86/kernel/call_64.S
@@ -0,0 +1,47 @@
+/*
+ * linux/arch/x86/kernel/call_64.S -- special 64-bits calling conventions
+ *
+ * Copyright (C) 2008 Mathieu Desnoyers
+ */
+
+#include <linux/linkage.h>
+
+/*
+ * Called by call_rax_rsi().
+ *
+ * Move rax to rdi and proceed to the standard call.
+ */
+.macro TRAMPOLINE_RAX_RSI symbol
+ENTRY(asm_\symbol)
+	movq	%rax, %rdi
+	jmp	_\symbol
+END(asm_\symbol)
+.endm
+
+/*
+ * Called by call_rbx_rsi().
+ *
+ * Move rbx to rdi and proceed to the standard call.
+ */
+.macro TRAMPOLINE_RBX_RSI symbol
+ENTRY(asm_\symbol)
+	movq	%rbx, %rdi
+	jmp	_\symbol
+END(asm_\symbol)
+.endm
+
+TRAMPOLINE_RAX_RSI psread_lock_slow_irq
+TRAMPOLINE_RAX_RSI psread_trylock_slow_irq
+TRAMPOLINE_RAX_RSI psread_lock_slow_bh
+TRAMPOLINE_RAX_RSI psread_trylock_slow_bh
+TRAMPOLINE_RAX_RSI psread_lock_slow_inatomic
+TRAMPOLINE_RAX_RSI psread_trylock_slow_inatomic
+TRAMPOLINE_RAX_RSI psread_lock_slow
+TRAMPOLINE_RAX_RSI psread_lock_interruptible_slow
+TRAMPOLINE_RAX_RSI psread_trylock_slow
+
+TRAMPOLINE_RAX_RSI pswrite_lock_slow
+TRAMPOLINE_RAX_RSI pswrite_lock_interruptible_slow
+TRAMPOLINE_RAX_RSI pswrite_trylock_slow
+TRAMPOLINE_RAX_RSI pswrite_unlock_slow
+TRAMPOLINE_RBX_RSI psrwlock_wakeup
diff --git a/stblinux-2.6.31/arch/x86/kernel/call_export_64.c b/stblinux-2.6.31/arch/x86/kernel/call_export_64.c
new file mode 100644
index 0000000..d407049
--- /dev/null
+++ b/stblinux-2.6.31/arch/x86/kernel/call_export_64.c
@@ -0,0 +1,40 @@
+/*
+ * linux/arch/x86/kernel/call_64.c -- special 64-bits calling conventions
+ *
+ * Export function symbols of special calling convention functions.
+ *
+ * Copyright (C) 2008 Mathieu Desnoyers
+ */
+
+#include <linux/module.h>
+#include <asm/call_64.h>
+
+void asm_psread_lock_slow_irq(void);
+EXPORT_SYMBOL_GPL(asm_psread_lock_slow_irq);
+void asm_psread_trylock_slow_irq(void);
+EXPORT_SYMBOL_GPL(asm_psread_trylock_slow_irq);
+void asm_psread_lock_slow_bh(void);
+EXPORT_SYMBOL_GPL(asm_psread_lock_slow_bh);
+void asm_psread_trylock_slow_bh(void);
+EXPORT_SYMBOL_GPL(asm_psread_trylock_slow_bh);
+void asm_psread_lock_slow_inatomic(void);
+EXPORT_SYMBOL_GPL(asm_psread_lock_slow_inatomic);
+void asm_psread_trylock_slow_inatomic(void);
+EXPORT_SYMBOL_GPL(asm_psread_trylock_slow_inatomic);
+void asm_psread_lock_slow(void);
+EXPORT_SYMBOL_GPL(asm_psread_lock_slow);
+void asm_psread_lock_interruptible_slow(void);
+EXPORT_SYMBOL_GPL(asm_psread_lock_interruptible_slow);
+void asm_psread_trylock_slow(void);
+EXPORT_SYMBOL_GPL(asm_psread_trylock_slow);
+
+void asm_pswrite_lock_slow(void);
+EXPORT_SYMBOL_GPL(asm_pswrite_lock_slow);
+void asm_pswrite_lock_interruptible_slow(void);
+EXPORT_SYMBOL_GPL(asm_pswrite_lock_interruptible_slow);
+void asm_pswrite_trylock_slow(void);
+EXPORT_SYMBOL_GPL(asm_pswrite_trylock_slow);
+void asm_pswrite_unlock_slow(void);
+EXPORT_SYMBOL_GPL(asm_pswrite_unlock_slow);
+void asm_psrwlock_wakeup(void);
+EXPORT_SYMBOL_GPL(asm_psrwlock_wakeup);
diff --git a/stblinux-2.6.31/arch/x86/kernel/cpu/common.c b/stblinux-2.6.31/arch/x86/kernel/cpu/common.c
index e338b5c..f109bf8 100644
--- a/stblinux-2.6.31/arch/x86/kernel/cpu/common.c
+++ b/stblinux-2.6.31/arch/x86/kernel/cpu/common.c
@@ -1039,6 +1039,7 @@ unsigned long kernel_eflags;
  * debugging, no special alignment required.
  */
 DEFINE_PER_CPU(struct orig_ist, orig_ist);
+EXPORT_PER_CPU_SYMBOL_GPL(orig_ist);
 
 #else	/* CONFIG_X86_64 */
 
diff --git a/stblinux-2.6.31/arch/x86/kernel/cpu/mcheck/p4.c b/stblinux-2.6.31/arch/x86/kernel/cpu/mcheck/p4.c
index 4482aea..b00052e 100644
--- a/stblinux-2.6.31/arch/x86/kernel/cpu/mcheck/p4.c
+++ b/stblinux-2.6.31/arch/x86/kernel/cpu/mcheck/p4.c
@@ -5,6 +5,7 @@
 #include <linux/types.h>
 #include <linux/init.h>
 #include <linux/smp.h>
+#include <trace/irq.h>
 
 #include <asm/processor.h>
 #include <asm/mce.h>
diff --git a/stblinux-2.6.31/arch/x86/kernel/cpu/mcheck/therm_throt.c b/stblinux-2.6.31/arch/x86/kernel/cpu/mcheck/therm_throt.c
index a14a451..0bb648f 100644
--- a/stblinux-2.6.31/arch/x86/kernel/cpu/mcheck/therm_throt.c
+++ b/stblinux-2.6.31/arch/x86/kernel/cpu/mcheck/therm_throt.c
@@ -23,6 +23,7 @@
 #include <linux/init.h>
 #include <linux/smp.h>
 #include <linux/cpu.h>
+#include <trace/irq.h>
 
 #include <asm/processor.h>
 #include <asm/system.h>
@@ -249,8 +250,10 @@ asmlinkage void smp_thermal_interrupt(struct pt_regs *regs)
 {
 	exit_idle();
 	irq_enter();
+	trace_irq_entry(THERMAL_APIC_VECTOR, regs, NULL);
 	inc_irq_stat(irq_thermal_count);
 	smp_thermal_vector();
+	trace_irq_exit(IRQ_HANDLED);
 	irq_exit();
 	/* Ack only at the end to avoid potential reentry */
 	ack_APIC_irq();
diff --git a/stblinux-2.6.31/arch/x86/kernel/dumpstack.c b/stblinux-2.6.31/arch/x86/kernel/dumpstack.c
index c840571..33b6d2f 100644
--- a/stblinux-2.6.31/arch/x86/kernel/dumpstack.c
+++ b/stblinux-2.6.31/arch/x86/kernel/dumpstack.c
@@ -16,6 +16,7 @@
 #include <linux/nmi.h>
 #include <linux/sysfs.h>
 #include <linux/ftrace.h>
+#include <linux/ltt-core.h>
 
 #include <asm/stacktrace.h>
 
@@ -238,6 +239,8 @@ void __kprobes oops_end(unsigned long flags, struct pt_regs *regs, int signr)
 
 	if (!signr)
 		return;
+	if (in_nmi())
+		panic("Fatal exception in non-maskable interrupt");
 	if (in_interrupt())
 		panic("Fatal exception in interrupt");
 	if (panic_on_oops)
@@ -262,6 +265,10 @@ int __kprobes __die(const char *str, struct pt_regs *regs, long err)
 	printk("DEBUG_PAGEALLOC");
 #endif
 	printk("\n");
+#ifdef CONFIG_LTT
+	printk(KERN_EMERG "LTT NESTING LEVEL : %u", __get_cpu_var(ltt_nesting));
+	printk("\n");
+#endif
 	sysfs_printk_last_file();
 	if (notify_die(DIE_OOPS, str, regs, err,
 			current->thread.trap_no, SIGSEGV) == NOTIFY_STOP)
diff --git a/stblinux-2.6.31/arch/x86/kernel/entry_32.S b/stblinux-2.6.31/arch/x86/kernel/entry_32.S
index c097e7d..402c1c3 100644
--- a/stblinux-2.6.31/arch/x86/kernel/entry_32.S
+++ b/stblinux-2.6.31/arch/x86/kernel/entry_32.S
@@ -79,6 +79,8 @@
 
 #define nr_syscalls ((syscall_table_size)/4)
 
+#define NMI_MASK 0x04000000
+
 #ifdef CONFIG_PREEMPT
 #define preempt_stop(clobbers)	DISABLE_INTERRUPTS(clobbers); TRACE_IRQS_OFF
 #else
@@ -343,8 +345,32 @@ END(ret_from_fork)
 	# userspace resumption stub bypassing syscall exit tracing
 	ALIGN
 	RING0_PTREGS_FRAME
+
 ret_from_exception:
 	preempt_stop(CLBR_ANY)
+	GET_THREAD_INFO(%ebp)
+	movl PT_EFLAGS(%esp), %eax	# mix EFLAGS and CS
+	movb PT_CS(%esp), %al
+	andl $(X86_EFLAGS_VM | SEGMENT_RPL_MASK), %eax
+	cmpl $USER_RPL, %eax
+	jae resume_userspace	# returning to v8086 or userspace
+	testl $NMI_MASK,TI_preempt_count(%ebp)
+	jz resume_kernel		/* Not nested over NMI ? */
+	testw $X86_EFLAGS_TF, PT_EFLAGS(%esp)
+	jnz resume_kernel		/*
+					 * If single-stepping an NMI handler,
+					 * use the normal iret path instead of
+					 * the popf/lret because lret would be
+					 * single-stepped. It should not
+					 * happen : it will reactivate NMIs
+					 * prematurely.
+					 */
+	TRACE_IRQS_IRET
+	RESTORE_REGS
+	addl $4, %esp			# skip orig_eax/error_code
+	CFI_ADJUST_CFA_OFFSET -4
+	INTERRUPT_RETURN_NMI_SAFE
+
 ret_from_intr:
 	GET_THREAD_INFO(%ebp)
 check_userspace:
@@ -871,6 +897,10 @@ ENTRY(native_iret)
 .previous
 END(native_iret)
 
+ENTRY(native_nmi_return)
+	NATIVE_INTERRUPT_RETURN_NMI_SAFE # Should we deal with popf exception ?
+END(native_nmi_return)
+
 ENTRY(native_irq_enable_sysexit)
 	sti
 	sysexit
diff --git a/stblinux-2.6.31/arch/x86/kernel/entry_64.S b/stblinux-2.6.31/arch/x86/kernel/entry_64.S
index c251be7..6d4f13b 100644
--- a/stblinux-2.6.31/arch/x86/kernel/entry_64.S
+++ b/stblinux-2.6.31/arch/x86/kernel/entry_64.S
@@ -163,6 +163,8 @@ GLOBAL(return_to_handler)
 #endif
 
 
+#define NMI_MASK 0x04000000
+
 #ifndef CONFIG_PREEMPT
 #define retint_kernel retint_restore_args
 #endif
@@ -517,6 +519,8 @@ sysret_check:
 	/* Handle reschedules */
 	/* edx:	work, edi: workmask */
 sysret_careful:
+	testl $_TIF_KERNEL_TRACE,%edx	/* Re-read : concurrently changed */
+	jnz ret_from_sys_call_trace
 	bt $TIF_NEED_RESCHED,%edx
 	jnc sysret_signal
 	TRACE_IRQS_ON
@@ -528,6 +532,16 @@ sysret_careful:
 	CFI_ADJUST_CFA_OFFSET -8
 	jmp sysret_check
 
+ret_from_sys_call_trace:
+	TRACE_IRQS_ON
+	sti
+	SAVE_REST
+	FIXUP_TOP_OF_STACK %rdi
+	movq %rsp,%rdi
+	LOAD_ARGS ARGOFFSET  /* reload args from stack in case ptrace changed it */
+	RESTORE_REST
+	jmp int_ret_from_sys_call
+
 	/* Handle a signal */
 sysret_signal:
 	TRACE_IRQS_ON
@@ -877,6 +891,9 @@ ENTRY(native_iret)
 	.section __ex_table,"a"
 	.quad native_iret, bad_iret
 	.previous
+
+ENTRY(native_nmi_return)
+	NATIVE_INTERRUPT_RETURN_NMI_SAFE
 #endif
 
 	.section .fixup,"ax"
@@ -931,6 +948,24 @@ retint_signal:
 	GET_THREAD_INFO(%rcx)
 	jmp retint_with_reschedule
 
+	/* Returning to kernel space from exception. */
+	/* rcx:	 threadinfo. interrupts off. */
+ENTRY(retexc_kernel)
+	testl $NMI_MASK,TI_preempt_count(%rcx)
+	jz retint_kernel		/* Not nested over NMI ? */
+	testw $X86_EFLAGS_TF,EFLAGS-ARGOFFSET(%rsp)	/* trap flag? */
+	jnz retint_kernel		/*
+					 * If single-stepping an NMI handler,
+					 * use the normal iret path instead of
+					 * the popf/lret because lret would be
+					 * single-stepped. It should not
+					 * happen : it will reactivate NMIs
+					 * prematurely.
+					 */
+	RESTORE_ARGS 0,8,0
+	TRACE_IRQS_IRETQ
+	INTERRUPT_RETURN_NMI_SAFE
+
 #ifdef CONFIG_PREEMPT
 	/* Returning to kernel space. Check if we need preemption */
 	/* rcx:	 threadinfo. interrupts off. */
@@ -1173,7 +1208,7 @@ bad_gs:
  * asm input arguments:
  *	rdi: fn, rsi: arg, rdx: flags
  */
-ENTRY(kernel_thread)
+ENTRY(kernel_thread_asm)
 	CFI_STARTPROC
 	FAKE_STACK_FRAME $child_rip
 	SAVE_ALL
@@ -1416,12 +1451,18 @@ ENTRY(paranoid_exit)
 paranoid_swapgs:
 	TRACE_IRQS_IRETQ 0
 	SWAPGS_UNSAFE_STACK
+paranoid_restore_no_nmi:
 	RESTORE_ALL 8
 	jmp irq_return
 paranoid_restore:
+	GET_THREAD_INFO(%rcx)
 	TRACE_IRQS_IRETQ 0
+	testl $NMI_MASK,TI_preempt_count(%rcx)
+	jz paranoid_restore_no_nmi              /* Nested over NMI ? */
+	testw $X86_EFLAGS_TF,EFLAGS-0(%rsp)     /* trap flag? */
+	jnz paranoid_restore_no_nmi
 	RESTORE_ALL 8
-	jmp irq_return
+	INTERRUPT_RETURN_NMI_SAFE
 paranoid_userspace:
 	GET_THREAD_INFO(%rcx)
 	movl TI_flags(%rcx),%ebx
@@ -1515,7 +1556,7 @@ ENTRY(error_exit)
 	TRACE_IRQS_OFF
 	GET_THREAD_INFO(%rcx)
 	testl %eax,%eax
-	jne retint_kernel
+	jne retexc_kernel
 	LOCKDEP_SYS_EXIT_IRQ
 	movl TI_flags(%rcx),%edx
 	movl $_TIF_WORK_MASK,%edi
diff --git a/stblinux-2.6.31/arch/x86/kernel/immediate.c b/stblinux-2.6.31/arch/x86/kernel/immediate.c
new file mode 100644
index 0000000..71f93e1
--- /dev/null
+++ b/stblinux-2.6.31/arch/x86/kernel/immediate.c
@@ -0,0 +1,301 @@
+/*
+ * Immediate Value - x86 architecture specific code.
+ *
+ * Rationale
+ *
+ * Required because of :
+ * - Erratum 49 fix for Intel PIII.
+ * - Still present on newer processors : Intel Core 2 Duo Processor for Intel
+ *   Centrino Duo Processor Technology Specification Update, AH33.
+ *   Unsynchronized Cross-Modifying Code Operations Can Cause Unexpected
+ *   Instruction Execution Results.
+ *
+ * Quoting "Intel Core 2 Duo Processor for IntelCentrino Duo Processor
+ * Technology Specification Update" (AH33)
+ *
+ * "The act of one processor, or system bus master, writing data into a
+ * currently executing code segment of a second processor with the intent of
+ * having the second processor execute that data as code is called
+ * cross-modifying code (XMC). XMC that does not force the second processor to
+ * execute a synchronizing instruction, prior to execution of the new code, is
+ * called unsynchronized XMC. Software using unsynchronized XMC to modify the
+ * instruction byte stream of a processor can see unexpected or unpredictable
+ * execution behavior from the processor that is executing the modified code."
+ *
+ * This code turns what would otherwise be an unsynchronized XMC into a
+ * synchronized XMC by making sure a synchronizing instruction (either iret
+ * returning from the breakpoint or cpuid from sync_core()) is executed by any
+ * CPU before it executes the modified instruction.
+ *
+ * Reentrant for NMI and trap handler instrumentation. Permits XMC to a
+ * location that has preemption enabled because it involves no temporary or
+ * reused data structure.
+ *
+ * Quoting Richard J Moore, source of the information motivating this
+ * implementation which differs from the one proposed by Intel which is not
+ * suitable for kernel context (does not support NMI and would require disabling
+ * interrupts on every CPU for a long period) :
+ *
+ * "There is another issue to consider when looking into using probes other
+ * then int3:
+ *
+ * Intel erratum 54 - Unsynchronized Cross-modifying code - refers to the
+ * practice of modifying code on one processor where another has prefetched
+ * the unmodified version of the code. Intel states that unpredictable general
+ * protection faults may result if a synchronizing instruction (iret, int,
+ * int3, cpuid, etc ) is not executed on the second processor before it
+ * executes the pre-fetched out-of-date copy of the instruction.
+ *
+ * When we became aware of this I had a long discussion with Intel's
+ * microarchitecture guys. It turns out that the reason for this erratum
+ * (which incidentally Intel does not intend to fix) is because the trace
+ * cache - the stream of micro-ops resulting from instruction interpretation -
+ * cannot be guaranteed to be valid. Reading between the lines I assume this
+ * issue arises because of optimization done in the trace cache, where it is
+ * no longer possible to identify the original instruction boundaries. If the
+ * CPU discoverers that the trace cache has been invalidated because of
+ * unsynchronized cross-modification then instruction execution will be
+ * aborted with a GPF. Further discussion with Intel revealed that replacing
+ * the first opcode byte with an int3 would not be subject to this erratum.
+ *
+ * So, is cmpxchg reliable? One has to guarantee more than mere atomicity."
+ *
+ * Overall design
+ *
+ * The algorithm proposed by Intel applies not so well in kernel context: it
+ * would imply disabling interrupts and looping on every CPUs while modifying
+ * the code and would not support instrumentation of code called from interrupt
+ * sources that cannot be disabled.
+ *
+ * Therefore, we use a different algorithm to respect Intel's erratum (see the
+ * quoted discussion above). We make sure that no CPU sees an out-of-date copy
+ * of a pre-fetched instruction by 1 - using a breakpoint, which skips the
+ * instruction that is going to be modified, 2 - issuing an IPI to every CPU to
+ * execute a sync_core(), to make sure that even when the breakpoint is removed,
+ * no cpu could possibly still have the out-of-date copy of the instruction,
+ * modify the now unused 2nd byte of the instruction, and then put back the
+ * original 1st byte of the instruction.
+ *
+ * It has exactly the same intent as the algorithm proposed by Intel, but
+ * it has less side-effects, scales better and supports NMI, SMI and MCE.
+ *
+ * (C) Copyright 2007-2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/preempt.h>
+#include <linux/smp.h>
+#include <linux/notifier.h>
+#include <linux/module.h>
+#include <linux/immediate.h>
+#include <linux/kdebug.h>
+#include <linux/rcupdate.h>
+#include <linux/kprobes.h>
+#include <linux/io.h>
+
+#include <asm/cacheflush.h>
+
+#define BREAKPOINT_INSTRUCTION  0xcc
+#define BREAKPOINT_INS_LEN	1
+#define NR_NOPS			10
+
+static unsigned long target_after_int3;	/* EIP of the target after the int3 */
+static unsigned long bypass_eip;	/* EIP of the bypass. */
+static unsigned long bypass_after_int3;	/* EIP after the end-of-bypass int3 */
+static unsigned long after_imv;	/*
+					 * EIP where to resume after the
+					 * single-stepping.
+					 */
+
+/*
+ * Internal bypass used during value update. The bypass is skipped by the
+ * function in which it is inserted.
+ * No need to be aligned because we exclude readers from the site during
+ * update.
+ * Layout is:
+ * (10x nop) int3
+ * (maximum size is 2 bytes opcode + 8 bytes immediate value for long on x86_64)
+ * The nops are the target replaced by the instruction to single-step.
+ * Align on 16 bytes to make sure the nops fit within a single page so remapping
+ * it can be done easily.
+ */
+static inline void _imv_bypass(unsigned long *bypassaddr,
+	unsigned long *breaknextaddr)
+{
+		asm volatile("jmp 2f;\n\t"
+				".align 16;\n\t"
+				"0:\n\t"
+				".space 10, 0x90;\n\t"
+				"1:\n\t"
+				"int3;\n\t"
+				"2:\n\t"
+				"mov $(0b),%0;\n\t"
+				"mov $((1b)+1),%1;\n\t"
+				: "=r" (*bypassaddr),
+				  "=r" (*breaknextaddr));
+}
+
+static void imv_synchronize_core(void *info)
+{
+	smp_rmb();	/* read new instructions before continuing */
+	sync_core();	/* use cpuid to stop speculative execution */
+}
+
+/*
+ * The eip value points right after the breakpoint instruction, in the second
+ * byte of the movl.
+ * Disable preemption in the bypass to make sure no thread will be preempted in
+ * it. We can then use synchronize_sched() to make sure every bypass users have
+ * ended.
+ */
+static int imv_notifier(struct notifier_block *nb,
+	unsigned long val, void *data)
+{
+	enum die_val die_val = (enum die_val) val;
+	struct die_args *args = data;
+
+	if (!args->regs || user_mode_vm(args->regs))
+		return NOTIFY_DONE;
+
+	if (die_val == DIE_INT3) {
+		if (args->regs->ip == target_after_int3) {
+			preempt_disable();
+			args->regs->ip = bypass_eip;
+			return NOTIFY_STOP;
+		} else if (args->regs->ip == bypass_after_int3) {
+			args->regs->ip = after_imv;
+			preempt_enable();
+			return NOTIFY_STOP;
+		}
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block imv_notify = {
+	.notifier_call = imv_notifier,
+	.priority = 0x7fffffff,	/* we need to be notified first */
+};
+
+/**
+ * arch_imv_update - update one immediate value
+ * @imv: pointer of type const struct __imv to update
+ * @early: early boot (1) or normal (0)
+ *
+ * Update one immediate value. Must be called with imv_mutex held.
+ */
+__kprobes int arch_imv_update(const struct __imv *imv, int early)
+{
+	int ret;
+	unsigned char opcode_size = imv->insn_size - imv->size;
+	unsigned long insn = imv->imv - opcode_size;
+	unsigned long len;
+	void *buffer;
+
+#ifdef CONFIG_KPROBES
+	/*
+	 * Fail if a kprobe has been set on this instruction.
+	 * (TODO: we could eventually do better and modify all the (possibly
+	 * nested) kprobes for this site if kprobes had an API for this.
+	 */
+	if (unlikely(!early
+			&& *(unsigned char *)insn == BREAKPOINT_INSTRUCTION)) {
+		printk(KERN_WARNING "Immediate value in conflict with kprobe. "
+				    "Variable at %p, "
+				    "instruction at %p, size %hu\n",
+				    (void *)imv->imv,
+				    (void *)imv->var, imv->size);
+		return -EBUSY;
+	}
+#endif
+
+	/*
+	 * If the variable and the instruction have the same value, there is
+	 * nothing to do.
+	 */
+	switch (imv->size) {
+	case 1:	if (*(uint8_t *)imv->imv
+				== *(uint8_t *)imv->var)
+			return 0;
+		break;
+	case 2:	if (*(uint16_t *)imv->imv
+				== *(uint16_t *)imv->var)
+			return 0;
+		break;
+	case 4:	if (*(uint32_t *)imv->imv
+				== *(uint32_t *)imv->var)
+			return 0;
+		break;
+#ifdef CONFIG_X86_64
+	case 8:	if (*(uint64_t *)imv->imv
+				== *(uint64_t *)imv->var)
+			return 0;
+		break;
+#endif
+	default:return -EINVAL;
+	}
+
+	if (!early) {
+		/* bypass is 10 bytes long for x86_64 long */
+		WARN_ON(imv->insn_size > 10);
+		_imv_bypass(&bypass_eip, &bypass_after_int3);
+
+		after_imv = imv->imv + imv->size;
+		len = NR_NOPS - imv->insn_size;
+
+		/* Allocate buffer to prepare new bypass */
+		buffer = kmalloc(imv->insn_size + len, GFP_KERNEL);
+		memcpy(buffer, (void *)insn, imv->insn_size);
+		/*
+		 * Fill the rest with nops.
+		 */
+		add_nops(buffer + imv->insn_size, len);
+		text_poke((void *)bypass_eip, buffer, imv->insn_size + len);
+		kfree(buffer);
+
+		target_after_int3 = insn + BREAKPOINT_INS_LEN;
+		/* register_die_notifier has memory barriers */
+		register_die_notifier(&imv_notify);
+		/* The breakpoint will single-step the bypass */
+		text_poke((void *)insn,
+			((unsigned char[]){BREAKPOINT_INSTRUCTION}), 1);
+		/*
+		 * Make sure the breakpoint is set before we continue (visible
+		 * to other CPUs and interrupts).
+		 */
+		smp_wmb();
+		/*
+		 * Execute smp_rmb() and serializing instruction on each CPU.
+		 */
+		ret = on_each_cpu(imv_synchronize_core, NULL, 1);
+		BUG_ON(ret != 0);
+
+		text_poke((void *)(insn + opcode_size), (void *)imv->var,
+				imv->size);
+		/*
+		 * Make sure the value can be seen from other CPUs and
+		 * interrupts.
+		 */
+		smp_wmb();
+		/*
+		 * Execute smp_rmb() on each CPU.
+		 */
+		ret = on_each_cpu(imv_synchronize_core, NULL, 1);
+		BUG_ON(ret != 0);
+		text_poke((void *)insn, (unsigned char *)bypass_eip, 1);
+		/*
+		 * Wait for all int3 handlers to end (interrupts are disabled in
+		 * int3). This CPU is clearly not in a int3 handler, because
+		 * int3 handler is not preemptible and there cannot be any more
+		 * int3 handler called for this site, because we placed the
+		 * original instruction back.  synchronize_sched has memory
+		 * barriers.
+		 */
+		synchronize_sched();
+		unregister_die_notifier(&imv_notify);
+		/* unregister_die_notifier has memory barriers */
+	} else
+		text_poke_early((void *)imv->imv, (void *)imv->var,
+			imv->size);
+	return 0;
+}
diff --git a/stblinux-2.6.31/arch/x86/kernel/paravirt.c b/stblinux-2.6.31/arch/x86/kernel/paravirt.c
index 70ec9b9..4eecdc4 100644
--- a/stblinux-2.6.31/arch/x86/kernel/paravirt.c
+++ b/stblinux-2.6.31/arch/x86/kernel/paravirt.c
@@ -161,6 +161,7 @@ unsigned paravirt_patch_default(u8 type, u16 clobbers, void *insnbuf,
 		ret = paravirt_patch_ident_64(insnbuf, len);
 
 	else if (type == PARAVIRT_PATCH(pv_cpu_ops.iret) ||
+		 type == PARAVIRT_PATCH(pv_cpu_ops.nmi_return) ||
 		 type == PARAVIRT_PATCH(pv_cpu_ops.irq_enable_sysexit) ||
 		 type == PARAVIRT_PATCH(pv_cpu_ops.usergs_sysret32) ||
 		 type == PARAVIRT_PATCH(pv_cpu_ops.usergs_sysret64))
@@ -214,6 +215,7 @@ static void native_flush_tlb_single(unsigned long addr)
 
 /* These are in entry.S */
 extern void native_iret(void);
+extern void native_nmi_return(void);
 extern void native_irq_enable_sysexit(void);
 extern void native_usergs_sysret32(void);
 extern void native_usergs_sysret64(void);
@@ -397,6 +399,7 @@ struct pv_cpu_ops pv_cpu_ops = {
 	.usergs_sysret64 = native_usergs_sysret64,
 #endif
 	.iret = native_iret,
+	.nmi_return = native_nmi_return,
 	.swapgs = native_swapgs,
 
 	.set_iopl_mask = native_set_iopl_mask,
diff --git a/stblinux-2.6.31/arch/x86/kernel/paravirt_patch_32.c b/stblinux-2.6.31/arch/x86/kernel/paravirt_patch_32.c
index d9f32e6..ac37277 100644
--- a/stblinux-2.6.31/arch/x86/kernel/paravirt_patch_32.c
+++ b/stblinux-2.6.31/arch/x86/kernel/paravirt_patch_32.c
@@ -1,10 +1,13 @@
-#include <asm/paravirt.h>
+#include <linux/stringify.h>
+#include <linux/irqflags.h>
 
 DEF_NATIVE(pv_irq_ops, irq_disable, "cli");
 DEF_NATIVE(pv_irq_ops, irq_enable, "sti");
 DEF_NATIVE(pv_irq_ops, restore_fl, "push %eax; popf");
 DEF_NATIVE(pv_irq_ops, save_fl, "pushf; pop %eax");
 DEF_NATIVE(pv_cpu_ops, iret, "iret");
+DEF_NATIVE(pv_cpu_ops, nmi_return,
+	__stringify(NATIVE_INTERRUPT_RETURN_NMI_SAFE));
 DEF_NATIVE(pv_cpu_ops, irq_enable_sysexit, "sti; sysexit");
 DEF_NATIVE(pv_mmu_ops, read_cr2, "mov %cr2, %eax");
 DEF_NATIVE(pv_mmu_ops, write_cr3, "mov %eax, %cr3");
@@ -41,6 +44,7 @@ unsigned native_patch(u8 type, u16 clobbers, void *ibuf,
 		PATCH_SITE(pv_irq_ops, restore_fl);
 		PATCH_SITE(pv_irq_ops, save_fl);
 		PATCH_SITE(pv_cpu_ops, iret);
+		PATCH_SITE(pv_cpu_ops, nmi_return);
 		PATCH_SITE(pv_cpu_ops, irq_enable_sysexit);
 		PATCH_SITE(pv_mmu_ops, read_cr2);
 		PATCH_SITE(pv_mmu_ops, read_cr3);
diff --git a/stblinux-2.6.31/arch/x86/kernel/paravirt_patch_64.c b/stblinux-2.6.31/arch/x86/kernel/paravirt_patch_64.c
index 3f08f34..5339e67 100644
--- a/stblinux-2.6.31/arch/x86/kernel/paravirt_patch_64.c
+++ b/stblinux-2.6.31/arch/x86/kernel/paravirt_patch_64.c
@@ -1,12 +1,15 @@
+#include <linux/irqflags.h>
+#include <linux/stringify.h>
 #include <asm/paravirt.h>
 #include <asm/asm-offsets.h>
-#include <linux/stringify.h>
 
 DEF_NATIVE(pv_irq_ops, irq_disable, "cli");
 DEF_NATIVE(pv_irq_ops, irq_enable, "sti");
 DEF_NATIVE(pv_irq_ops, restore_fl, "pushq %rdi; popfq");
 DEF_NATIVE(pv_irq_ops, save_fl, "pushfq; popq %rax");
 DEF_NATIVE(pv_cpu_ops, iret, "iretq");
+DEF_NATIVE(pv_cpu_ops, nmi_return,
+	__stringify(NATIVE_INTERRUPT_RETURN_NMI_SAFE));
 DEF_NATIVE(pv_mmu_ops, read_cr2, "movq %cr2, %rax");
 DEF_NATIVE(pv_mmu_ops, read_cr3, "movq %cr3, %rax");
 DEF_NATIVE(pv_mmu_ops, write_cr3, "movq %rdi, %cr3");
@@ -51,6 +54,7 @@ unsigned native_patch(u8 type, u16 clobbers, void *ibuf,
 		PATCH_SITE(pv_irq_ops, irq_enable);
 		PATCH_SITE(pv_irq_ops, irq_disable);
 		PATCH_SITE(pv_cpu_ops, iret);
+		PATCH_SITE(pv_cpu_ops, nmi_return);
 		PATCH_SITE(pv_cpu_ops, irq_enable_sysexit);
 		PATCH_SITE(pv_cpu_ops, usergs_sysret32);
 		PATCH_SITE(pv_cpu_ops, usergs_sysret64);
diff --git a/stblinux-2.6.31/arch/x86/kernel/process_32.c b/stblinux-2.6.31/arch/x86/kernel/process_32.c
index 59f4524..175e1e6 100644
--- a/stblinux-2.6.31/arch/x86/kernel/process_32.c
+++ b/stblinux-2.6.31/arch/x86/kernel/process_32.c
@@ -40,6 +40,10 @@
 #include <linux/uaccess.h>
 #include <linux/io.h>
 #include <linux/kdebug.h>
+#include <linux/notifier.h>
+#include <linux/idle.h>
+#include <trace/sched.h>
+#include <trace/pm.h>
 
 #include <asm/pgtable.h>
 #include <asm/system.h>
@@ -64,6 +68,36 @@ asmlinkage void ret_from_fork(void) __asm__("ret_from_fork");
 DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
 
+DEFINE_TRACE(sched_kthread_create);
+DEFINE_TRACE(pm_idle_exit);
+DEFINE_TRACE(pm_idle_entry);
+
+static DEFINE_PER_CPU(unsigned char, is_idle);
+
+void enter_idle(void)
+{
+	percpu_write(is_idle, 1);
+	trace_pm_idle_entry();
+	notify_idle(IDLE_START);
+}
+
+static void __exit_idle(void)
+{
+	if (x86_test_and_clear_bit_percpu(0, is_idle) == 0)
+		return;
+	notify_idle(IDLE_END);
+	trace_pm_idle_exit();
+}
+
+/* Called from interrupts to signify idle end */
+void exit_idle(void)
+{
+	/* idle loop has pid 0 */
+	if (current->pid)
+		return;
+	__exit_idle();
+}
+
 /*
  * Return saved PC of a blocked thread.
  */
@@ -112,10 +146,15 @@ void cpu_idle(void)
 				play_dead();
 
 			local_irq_disable();
+			enter_idle();
 			/* Don't trace irqs off for idle */
 			stop_critical_timings();
 			pm_idle();
 			start_critical_timings();
+			/* In many cases the interrupt that ended idle
+			   has already called exit_idle. But some idle
+			   loops can be woken up without interrupt. */
+			__exit_idle();
 		}
 		tick_nohz_restart_sched_tick();
 		preempt_enable_no_resched();
@@ -207,6 +246,7 @@ extern void kernel_thread_helper(void);
 int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 {
 	struct pt_regs regs;
+	long pid;
 
 	memset(&regs, 0, sizeof(regs));
 
@@ -223,7 +263,10 @@ int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 	regs.flags = X86_EFLAGS_IF | X86_EFLAGS_SF | X86_EFLAGS_PF | 0x2;
 
 	/* Ok, create the new process.. */
-	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+	pid = do_fork(flags | CLONE_VM | CLONE_UNTRACED,
+			0, &regs, 0, NULL, NULL);
+	trace_sched_kthread_create(fn, pid);
+	return pid;
 }
 EXPORT_SYMBOL(kernel_thread);
 
diff --git a/stblinux-2.6.31/arch/x86/kernel/process_64.c b/stblinux-2.6.31/arch/x86/kernel/process_64.c
index 80c2372..73e6e50 100644
--- a/stblinux-2.6.31/arch/x86/kernel/process_64.c
+++ b/stblinux-2.6.31/arch/x86/kernel/process_64.c
@@ -36,9 +36,12 @@
 #include <linux/tick.h>
 #include <linux/prctl.h>
 #include <linux/uaccess.h>
+#include <linux/idle.h>
 #include <linux/io.h>
 #include <linux/ftrace.h>
 #include <linux/dmi.h>
+#include <trace/sched.h>
+#include <trace/pm.h>
 
 #include <asm/pgtable.h>
 #include <asm/system.h>
@@ -53,8 +56,15 @@
 #include <asm/syscalls.h>
 #include <asm/ds.h>
 
+DEFINE_TRACE(sched_kthread_create);
+DEFINE_TRACE(pm_idle_exit);
+DEFINE_TRACE(pm_idle_entry);
+
 asmlinkage extern void ret_from_fork(void);
 
+asmlinkage long kernel_thread_asm(int (*fn)(void *), void * arg,
+	unsigned long flags);
+
 DEFINE_PER_CPU(struct task_struct *, current_task) = &init_task;
 EXPORT_PER_CPU_SYMBOL(current_task);
 
@@ -63,31 +73,23 @@ static DEFINE_PER_CPU(unsigned char, is_idle);
 
 unsigned long kernel_thread_flags = CLONE_VM | CLONE_UNTRACED;
 
-static ATOMIC_NOTIFIER_HEAD(idle_notifier);
-
-void idle_notifier_register(struct notifier_block *n)
-{
-	atomic_notifier_chain_register(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_register);
-
-void idle_notifier_unregister(struct notifier_block *n)
-{
-	atomic_notifier_chain_unregister(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_unregister);
-
 void enter_idle(void)
 {
 	percpu_write(is_idle, 1);
-	atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
+	/*
+	 * Trace last event before calling notifiers. Notifiers flush
+	 * data from buffers before going to idle.
+	 */
+	trace_pm_idle_entry();
+	notify_idle(IDLE_START);
 }
 
 static void __exit_idle(void)
 {
 	if (x86_test_and_clear_bit_percpu(0, is_idle) == 0)
 		return;
-	atomic_notifier_call_chain(&idle_notifier, IDLE_END, NULL);
+	notify_idle(IDLE_END);
+	trace_pm_idle_exit();
 }
 
 /* Called from interrupts to signify idle end */
@@ -669,3 +671,10 @@ long sys_arch_prctl(int code, unsigned long addr)
 	return do_arch_prctl(current, code, addr);
 }
 
+asmlinkage int kernel_thread(int (*fn)(void *), void * arg,
+	 unsigned long flags)
+{
+	int pid = kernel_thread_asm(fn, arg, flags);
+	trace_sched_kthread_create(fn, pid);
+	return pid;
+}
diff --git a/stblinux-2.6.31/arch/x86/kernel/ptrace.c b/stblinux-2.6.31/arch/x86/kernel/ptrace.c
index 96e2a86..14a6bcf 100644
--- a/stblinux-2.6.31/arch/x86/kernel/ptrace.c
+++ b/stblinux-2.6.31/arch/x86/kernel/ptrace.c
@@ -22,6 +22,7 @@
 #include <linux/seccomp.h>
 #include <linux/signal.h>
 #include <linux/workqueue.h>
+#include <trace/syscall.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -63,6 +64,9 @@ enum x86_regset {
 				  X86_EFLAGS_DF | X86_EFLAGS_OF |	\
 				  X86_EFLAGS_RF | X86_EFLAGS_AC))
 
+DEFINE_TRACE(syscall_entry);
+DEFINE_TRACE(syscall_exit);
+
 /*
  * Determines whether a value may be installed in a segment register.
  */
@@ -1487,6 +1491,8 @@ asmregparm long syscall_trace_enter(struct pt_regs *regs)
 	if (test_thread_flag(TIF_SINGLESTEP))
 		regs->flags |= X86_EFLAGS_TF;
 
+	trace_syscall_entry(regs, regs->orig_ax);
+
 	/* do the secure computing check first */
 	secure_computing(regs->orig_ax);
 
@@ -1520,6 +1526,8 @@ asmregparm long syscall_trace_enter(struct pt_regs *regs)
 
 asmregparm void syscall_trace_leave(struct pt_regs *regs)
 {
+	trace_syscall_exit(regs->ax);
+
 	if (unlikely(current->audit_context))
 		audit_syscall_exit(AUDITSC_RESULT(regs->ax), regs->ax);
 
diff --git a/stblinux-2.6.31/arch/x86/kernel/sys_i386_32.c b/stblinux-2.6.31/arch/x86/kernel/sys_i386_32.c
index 1884a8d..5e16459 100644
--- a/stblinux-2.6.31/arch/x86/kernel/sys_i386_32.c
+++ b/stblinux-2.6.31/arch/x86/kernel/sys_i386_32.c
@@ -18,12 +18,15 @@
 #include <linux/file.h>
 #include <linux/utsname.h>
 #include <linux/ipc.h>
+#include <trace/ipc.h>
 
 #include <linux/uaccess.h>
 #include <linux/unistd.h>
 
 #include <asm/syscalls.h>
 
+DEFINE_TRACE(ipc_call);
+
 asmlinkage long sys_mmap2(unsigned long addr, unsigned long len,
 			  unsigned long prot, unsigned long flags,
 			  unsigned long fd, unsigned long pgoff)
@@ -113,6 +116,8 @@ asmlinkage int sys_ipc(uint call, int first, int second,
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_ipc_call(call, first);
+
 	switch (call) {
 	case SEMOP:
 		return sys_semtimedop(first, (struct sembuf __user *)ptr, second, NULL);
diff --git a/stblinux-2.6.31/arch/x86/kernel/syscall_64.c b/stblinux-2.6.31/arch/x86/kernel/syscall_64.c
index de87d60..5e74f6a 100644
--- a/stblinux-2.6.31/arch/x86/kernel/syscall_64.c
+++ b/stblinux-2.6.31/arch/x86/kernel/syscall_64.c
@@ -1,8 +1,11 @@
 /* System call table for x86-64. */
 
 #include <linux/linkage.h>
+#include <linux/module.h>
 #include <linux/sys.h>
 #include <linux/cache.h>
+#include <linux/marker.h>
+#include <linux/kallsyms.h>
 #include <asm/asm-offsets.h>
 
 #define __NO_STUBS
@@ -27,3 +30,18 @@ const sys_call_ptr_t sys_call_table[__NR_syscall_max+1] = {
 	[0 ... __NR_syscall_max] = &sys_ni_syscall,
 #include <asm/unistd_64.h>
 };
+
+void ltt_dump_sys_call_table(void *call_data)
+{
+	int i;
+	char namebuf[KSYM_NAME_LEN];
+
+	for (i = 0; i < __NR_syscall_max + 1; i++) {
+		sprint_symbol(namebuf, (unsigned long)sys_call_table[i]);
+		__trace_mark(0, syscall_state, sys_call_table,
+			call_data,
+			"id %d address %p symbol %s",
+			i, (void*)sys_call_table[i], namebuf);
+	}
+}
+EXPORT_SYMBOL_GPL(ltt_dump_sys_call_table);
diff --git a/stblinux-2.6.31/arch/x86/kernel/trace-clock.c b/stblinux-2.6.31/arch/x86/kernel/trace-clock.c
new file mode 100644
index 0000000..b95e4ed
--- /dev/null
+++ b/stblinux-2.6.31/arch/x86/kernel/trace-clock.c
@@ -0,0 +1,247 @@
+/*
+ * arch/x86/kernel/trace-clock.c
+ *
+ * Trace clock for x86.
+ *
+ * Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>, October 2008
+ */
+
+#include <linux/module.h>
+#include <linux/trace-clock.h>
+#include <linux/jiffies.h>
+#include <linux/timer.h>
+#include <linux/cpu.h>
+
+static cycles_t trace_clock_last_tsc;
+static DEFINE_PER_CPU(struct timer_list, update_timer);
+static DEFINE_SPINLOCK(async_tsc_lock);
+static int async_tsc_refcount;	/* Number of readers */
+static int async_tsc_enabled;	/* Async TSC enabled on all online CPUs */
+
+int _trace_clock_is_sync = 1;
+EXPORT_SYMBOL_GPL(_trace_clock_is_sync);
+
+/*
+ * Called by check_tsc_sync_source from CPU hotplug.
+ */
+void set_trace_clock_is_sync(int state)
+{
+	_trace_clock_is_sync = state;
+}
+
+#if BITS_PER_LONG == 64
+static cycles_t read_last_tsc(void)
+{
+	return trace_clock_last_tsc;
+}
+#else
+/*
+ * A cmpxchg64 update can happen concurrently. Based on the assumption that
+ * two cmpxchg64 will never update it to the same value (the count always
+ * increases), reading it twice insures that we read a coherent value with the
+ * same "sequence number".
+ */
+static cycles_t read_last_tsc(void)
+{
+	cycles_t val1, val2;
+
+	val1 = trace_clock_last_tsc;
+	for (;;) {
+		val2 = val1;
+		barrier();
+		val1 = trace_clock_last_tsc;
+		if (likely(val1 == val2))
+			break;
+	}
+	return val1;
+}
+#endif
+
+/*
+ * Support for architectures with non-sync TSCs.
+ * When the local TSC is discovered to lag behind the highest TSC counter, we
+ * increment the TSC count of an amount that should be, ideally, lower than the
+ * execution time of this routine, in cycles : this is the granularity we look
+ * for : we must be able to order the events.
+ */
+notrace cycles_t trace_clock_async_tsc_read(void)
+{
+	cycles_t new_tsc, last_tsc;
+
+	WARN_ON(!async_tsc_refcount || !async_tsc_enabled);
+	rdtsc_barrier();
+	new_tsc = get_cycles();
+	rdtsc_barrier();
+	last_tsc = read_last_tsc();
+	do {
+		if (new_tsc < last_tsc)
+			new_tsc = last_tsc + TRACE_CLOCK_MIN_PROBE_DURATION;
+		/*
+		 * If cmpxchg fails with a value higher than the new_tsc, don't
+		 * retry : the value has been incremented and the events
+		 * happened almost at the same time.
+		 * We must retry if cmpxchg fails with a lower value :
+		 * it means that we are the CPU with highest frequency and
+		 * therefore MUST update the value.
+		 */
+		last_tsc = cmpxchg64(&trace_clock_last_tsc, last_tsc, new_tsc);
+	} while (unlikely(last_tsc < new_tsc));
+	return new_tsc;
+}
+EXPORT_SYMBOL_GPL(trace_clock_async_tsc_read);
+
+static void update_timer_ipi(void *info)
+{
+	(void)trace_clock_async_tsc_read();
+}
+
+/*
+ * update_timer_fct : - Timer function to resync the clocks
+ * @data: unused
+ *
+ * Fires every jiffy.
+ */
+static void update_timer_fct(unsigned long data)
+{
+	(void)trace_clock_async_tsc_read();
+
+	per_cpu(update_timer, smp_processor_id()).expires = jiffies + 1;
+	add_timer_on(&per_cpu(update_timer, smp_processor_id()),
+		     smp_processor_id());
+}
+
+static void enable_trace_clock(int cpu)
+{
+	init_timer(&per_cpu(update_timer, cpu));
+	per_cpu(update_timer, cpu).function = update_timer_fct;
+	per_cpu(update_timer, cpu).expires = jiffies + 1;
+	smp_call_function_single(cpu, update_timer_ipi, NULL, 1);
+	add_timer_on(&per_cpu(update_timer, cpu), cpu);
+}
+
+static void disable_trace_clock(int cpu)
+{
+	del_timer_sync(&per_cpu(update_timer, cpu));
+}
+
+/*
+ * 	hotcpu_callback - CPU hotplug callback
+ * 	@nb: notifier block
+ * 	@action: hotplug action to take
+ * 	@hcpu: CPU number
+ *
+ * 	Returns the success/failure of the operation. (NOTIFY_OK, NOTIFY_BAD)
+ */
+static int __cpuinit hotcpu_callback(struct notifier_block *nb,
+				unsigned long action,
+				void *hcpu)
+{
+	unsigned int hotcpu = (unsigned long)hcpu;
+	int cpu;
+
+	spin_lock(&async_tsc_lock);
+	switch (action) {
+	case CPU_UP_PREPARE:
+	case CPU_UP_PREPARE_FROZEN:
+		break;
+	case CPU_ONLINE:
+	case CPU_ONLINE_FROZEN:
+		/*
+		 * trace_clock_is_sync() is updated by set_trace_clock_is_sync()
+		 * code, protected by cpu hotplug disable.
+		 * It is ok to let the hotplugged CPU read the timebase before
+		 * the CPU_ONLINE notification. It's just there to give a
+		 * maximum bound to the TSC error.
+		 */
+		if (async_tsc_refcount && !trace_clock_is_sync()) {
+			if (!async_tsc_enabled) {
+				async_tsc_enabled = 1;
+				for_each_online_cpu(cpu)
+					enable_trace_clock(cpu);
+			} else {
+				enable_trace_clock(hotcpu);
+			}
+		}
+		break;
+#ifdef CONFIG_HOTPLUG_CPU
+	case CPU_UP_CANCELED:
+	case CPU_UP_CANCELED_FROZEN:
+		if (!async_tsc_refcount && num_online_cpus() == 1)
+			set_trace_clock_is_sync(1);
+		break;
+	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
+		/*
+		 * We cannot stop the trace clock on other CPUs when readers are
+		 * active even if we go back to a synchronized state (1 CPU)
+		 * because the CPU left could be the one lagging behind.
+		 */
+		if (async_tsc_refcount && async_tsc_enabled)
+			disable_trace_clock(hotcpu);
+		if (!async_tsc_refcount && num_online_cpus() == 1)
+			set_trace_clock_is_sync(1);
+		break;
+#endif /* CONFIG_HOTPLUG_CPU */
+	}
+	spin_unlock(&async_tsc_lock);
+
+	return NOTIFY_OK;
+}
+
+void get_trace_clock(void)
+{
+	int cpu;
+
+	if (!trace_clock_is_sync()) {
+		printk(KERN_WARNING
+			"Trace clock falls back on cache-line bouncing\n"
+			"workaround due to non-synchronized TSCs.\n"
+			"This workaround preserves event order across CPUs.\n"
+			"Please consider disabling Speedstep or PowerNow and\n"
+			"using kernel parameters "
+			"\"force_tsc_sync=1 idle=poll\"\n"
+			"for accurate and fast tracing clock source.\n");
+	}
+
+	get_online_cpus();
+	spin_lock(&async_tsc_lock);
+	if (async_tsc_refcount++ || trace_clock_is_sync())
+		goto end;
+
+	async_tsc_enabled = 1;
+	for_each_online_cpu(cpu)
+		enable_trace_clock(cpu);
+end:
+	spin_unlock(&async_tsc_lock);
+	put_online_cpus();
+}
+EXPORT_SYMBOL_GPL(get_trace_clock);
+
+void put_trace_clock(void)
+{
+	int cpu;
+
+	get_online_cpus();
+	spin_lock(&async_tsc_lock);
+	WARN_ON(async_tsc_refcount <= 0);
+	if (async_tsc_refcount != 1 || !async_tsc_enabled)
+		goto end;
+
+	for_each_online_cpu(cpu)
+		disable_trace_clock(cpu);
+	async_tsc_enabled = 0;
+end:
+	async_tsc_refcount--;
+	if (!async_tsc_refcount && num_online_cpus() == 1)
+		set_trace_clock_is_sync(1);
+	spin_unlock(&async_tsc_lock);
+	put_online_cpus();
+}
+EXPORT_SYMBOL_GPL(put_trace_clock);
+
+static __init int init_unsync_trace_clock(void)
+{
+	hotcpu_notifier(hotcpu_callback, 4);
+	return 0;
+}
+early_initcall(init_unsync_trace_clock);
diff --git a/stblinux-2.6.31/arch/x86/kernel/traps.c b/stblinux-2.6.31/arch/x86/kernel/traps.c
index 5204332..c4e746c 100644
--- a/stblinux-2.6.31/arch/x86/kernel/traps.c
+++ b/stblinux-2.6.31/arch/x86/kernel/traps.c
@@ -31,6 +31,7 @@
 #include <linux/mm.h>
 #include <linux/smp.h>
 #include <linux/io.h>
+#include <trace/trap.h>
 
 #ifdef CONFIG_EISA
 #include <linux/ioport.h>
@@ -65,6 +66,7 @@
 #include <asm/processor-flags.h>
 #include <asm/setup.h>
 #include <asm/traps.h>
+#include <asm/unistd.h>
 
 asmlinkage int system_call(void);
 
@@ -78,11 +80,21 @@ char ignore_fpu_irq;
  */
 gate_desc idt_table[256]
 	__attribute__((__section__(".data.idt"))) = { { { { 0, 0 } } }, };
+
+extern unsigned long sys_call_table[];
+extern unsigned long syscall_table_size;
+
 #endif
 
 DECLARE_BITMAP(used_vectors, NR_VECTORS);
 EXPORT_SYMBOL_GPL(used_vectors);
 
+/*
+ * Also used in arch/x86/mm/fault.c.
+ */
+DEFINE_TRACE(trap_entry);
+DEFINE_TRACE(trap_exit);
+
 static int ignore_nmis;
 
 static inline void conditional_sti(struct pt_regs *regs)
@@ -126,6 +138,8 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 {
 	struct task_struct *tsk = current;
 
+	trace_trap_entry(regs, trapnr);
+
 #ifdef CONFIG_X86_32
 	if (regs->flags & X86_VM_MASK) {
 		/*
@@ -172,7 +186,7 @@ trap_signal:
 		force_sig_info(signr, info, tsk);
 	else
 		force_sig(signr, tsk);
-	return;
+	goto end;
 
 kernel_trap:
 	if (!fixup_exception(regs)) {
@@ -180,15 +194,17 @@ kernel_trap:
 		tsk->thread.trap_no = trapnr;
 		die(str, regs, error_code);
 	}
-	return;
+	goto end;
 
 #ifdef CONFIG_X86_32
 vm86_trap:
 	if (handle_vm86_trap((struct kernel_vm86_regs *) regs,
 						error_code, trapnr))
 		goto trap_signal;
-	return;
+	goto end;
 #endif
+end:
+	trace_trap_exit();
 }
 
 #define DO_ERROR(trapnr, signr, str, name)				\
@@ -289,7 +305,9 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		printk("\n");
 	}
 
+	trace_trap_entry(regs, 13);
 	force_sig(SIGSEGV, tsk);
+	trace_trap_exit();
 	return;
 
 #ifdef CONFIG_X86_32
@@ -399,27 +417,29 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	if (!cpu)
 		reason = get_nmi_reason();
 
+	trace_trap_entry(regs, 2);
+
 	if (!(reason & 0xc0)) {
 		if (notify_die(DIE_NMI_IPI, "nmi_ipi", regs, reason, 2, SIGINT)
 								== NOTIFY_STOP)
-			return;
+			goto end;
 #ifdef CONFIG_X86_LOCAL_APIC
 		/*
 		 * Ok, so this is none of the documented NMI sources,
 		 * so it must be the NMI watchdog.
 		 */
 		if (nmi_watchdog_tick(regs, reason))
-			return;
+			goto end;
 		if (!do_nmi_callback(regs, cpu))
 			unknown_nmi_error(reason, regs);
 #else
 		unknown_nmi_error(reason, regs);
 #endif
 
-		return;
+		goto end;
 	}
 	if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT) == NOTIFY_STOP)
-		return;
+		goto end;
 
 	/* AK: following checks seem to be broken on modern chipsets. FIXME */
 	if (reason & 0x80)
@@ -433,6 +453,8 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	 */
 	reassert_nmi();
 #endif
+end:
+	trace_trap_exit();
 }
 
 dotraplinkage notrace __kprobes void
@@ -463,7 +485,7 @@ void restart_nmi(void)
 /* May run on IST stack. */
 dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
 {
-#ifdef CONFIG_KPROBES
+#if (defined(CONFIG_KPROBES) || defined(USE_IMMEDIATE))
 	if (notify_die(DIE_INT3, "int3", regs, error_code, 3, SIGTRAP)
 			== NOTIFY_STOP)
 		return;
@@ -579,7 +601,9 @@ dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 
 	si_code = get_si_code(condition);
 	/* Ok, finally something we can handle */
+	trace_trap_entry(regs, 1);
 	send_sigtrap(tsk, regs, error_code, si_code);
+	trace_trap_exit();
 
 	/*
 	 * Disable additional traps. They'll be re-enabled when
@@ -594,7 +618,9 @@ clear_dr7:
 debug_vm86:
 	/* reenable preemption: handle_vm86_trap() might sleep */
 	dec_preempt_count();
+	trace_trap_entry(regs, 1);
 	handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code, 1);
+	trace_trap_exit();
 	conditional_cli(regs);
 	return;
 #endif
@@ -620,6 +646,22 @@ static int kernel_math_error(struct pt_regs *regs, const char *str, int trapnr)
 }
 #endif
 
+#ifdef CONFIG_X86_32
+void ltt_dump_sys_call_table(void *call_data)
+{
+	int i;
+	char namebuf[KSYM_NAME_LEN];
+
+	for (i = 0; i < NR_syscalls; i++) {
+		sprint_symbol(namebuf, sys_call_table[i]);
+		__trace_mark(0, syscall_state, sys_call_table, call_data,
+			"id %d address %p symbol %s",
+			i, (void*)sys_call_table[i], namebuf);
+	}
+}
+EXPORT_SYMBOL_GPL(ltt_dump_sys_call_table);
+#endif
+
 /*
  * Note that we play around with the 'TS' bit in an attempt to get
  * the correct behaviour even in the presence of the asynchronous
@@ -779,11 +821,13 @@ do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 {
+	trace_trap_entry(regs, 16);
 	conditional_sti(regs);
 #if 0
 	/* No need to warn about this any longer. */
 	printk(KERN_INFO "Ignoring P6 Local APIC Spurious Interrupt Bug...\n");
 #endif
+	trace_trap_exit();
 }
 
 #ifdef CONFIG_X86_32
@@ -815,6 +859,21 @@ asmlinkage void __attribute__((weak)) smp_threshold_interrupt(void)
 {
 }
 
+void ltt_dump_idt_table(void *call_data)
+{
+	int i;
+	char namebuf[KSYM_NAME_LEN];
+
+	for (i = 0; i < IDT_ENTRIES; i++) {
+		unsigned long address = gate_offset(idt_table[i]);
+		sprint_symbol(namebuf, address);
+		__trace_mark(0, irq_state, idt_table, call_data,
+			"irq %d address %p symbol %s",
+			i, (void *)address, namebuf);
+	}
+}
+EXPORT_SYMBOL_GPL(ltt_dump_idt_table);
+
 /*
  * 'math_state_restore()' saves the current math information in the
  * old math state array, and gets the new ones from the current task
diff --git a/stblinux-2.6.31/arch/x86/kernel/tsc_sync.c b/stblinux-2.6.31/arch/x86/kernel/tsc_sync.c
deleted file mode 100644
index 027b5b4..0000000
--- a/stblinux-2.6.31/arch/x86/kernel/tsc_sync.c
+++ /dev/null
@@ -1,197 +0,0 @@
-/*
- * check TSC synchronization.
- *
- * Copyright (C) 2006, Red Hat, Inc., Ingo Molnar
- *
- * We check whether all boot CPUs have their TSC's synchronized,
- * print a warning if not and turn off the TSC clock-source.
- *
- * The warp-check is point-to-point between two CPUs, the CPU
- * initiating the bootup is the 'source CPU', the freshly booting
- * CPU is the 'target CPU'.
- *
- * Only two CPUs may participate - they can enter in any order.
- * ( The serial nature of the boot logic and the CPU hotplug lock
- *   protects against more than 2 CPUs entering this code. )
- */
-#include <linux/spinlock.h>
-#include <linux/kernel.h>
-#include <linux/init.h>
-#include <linux/smp.h>
-#include <linux/nmi.h>
-#include <asm/tsc.h>
-
-/*
- * Entry/exit counters that make sure that both CPUs
- * run the measurement code at once:
- */
-static __cpuinitdata atomic_t start_count;
-static __cpuinitdata atomic_t stop_count;
-
-/*
- * We use a raw spinlock in this exceptional case, because
- * we want to have the fastest, inlined, non-debug version
- * of a critical section, to be able to prove TSC time-warps:
- */
-static __cpuinitdata raw_spinlock_t sync_lock = __RAW_SPIN_LOCK_UNLOCKED;
-
-static __cpuinitdata cycles_t last_tsc;
-static __cpuinitdata cycles_t max_warp;
-static __cpuinitdata int nr_warps;
-
-/*
- * TSC-warp measurement loop running on both CPUs:
- */
-static __cpuinit void check_tsc_warp(void)
-{
-	cycles_t start, now, prev, end;
-	int i;
-
-	rdtsc_barrier();
-	start = get_cycles();
-	rdtsc_barrier();
-	/*
-	 * The measurement runs for 20 msecs:
-	 */
-	end = start + tsc_khz * 20ULL;
-	now = start;
-
-	for (i = 0; ; i++) {
-		/*
-		 * We take the global lock, measure TSC, save the
-		 * previous TSC that was measured (possibly on
-		 * another CPU) and update the previous TSC timestamp.
-		 */
-		__raw_spin_lock(&sync_lock);
-		prev = last_tsc;
-		rdtsc_barrier();
-		now = get_cycles();
-		rdtsc_barrier();
-		last_tsc = now;
-		__raw_spin_unlock(&sync_lock);
-
-		/*
-		 * Be nice every now and then (and also check whether
-		 * measurement is done [we also insert a 10 million
-		 * loops safety exit, so we dont lock up in case the
-		 * TSC readout is totally broken]):
-		 */
-		if (unlikely(!(i & 7))) {
-			if (now > end || i > 10000000)
-				break;
-			cpu_relax();
-			touch_nmi_watchdog();
-		}
-		/*
-		 * Outside the critical section we can now see whether
-		 * we saw a time-warp of the TSC going backwards:
-		 */
-		if (unlikely(prev > now)) {
-			__raw_spin_lock(&sync_lock);
-			max_warp = max(max_warp, prev - now);
-			nr_warps++;
-			__raw_spin_unlock(&sync_lock);
-		}
-	}
-	WARN(!(now-start),
-		"Warning: zero tsc calibration delta: %Ld [max: %Ld]\n",
-			now-start, end-start);
-}
-
-/*
- * Source CPU calls into this - it waits for the freshly booted
- * target CPU to arrive and then starts the measurement:
- */
-void __cpuinit check_tsc_sync_source(int cpu)
-{
-	int cpus = 2;
-
-	/*
-	 * No need to check if we already know that the TSC is not
-	 * synchronized:
-	 */
-	if (unsynchronized_tsc())
-		return;
-
-	if (boot_cpu_has(X86_FEATURE_TSC_RELIABLE)) {
-		pr_info("Skipping synchronization checks as TSC is reliable.\n");
-		return;
-	}
-
-	pr_info("checking TSC synchronization [CPU#%d -> CPU#%d]:",
-		smp_processor_id(), cpu);
-
-	/*
-	 * Reset it - in case this is a second bootup:
-	 */
-	atomic_set(&stop_count, 0);
-
-	/*
-	 * Wait for the target to arrive:
-	 */
-	while (atomic_read(&start_count) != cpus-1)
-		cpu_relax();
-	/*
-	 * Trigger the target to continue into the measurement too:
-	 */
-	atomic_inc(&start_count);
-
-	check_tsc_warp();
-
-	while (atomic_read(&stop_count) != cpus-1)
-		cpu_relax();
-
-	if (nr_warps) {
-		printk("\n");
-		pr_warning("Measured %Ld cycles TSC warp between CPUs, "
-			   "turning off TSC clock.\n", max_warp);
-		mark_tsc_unstable("check_tsc_sync_source failed");
-	} else {
-		printk(" passed.\n");
-	}
-
-	/*
-	 * Reset it - just in case we boot another CPU later:
-	 */
-	atomic_set(&start_count, 0);
-	nr_warps = 0;
-	max_warp = 0;
-	last_tsc = 0;
-
-	/*
-	 * Let the target continue with the bootup:
-	 */
-	atomic_inc(&stop_count);
-}
-
-/*
- * Freshly booted CPUs call into this:
- */
-void __cpuinit check_tsc_sync_target(void)
-{
-	int cpus = 2;
-
-	if (unsynchronized_tsc() || boot_cpu_has(X86_FEATURE_TSC_RELIABLE))
-		return;
-
-	/*
-	 * Register this CPU's participation and wait for the
-	 * source CPU to start the measurement:
-	 */
-	atomic_inc(&start_count);
-	while (atomic_read(&start_count) != cpus)
-		cpu_relax();
-
-	check_tsc_warp();
-
-	/*
-	 * Ok, we are done:
-	 */
-	atomic_inc(&stop_count);
-
-	/*
-	 * Wait for the source CPU to print stuff:
-	 */
-	while (atomic_read(&stop_count) != cpus)
-		cpu_relax();
-}
diff --git a/stblinux-2.6.31/arch/x86/kernel/vmi_32.c b/stblinux-2.6.31/arch/x86/kernel/vmi_32.c
index 95a7289..5f9a59f 100644
--- a/stblinux-2.6.31/arch/x86/kernel/vmi_32.c
+++ b/stblinux-2.6.31/arch/x86/kernel/vmi_32.c
@@ -152,6 +152,8 @@ static unsigned vmi_patch(u8 type, u16 clobbers, void *insns,
 					      insns, ip);
 		case PARAVIRT_PATCH(pv_cpu_ops.iret):
 			return patch_internal(VMI_CALL_IRET, len, insns, ip);
+		case PARAVIRT_PATCH(pv_cpu_ops.nmi_return):
+			return patch_internal(VMI_CALL_IRET, len, insns, ip);
 		case PARAVIRT_PATCH(pv_cpu_ops.irq_enable_sysexit):
 			return patch_internal(VMI_CALL_SYSEXIT, len, insns, ip);
 		default:
diff --git a/stblinux-2.6.31/arch/x86/kvm/x86.c b/stblinux-2.6.31/arch/x86/kvm/x86.c
index 26e454c..8da2d76 100644
--- a/stblinux-2.6.31/arch/x86/kvm/x86.c
+++ b/stblinux-2.6.31/arch/x86/kvm/x86.c
@@ -3376,7 +3376,7 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
 	/*
 	 * Profile KVM exit RIPs:
 	 */
-	if (unlikely(prof_on == KVM_PROFILING)) {
+	if (unlikely(imv_read(prof_on) == KVM_PROFILING)) {
 		unsigned long rip = kvm_rip_read(vcpu);
 		profile_hit(KVM_PROFILING, (void *)rip);
 	}
diff --git a/stblinux-2.6.31/arch/x86/lguest/boot.c b/stblinux-2.6.31/arch/x86/lguest/boot.c
index d677fa9..e7de169 100644
--- a/stblinux-2.6.31/arch/x86/lguest/boot.c
+++ b/stblinux-2.6.31/arch/x86/lguest/boot.c
@@ -1278,6 +1278,7 @@ __init void lguest_init(void)
 	pv_cpu_ops.cpuid = lguest_cpuid;
 	pv_cpu_ops.load_idt = lguest_load_idt;
 	pv_cpu_ops.iret = lguest_iret;
+	pv_cpu_ops.nmi_return = lguest_iret;
 	pv_cpu_ops.load_sp0 = lguest_load_sp0;
 	pv_cpu_ops.load_tr_desc = lguest_load_tr_desc;
 	pv_cpu_ops.set_ldt = lguest_set_ldt;
diff --git a/stblinux-2.6.31/arch/x86/mm/fault.c b/stblinux-2.6.31/arch/x86/mm/fault.c
index bfae139..76acb8f 100644
--- a/stblinux-2.6.31/arch/x86/mm/fault.c
+++ b/stblinux-2.6.31/arch/x86/mm/fault.c
@@ -11,6 +11,7 @@
 #include <linux/kprobes.h>		/* __kprobes, ...		*/
 #include <linux/mmiotrace.h>		/* kmmio_handler, ...		*/
 #include <linux/perf_counter.h>		/* perf_swcounter_event		*/
+#include <trace/fault.h>
 
 #include <asm/traps.h>			/* dotraplinkage, ...		*/
 #include <asm/pgalloc.h>		/* pgd_*(), ...			*/
@@ -34,6 +35,11 @@ enum x86_pf_error_code {
 	PF_INSTR	=		1 << 4,
 };
 
+DEFINE_TRACE(page_fault_entry);
+DEFINE_TRACE(page_fault_exit);
+DEFINE_TRACE(page_fault_nosem_entry);
+DEFINE_TRACE(page_fault_nosem_exit);
+
 /*
  * Returns 0 if mmiotrace is disabled, or if the fault is not
  * handled by mmiotrace:
@@ -363,6 +369,7 @@ void vmalloc_sync_all(void)
  */
 static noinline int vmalloc_fault(unsigned long address)
 {
+	unsigned long pgd_paddr;
 	pgd_t *pgd, *pgd_ref;
 	pud_t *pud, *pud_ref;
 	pmd_t *pmd, *pmd_ref;
@@ -377,7 +384,8 @@ static noinline int vmalloc_fault(unsigned long address)
 	 * happen within a race in page table update. In the later
 	 * case just flush:
 	 */
-	pgd = pgd_offset(current->active_mm, address);
+	pgd_paddr = read_cr3();
+	pgd = __va(pgd_paddr) + pgd_index(address);
 	pgd_ref = pgd_offset_k(address);
 	if (pgd_none(*pgd_ref))
 		return -1;
@@ -730,6 +738,7 @@ __bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,
 		if (is_errata100(regs, address))
 			return;
 
+		trace_page_fault_nosem_entry(regs, 14, address);
 		if (unlikely(show_unhandled_signals))
 			show_signal_msg(regs, error_code, address, tsk);
 
@@ -739,6 +748,7 @@ __bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,
 		tsk->thread.trap_no	= 14;
 
 		force_sig_info_fault(SIGSEGV, si_code, address, tsk);
+		trace_page_fault_nosem_exit();
 
 		return;
 	}
@@ -1114,7 +1124,9 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault:
 	 */
+	trace_page_fault_entry(regs, 14, mm, vma, address, write);
 	fault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);
+	trace_page_fault_exit(fault);
 
 	if (unlikely(fault & VM_FAULT_ERROR)) {
 		mm_fault_error(regs, error_code, address, fault);
diff --git a/stblinux-2.6.31/arch/x86/mm/tlb.c b/stblinux-2.6.31/arch/x86/mm/tlb.c
index c814e14..df2d712 100644
--- a/stblinux-2.6.31/arch/x86/mm/tlb.c
+++ b/stblinux-2.6.31/arch/x86/mm/tlb.c
@@ -5,6 +5,7 @@
 #include <linux/smp.h>
 #include <linux/interrupt.h>
 #include <linux/module.h>
+#include <trace/irq.h>
 
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
@@ -136,6 +137,8 @@ void smp_invalidate_interrupt(struct pt_regs *regs)
 	sender = ~regs->orig_ax - INVALIDATE_TLB_VECTOR_START;
 	f = &flush_state[sender];
 
+	trace_irq_entry(sender, regs, NULL);
+
 	if (!cpumask_test_cpu(cpu, to_cpumask(f->flush_cpumask)))
 		goto out;
 		/*
@@ -162,6 +165,7 @@ out:
 	cpumask_clear_cpu(cpu, to_cpumask(f->flush_cpumask));
 	smp_mb__after_clear_bit();
 	inc_irq_stat(irq_tlb_count);
+	trace_irq_exit(IRQ_HANDLED);
 }
 
 static void flush_tlb_others_ipi(const struct cpumask *cpumask,
diff --git a/stblinux-2.6.31/arch/x86/xen/enlighten.c b/stblinux-2.6.31/arch/x86/xen/enlighten.c
index a11a115..61fccfd 100644
--- a/stblinux-2.6.31/arch/x86/xen/enlighten.c
+++ b/stblinux-2.6.31/arch/x86/xen/enlighten.c
@@ -960,6 +960,7 @@ static const struct pv_cpu_ops xen_cpu_ops __initdata = {
 	.read_pmc = native_read_pmc,
 
 	.iret = xen_iret,
+	.nmi_return = xen_iret,
 	.irq_enable_sysexit = xen_sysexit,
 #ifdef CONFIG_X86_64
 	.usergs_sysret32 = xen_sysret32,
diff --git a/stblinux-2.6.31/arch/xtensa/include/asm/thread_info.h b/stblinux-2.6.31/arch/xtensa/include/asm/thread_info.h
index 1316564..27c0d58 100644
--- a/stblinux-2.6.31/arch/xtensa/include/asm/thread_info.h
+++ b/stblinux-2.6.31/arch/xtensa/include/asm/thread_info.h
@@ -131,6 +131,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_IRET		4	/* return with iret */
 #define TIF_MEMDIE		5
 #define TIF_RESTORE_SIGMASK	6	/* restore signal mask in do_signal() */
+#define TIF_KERNEL_TRACE	7	/* kernel trace active */
 #define TIF_POLLING_NRFLAG	16	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_FREEZE		17	/* is freezing for suspend */
 
@@ -139,11 +140,12 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_SINGLESTEP		(1<<TIF_SINGLESTEP)
 #define _TIF_IRET		(1<<TIF_IRET)
+#define _TIF_KERNEL_TRACE	(1<<TIF_KERNEL_TRACE)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
 
-#define _TIF_WORK_MASK		0x0000FFFE	/* work to do on interrupt/exception return */
+#define _TIF_WORK_MASK		0x0000FF7E	/* work to do on interrupt/exception return */
 #define _TIF_ALLWORK_MASK	0x0000FFFF	/* work to do on any return to u-space */
 
 /*
diff --git a/stblinux-2.6.31/drivers/input/input.c b/stblinux-2.6.31/drivers/input/input.c
index 7c237e6..be58c5b 100644
--- a/stblinux-2.6.31/drivers/input/input.c
+++ b/stblinux-2.6.31/drivers/input/input.c
@@ -165,6 +165,9 @@ static void input_handle_event(struct input_dev *dev,
 {
 	int disposition = INPUT_IGNORE_EVENT;
 
+	trace_mark(input, input_event,
+		   "type %u code %u value %d", type, code, value);
+
 	switch (type) {
 
 	case EV_SYN:
diff --git a/stblinux-2.6.31/drivers/net/wan/hd64570.c b/stblinux-2.6.31/drivers/net/wan/hd64570.c
index 1ea1ef6..bfe3094 100644
--- a/stblinux-2.6.31/drivers/net/wan/hd64570.c
+++ b/stblinux-2.6.31/drivers/net/wan/hd64570.c
@@ -105,7 +105,7 @@ static inline u16 desc_abs_number(port_t *port, u16 desc, int transmit)
 }
 
 
-static inline u16 desc_offset(port_t *port, u16 desc, int transmit)
+static inline u16 hd_desc_offset(port_t *port, u16 desc, int transmit)
 {
 	/* Descriptor offset always fits in 16 bits */
 	return desc_abs_number(port, desc, transmit) * sizeof(pkt_desc);
@@ -117,10 +117,10 @@ static inline pkt_desc __iomem *desc_address(port_t *port, u16 desc,
 {
 #ifdef PAGE0_ALWAYS_MAPPED
 	return (pkt_desc __iomem *)(win0base(port_to_card(port))
-				    + desc_offset(port, desc, transmit));
+				    + hd_desc_offset(port, desc, transmit));
 #else
 	return (pkt_desc __iomem *)(winbase(port_to_card(port))
-				    + desc_offset(port, desc, transmit));
+				    + hd_desc_offset(port, desc, transmit));
 #endif
 }
 
@@ -170,7 +170,7 @@ static void sca_init_port(port_t *port)
 
 		for (i = 0; i < buffs; i++) {
 			pkt_desc __iomem *desc = desc_address(port, i, transmit);
-			u16 chain_off = desc_offset(port, i + 1, transmit);
+			u16 chain_off = hd_desc_offset(port, i + 1, transmit);
 			u32 buff_off = buffer_offset(port, i, transmit);
 
 			writew(chain_off, &desc->cp);
@@ -188,12 +188,12 @@ static void sca_init_port(port_t *port)
 
 		/* current desc addr */
 		sca_out(0, dmac + CPB, card); /* pointer base */
-		sca_outw(desc_offset(port, 0, transmit), dmac + CDAL, card);
+		sca_outw(hd_desc_offset(port, 0, transmit), dmac + CDAL, card);
 		if (!transmit)
-			sca_outw(desc_offset(port, buffs - 1, transmit),
+			sca_outw(hd_desc_offset(port, buffs - 1, transmit),
 				 dmac + EDAL, card);
 		else
-			sca_outw(desc_offset(port, 0, transmit), dmac + EDAL,
+			sca_outw(hd_desc_offset(port, 0, transmit), dmac + EDAL,
 				 card);
 
 		/* clear frame end interrupt counter */
@@ -306,7 +306,7 @@ static inline void sca_rx_intr(port_t *port)
 		dev->stats.rx_over_errors++;
 
 	while (1) {
-		u32 desc_off = desc_offset(port, port->rxin, 0);
+		u32 desc_off = hd_desc_offset(port, port->rxin, 0);
 		pkt_desc __iomem *desc;
 		u32 cda = sca_inw(dmac + CDAL, card);
 
@@ -360,7 +360,7 @@ static inline void sca_tx_intr(port_t *port)
 	while (1) {
 		pkt_desc __iomem *desc;
 
-		u32 desc_off = desc_offset(port, port->txlast, 1);
+		u32 desc_off = hd_desc_offset(port, port->txlast, 1);
 		u32 cda = sca_inw(dmac + CDAL, card);
 		if ((cda >= desc_off) && (cda < desc_off + sizeof(pkt_desc)))
 			break;	/* Transmitter is/will_be sending this frame */
@@ -662,7 +662,7 @@ static int sca_xmit(struct sk_buff *skb, struct net_device *dev)
 	dev->trans_start = jiffies;
 
 	port->txin = next_desc(port, port->txin, 1);
-	sca_outw(desc_offset(port, port->txin, 1),
+	sca_outw(hd_desc_offset(port, port->txin, 1),
 		 get_dmac_tx(port) + EDAL, card);
 
 	sca_out(DSR_DE, DSR_TX(phy_node(port)), card); /* Enable TX DMA */
diff --git a/stblinux-2.6.31/drivers/net/wan/hd64572.c b/stblinux-2.6.31/drivers/net/wan/hd64572.c
index f099c34..633e283 100644
--- a/stblinux-2.6.31/drivers/net/wan/hd64572.c
+++ b/stblinux-2.6.31/drivers/net/wan/hd64572.c
@@ -88,7 +88,7 @@ static inline u16 desc_abs_number(port_t *port, u16 desc, int transmit)
 }
 
 
-static inline u16 desc_offset(port_t *port, u16 desc, int transmit)
+static inline u16 hd_desc_offset(port_t *port, u16 desc, int transmit)
 {
 	/* Descriptor offset always fits in 16 bits */
 	return desc_abs_number(port, desc, transmit) * sizeof(pkt_desc);
@@ -99,7 +99,7 @@ static inline pkt_desc __iomem *desc_address(port_t *port, u16 desc,
 					     int transmit)
 {
 	return (pkt_desc __iomem *)(port->card->rambase +
-				    desc_offset(port, desc, transmit));
+				    hd_desc_offset(port, desc, transmit));
 }
 
 
@@ -144,7 +144,7 @@ static void sca_init_port(port_t *port)
 
 		for (i = 0; i < buffs; i++) {
 			pkt_desc __iomem *desc = desc_address(port, i, transmit);
-			u16 chain_off = desc_offset(port, i + 1, transmit);
+			u16 chain_off = hd_desc_offset(port, i + 1, transmit);
 			u32 buff_off = buffer_offset(port, i, transmit);
 
 			writel(chain_off, &desc->cp);
@@ -163,11 +163,11 @@ static void sca_init_port(port_t *port)
 	sca_out(DCR_ABORT, DCR_TX(port->chan), card);
 
 	/* current desc addr */
-	sca_outl(desc_offset(port, 0, 0), dmac_rx + CDAL, card);
-	sca_outl(desc_offset(port, card->tx_ring_buffers - 1, 0),
+	sca_outl(hd_desc_offset(port, 0, 0), dmac_rx + CDAL, card);
+	sca_outl(hd_desc_offset(port, card->tx_ring_buffers - 1, 0),
 		 dmac_rx + EDAL, card);
-	sca_outl(desc_offset(port, 0, 1), dmac_tx + CDAL, card);
-	sca_outl(desc_offset(port, 0, 1), dmac_tx + EDAL, card);
+	sca_outl(hd_desc_offset(port, 0, 1), dmac_tx + CDAL, card);
+	sca_outl(hd_desc_offset(port, 0, 1), dmac_tx + EDAL, card);
 
 	/* clear frame end interrupt counter */
 	sca_out(DCR_CLEAR_EOF, DCR_RX(port->chan), card);
@@ -250,7 +250,7 @@ static inline int sca_rx_done(port_t *port, int budget)
 		dev->stats.rx_over_errors++;
 
 	while (received < budget) {
-		u32 desc_off = desc_offset(port, port->rxin, 0);
+		u32 desc_off = hd_desc_offset(port, port->rxin, 0);
 		pkt_desc __iomem *desc;
 		u32 cda = sca_inl(dmac + CDAL, card);
 
@@ -589,7 +589,7 @@ static int sca_xmit(struct sk_buff *skb, struct net_device *dev)
 	dev->trans_start = jiffies;
 
 	port->txin = (port->txin + 1) % card->tx_ring_buffers;
-	sca_outl(desc_offset(port, port->txin, 1),
+	sca_outl(hd_desc_offset(port, port->txin, 1),
 		 get_dmac_tx(port) + EDAL, card);
 
 	sca_out(DSR_DE, DSR_TX(port->chan), card); /* Enable TX DMA */
diff --git a/stblinux-2.6.31/fs/buffer.c b/stblinux-2.6.31/fs/buffer.c
index 28f320f..4aef49c 100644
--- a/stblinux-2.6.31/fs/buffer.c
+++ b/stblinux-2.6.31/fs/buffer.c
@@ -41,11 +41,15 @@
 #include <linux/bitops.h>
 #include <linux/mpage.h>
 #include <linux/bit_spinlock.h>
+#include <trace/fs.h>
 
 static int fsync_buffers_list(spinlock_t *lock, struct list_head *list);
 
 #define BH_ENTRY(list) list_entry((list), struct buffer_head, b_assoc_buffers)
 
+DEFINE_TRACE(fs_buffer_wait_start);
+DEFINE_TRACE(fs_buffer_wait_end);
+
 inline void
 init_buffer(struct buffer_head *bh, bh_end_io_t *handler, void *private)
 {
@@ -88,7 +92,9 @@ void unlock_buffer(struct buffer_head *bh)
  */
 void __wait_on_buffer(struct buffer_head * bh)
 {
+	trace_fs_buffer_wait_start(bh);
 	wait_on_bit(&bh->b_state, BH_Lock, sync_buffer, TASK_UNINTERRUPTIBLE);
+	trace_fs_buffer_wait_end(bh);
 }
 
 static void
diff --git a/stblinux-2.6.31/fs/compat.c b/stblinux-2.6.31/fs/compat.c
index 6d6f98f..853d63a 100644
--- a/stblinux-2.6.31/fs/compat.c
+++ b/stblinux-2.6.31/fs/compat.c
@@ -51,6 +51,7 @@
 #include <linux/mm.h>
 #include <linux/eventpoll.h>
 #include <linux/fs_struct.h>
+#include <trace/fs.h>
 
 #include <asm/uaccess.h>
 #include <asm/mmu_context.h>
@@ -1539,6 +1540,7 @@ int compat_do_execve(char * filename,
 	if (retval < 0)
 		goto out;
 
+	trace_fs_exec(filename);
 	/* execve succeeded */
 	current->fs->in_exec = 0;
 	current->in_execve = 0;
diff --git a/stblinux-2.6.31/fs/exec.c b/stblinux-2.6.31/fs/exec.c
index 62cd056..43e8918 100644
--- a/stblinux-2.6.31/fs/exec.c
+++ b/stblinux-2.6.31/fs/exec.c
@@ -55,6 +55,7 @@
 #include <linux/kmod.h>
 #include <linux/fsnotify.h>
 #include <linux/fs_struct.h>
+#include <trace/fs.h>
 
 #include <asm/uaccess.h>
 #include <asm/mmu_context.h>
@@ -70,6 +71,11 @@ int suid_dumpable = 0;
 static LIST_HEAD(formats);
 static DEFINE_RWLOCK(binfmt_lock);
 
+/*
+ * Also used in compat.c.
+ */
+DEFINE_TRACE(fs_exec);
+
 int __register_binfmt(struct linux_binfmt * fmt, int insert)
 {
 	if (!fmt)
@@ -1360,6 +1366,7 @@ int do_execve(char * filename,
 	if (retval < 0)
 		goto out;
 
+	trace_fs_exec(filename);
 	/* execve succeeded */
 	current->fs->in_exec = 0;
 	current->in_execve = 0;
diff --git a/stblinux-2.6.31/fs/ioctl.c b/stblinux-2.6.31/fs/ioctl.c
index 5612880..ba14850 100644
--- a/stblinux-2.6.31/fs/ioctl.c
+++ b/stblinux-2.6.31/fs/ioctl.c
@@ -16,9 +16,12 @@
 #include <linux/writeback.h>
 #include <linux/buffer_head.h>
 #include <linux/falloc.h>
+#include <trace/fs.h>
 
 #include <asm/ioctls.h>
 
+DEFINE_TRACE(fs_ioctl);
+
 /* So that the fiemap access checks can't overflow on 32 bit machines. */
 #define FIEMAP_MAX_EXTENTS	(UINT_MAX / sizeof(struct fiemap_extent))
 
@@ -616,6 +619,8 @@ SYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd, unsigned long, arg)
 	if (!filp)
 		goto out;
 
+	trace_fs_ioctl(fd, cmd, arg);
+
 	error = security_file_ioctl(filp, cmd, arg);
 	if (error)
 		goto out_fput;
diff --git a/stblinux-2.6.31/fs/open.c b/stblinux-2.6.31/fs/open.c
index dd98e80..26ab656 100644
--- a/stblinux-2.6.31/fs/open.c
+++ b/stblinux-2.6.31/fs/open.c
@@ -30,6 +30,10 @@
 #include <linux/audit.h>
 #include <linux/falloc.h>
 #include <linux/fs_struct.h>
+#include <trace/fs.h>
+
+DEFINE_TRACE(fs_open);
+DEFINE_TRACE(fs_close);
 
 int vfs_statfs(struct dentry *dentry, struct kstatfs *buf)
 {
@@ -1041,6 +1045,7 @@ long do_sys_open(int dfd, const char __user *filename, int flags, int mode)
 				fsnotify_open(f->f_path.dentry);
 				fd_install(fd, f);
 			}
+			trace_fs_open(fd, tmp);
 		}
 		putname(tmp);
 	}
@@ -1130,6 +1135,7 @@ SYSCALL_DEFINE1(close, unsigned int, fd)
 	filp = fdt->fd[fd];
 	if (!filp)
 		goto out_unlock;
+	trace_fs_close(fd);
 	rcu_assign_pointer(fdt->fd[fd], NULL);
 	FD_CLR(fd, fdt->close_on_exec);
 	__put_unused_fd(files, fd);
diff --git a/stblinux-2.6.31/fs/pipe.c b/stblinux-2.6.31/fs/pipe.c
index ae17d02..df427d5 100644
--- a/stblinux-2.6.31/fs/pipe.c
+++ b/stblinux-2.6.31/fs/pipe.c
@@ -222,6 +222,7 @@ void *generic_pipe_buf_map(struct pipe_inode_info *pipe,
 
 	return kmap(buf->page);
 }
+EXPORT_SYMBOL_GPL(generic_pipe_buf_map);
 
 /**
  * generic_pipe_buf_unmap - unmap a previously mapped pipe buffer
@@ -241,6 +242,7 @@ void generic_pipe_buf_unmap(struct pipe_inode_info *pipe,
 	} else
 		kunmap(buf->page);
 }
+EXPORT_SYMBOL_GPL(generic_pipe_buf_unmap);
 
 /**
  * generic_pipe_buf_steal - attempt to take ownership of a &pipe_buffer
@@ -271,6 +273,7 @@ int generic_pipe_buf_steal(struct pipe_inode_info *pipe,
 
 	return 1;
 }
+EXPORT_SYMBOL_GPL(generic_pipe_buf_steal);
 
 /**
  * generic_pipe_buf_get - get a reference to a &struct pipe_buffer
@@ -286,6 +289,7 @@ void generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)
 {
 	page_cache_get(buf->page);
 }
+EXPORT_SYMBOL_GPL(generic_pipe_buf_get);
 
 /**
  * generic_pipe_buf_confirm - verify contents of the pipe buffer
@@ -301,6 +305,7 @@ int generic_pipe_buf_confirm(struct pipe_inode_info *info,
 {
 	return 0;
 }
+EXPORT_SYMBOL_GPL(generic_pipe_buf_confirm);
 
 /**
  * generic_pipe_buf_release - put a reference to a &struct pipe_buffer
diff --git a/stblinux-2.6.31/fs/read_write.c b/stblinux-2.6.31/fs/read_write.c
index 6c8c55d..02688be 100644
--- a/stblinux-2.6.31/fs/read_write.c
+++ b/stblinux-2.6.31/fs/read_write.c
@@ -16,6 +16,7 @@
 #include <linux/syscalls.h>
 #include <linux/pagemap.h>
 #include <linux/splice.h>
+#include <trace/fs.h>
 #include "read_write.h"
 
 #include <asm/uaccess.h>
@@ -31,6 +32,15 @@ const struct file_operations generic_ro_fops = {
 
 EXPORT_SYMBOL(generic_ro_fops);
 
+DEFINE_TRACE(fs_lseek);
+DEFINE_TRACE(fs_llseek);
+DEFINE_TRACE(fs_read);
+DEFINE_TRACE(fs_write);
+DEFINE_TRACE(fs_pread64);
+DEFINE_TRACE(fs_pwrite64);
+DEFINE_TRACE(fs_readv);
+DEFINE_TRACE(fs_writev);
+
 /**
  * generic_file_llseek_unlocked - lockless generic llseek implementation
  * @file:	file structure to seek on
@@ -165,6 +175,9 @@ SYSCALL_DEFINE3(lseek, unsigned int, fd, off_t, offset, unsigned int, origin)
 		if (res != (loff_t)retval)
 			retval = -EOVERFLOW;	/* LFS: should only happen on 32 bit platforms */
 	}
+
+	trace_fs_lseek(fd, offset, origin);
+
 	fput_light(file, fput_needed);
 bad:
 	return retval;
@@ -192,6 +205,8 @@ SYSCALL_DEFINE5(llseek, unsigned int, fd, unsigned long, offset_high,
 	offset = vfs_llseek(file, ((loff_t) offset_high << 32) | offset_low,
 			origin);
 
+	trace_fs_llseek(fd, offset, origin);
+
 	retval = (int)offset;
 	if (offset >= 0) {
 		retval = -EFAULT;
@@ -379,6 +394,7 @@ SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count)
 	if (file) {
 		loff_t pos = file_pos_read(file);
 		ret = vfs_read(file, buf, count, &pos);
+		trace_fs_read(fd, buf, count, ret);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
 	}
@@ -397,6 +413,7 @@ SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf,
 	if (file) {
 		loff_t pos = file_pos_read(file);
 		ret = vfs_write(file, buf, count, &pos);
+		trace_fs_write(fd, buf, count, ret);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
 	}
@@ -417,8 +434,11 @@ SYSCALL_DEFINE(pread64)(unsigned int fd, char __user *buf,
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		ret = -ESPIPE;
-		if (file->f_mode & FMODE_PREAD)
+		if (file->f_mode & FMODE_PREAD) {
 			ret = vfs_read(file, buf, count, &pos);
+			trace_fs_pread64(fd, buf, count, pos, ret);
+		}
+
 		fput_light(file, fput_needed);
 	}
 
@@ -446,8 +466,10 @@ SYSCALL_DEFINE(pwrite64)(unsigned int fd, const char __user *buf,
 	file = fget_light(fd, &fput_needed);
 	if (file) {
 		ret = -ESPIPE;
-		if (file->f_mode & FMODE_PWRITE)  
+		if (file->f_mode & FMODE_PWRITE) {
 			ret = vfs_write(file, buf, count, &pos);
+			trace_fs_pwrite64(fd, buf, count, pos, ret);
+		}
 		fput_light(file, fput_needed);
 	}
 
@@ -700,6 +722,7 @@ SYSCALL_DEFINE3(readv, unsigned long, fd, const struct iovec __user *, vec,
 	if (file) {
 		loff_t pos = file_pos_read(file);
 		ret = vfs_readv(file, vec, vlen, &pos);
+		trace_fs_readv(fd, vec, vlen, ret);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
 	}
@@ -721,6 +744,7 @@ SYSCALL_DEFINE3(writev, unsigned long, fd, const struct iovec __user *, vec,
 	if (file) {
 		loff_t pos = file_pos_read(file);
 		ret = vfs_writev(file, vec, vlen, &pos);
+		trace_fs_writev(fd, vec, vlen, ret);
 		file_pos_write(file, pos);
 		fput_light(file, fput_needed);
 	}
diff --git a/stblinux-2.6.31/fs/select.c b/stblinux-2.6.31/fs/select.c
index 8084834..706b783 100644
--- a/stblinux-2.6.31/fs/select.c
+++ b/stblinux-2.6.31/fs/select.c
@@ -25,6 +25,7 @@
 #include <linux/fs.h>
 #include <linux/rcupdate.h>
 #include <linux/hrtimer.h>
+#include <trace/fs.h>
 
 #include <asm/uaccess.h>
 
@@ -91,6 +92,9 @@ struct poll_table_page {
 #define POLL_TABLE_FULL(table) \
 	((unsigned long)((table)->entry+1) > PAGE_SIZE + (unsigned long)(table))
 
+DEFINE_TRACE(fs_select);
+DEFINE_TRACE(fs_poll);
+
 /*
  * Ok, Peter made a complicated, but straightforward multiple_wait() function.
  * I have rewritten this, taking some shortcuts: This code may not be easy to
@@ -105,6 +109,9 @@ struct poll_table_page {
  */
 static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,
 		       poll_table *p);
+static void __pollwait_exclusive(struct file *filp,
+				 wait_queue_head_t *wait_address,
+				 poll_table *p);
 
 void poll_initwait(struct poll_wqueues *pwq)
 {
@@ -145,6 +152,20 @@ void poll_freewait(struct poll_wqueues *pwq)
 }
 EXPORT_SYMBOL(poll_freewait);
 
+/**
+ * poll_wait_set_exclusive - set poll wait queue to exclusive
+ *
+ * Sets up a poll wait queue to use exclusive wakeups. This is useful to
+ * wake up only one waiter at each wakeup. Used to work-around "thundering herd"
+ * problem.
+ */
+void poll_wait_set_exclusive(poll_table *p)
+{
+	if (p)
+		init_poll_funcptr(p, __pollwait_exclusive);
+}
+EXPORT_SYMBOL(poll_wait_set_exclusive);
+
 static struct poll_table_entry *poll_get_entry(struct poll_wqueues *p)
 {
 	struct poll_table_page *table = p->table;
@@ -206,8 +227,10 @@ static int pollwake(wait_queue_t *wait, unsigned mode, int sync, void *key)
 }
 
 /* Add a new entry */
-static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,
-				poll_table *p)
+static void __pollwait_common(struct file *filp,
+			      wait_queue_head_t *wait_address,
+			      poll_table *p,
+			      int exclusive)
 {
 	struct poll_wqueues *pwq = container_of(p, struct poll_wqueues, pt);
 	struct poll_table_entry *entry = poll_get_entry(pwq);
@@ -219,7 +242,23 @@ static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,
 	entry->key = p->key;
 	init_waitqueue_func_entry(&entry->wait, pollwake);
 	entry->wait.private = pwq;
-	add_wait_queue(wait_address, &entry->wait);
+	if (!exclusive)
+		add_wait_queue(wait_address, &entry->wait);
+	else
+		add_wait_queue_exclusive(wait_address, &entry->wait);
+}
+
+static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,
+				poll_table *p)
+{
+	__pollwait_common(filp, wait_address, p, 0);
+}
+
+static void __pollwait_exclusive(struct file *filp,
+				 wait_queue_head_t *wait_address,
+				 poll_table *p)
+{
+	__pollwait_common(filp, wait_address, p, 1);
 }
 
 int poll_schedule_timeout(struct poll_wqueues *pwq, int state,
@@ -441,6 +480,7 @@ int do_select(int n, fd_set_bits *fds, struct timespec *end_time)
 				file = fget_light(i, &fput_needed);
 				if (file) {
 					f_op = file->f_op;
+					trace_fs_select(i, end_time);
 					mask = DEFAULT_POLLMASK;
 					if (f_op && f_op->poll) {
 						wait_key_set(wait, in, out, bit);
@@ -713,6 +753,7 @@ static inline unsigned int do_pollfd(struct pollfd *pollfd, poll_table *pwait)
 		file = fget_light(fd, &fput_needed);
 		mask = POLLNVAL;
 		if (file != NULL) {
+			trace_fs_poll(fd);
 			mask = DEFAULT_POLLMASK;
 			if (file->f_op && file->f_op->poll) {
 				if (pwait)
diff --git a/stblinux-2.6.31/fs/seq_file.c b/stblinux-2.6.31/fs/seq_file.c
index 6c95927..0e3c9fa 100644
--- a/stblinux-2.6.31/fs/seq_file.c
+++ b/stblinux-2.6.31/fs/seq_file.c
@@ -691,5 +691,48 @@ struct list_head *seq_list_next(void *v, struct list_head *head, loff_t *ppos)
 	++*ppos;
 	return lh == head ? NULL : lh;
 }
-
 EXPORT_SYMBOL(seq_list_next);
+
+struct list_head *seq_sorted_list_start(struct list_head *head, loff_t *ppos)
+{
+	struct list_head *lh;
+
+	list_for_each(lh, head)
+		if ((unsigned long)lh >= *ppos) {
+			*ppos = (unsigned long)lh;
+			return lh;
+		}
+	return NULL;
+}
+EXPORT_SYMBOL(seq_sorted_list_start);
+
+struct list_head *seq_sorted_list_start_head(struct list_head *head,
+		loff_t *ppos)
+{
+	struct list_head *lh;
+
+	if (!*ppos) {
+		*ppos = (unsigned long)head;
+		return head;
+	}
+	list_for_each(lh, head)
+		if ((unsigned long)lh >= *ppos) {
+			*ppos = (long)lh->prev;
+			return lh->prev;
+		}
+	return NULL;
+}
+EXPORT_SYMBOL(seq_sorted_list_start_head);
+
+struct list_head *seq_sorted_list_next(void *p, struct list_head *head,
+		loff_t *ppos)
+{
+	struct list_head *lh;
+	void *next;
+
+	lh = ((struct list_head *)p)->next;
+	next = (lh == head) ? NULL : lh;
+	*ppos = next ? ((unsigned long)next) : (-1UL);
+	return next;
+}
+EXPORT_SYMBOL(seq_sorted_list_next);
diff --git a/stblinux-2.6.31/fs/splice.c b/stblinux-2.6.31/fs/splice.c
index 73766d2..7c5561a 100644
--- a/stblinux-2.6.31/fs/splice.c
+++ b/stblinux-2.6.31/fs/splice.c
@@ -258,6 +258,7 @@ ssize_t splice_to_pipe(struct pipe_inode_info *pipe,
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(splice_to_pipe);
 
 static void spd_release_page(struct splice_pipe_desc *spd, unsigned int i)
 {
diff --git a/stblinux-2.6.31/include/asm-generic/trace-clock.h b/stblinux-2.6.31/include/asm-generic/trace-clock.h
new file mode 100644
index 0000000..138ac9b
--- /dev/null
+++ b/stblinux-2.6.31/include/asm-generic/trace-clock.h
@@ -0,0 +1,76 @@
+#ifndef _ASM_GENERIC_TRACE_CLOCK_H
+#define _ASM_GENERIC_TRACE_CLOCK_H
+
+/*
+ * include/asm-generic/trace-clock.h
+ *
+ * Copyright (C) 2007 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Generic tracing clock for architectures without TSC.
+ */
+
+#include <linux/param.h>	/* For HZ */
+#include <asm/atomic.h>
+
+#define TRACE_CLOCK_SHIFT 13
+
+/*
+ * Number of hardware clock bits. The higher order bits are expected to be 0.
+ * If the hardware clock source has more than 32 bits, the bits higher than the
+ * 32nd will be truncated by a cast to a 32 bits unsigned. Range : 1 - 32.
+ * (too few bits would be unrealistic though, since we depend on the timer to
+ * detect the overflows).
+ */
+#define TC_HW_BITS			32
+
+/* Expected maximum interrupt latency in ms : 15ms, *2 for security */
+#define TC_EXPECTED_INTERRUPT_LATENCY	30
+
+extern atomic_long_t trace_clock_var;
+
+static inline u32 trace_clock_read32(void)
+{
+	return (u32)atomic_long_add_return(1, &trace_clock_var);
+}
+
+#ifdef CONFIG_HAVE_TRACE_CLOCK_32_TO_64
+extern u64 trace_clock_read_synthetic_tsc(void);
+extern void get_synthetic_tsc(void);
+extern void put_synthetic_tsc(void);
+
+static inline u64 trace_clock_read64(void)
+{
+	return trace_clock_read_synthetic_tsc();
+}
+#else
+static inline void get_synthetic_tsc(void)
+{
+}
+
+static inline void put_synthetic_tsc(void)
+{
+}
+
+static inline u64 trace_clock_read64(void)
+{
+	return atomic_long_add_return(1, &trace_clock_var);
+}
+#endif
+
+static inline unsigned int trace_clock_frequency(void)
+{
+	return HZ << TRACE_CLOCK_SHIFT;
+}
+
+static inline u32 trace_clock_freq_scale(void)
+{
+	return 1;
+}
+
+extern void get_trace_clock(void);
+extern void put_trace_clock(void);
+
+static inline void set_trace_clock_is_sync(int state)
+{
+}
+#endif /* _ASM_GENERIC_TRACE_CLOCK_H */
diff --git a/stblinux-2.6.31/include/asm-generic/vmlinux.lds.h b/stblinux-2.6.31/include/asm-generic/vmlinux.lds.h
index 6ad76bf..d33a897 100644
--- a/stblinux-2.6.31/include/asm-generic/vmlinux.lds.h
+++ b/stblinux-2.6.31/include/asm-generic/vmlinux.lds.h
@@ -156,6 +156,9 @@
 	VMLINUX_SYMBOL(__start___tracepoints) = .;			\
 	*(__tracepoints)						\
 	VMLINUX_SYMBOL(__stop___tracepoints) = .;			\
+	VMLINUX_SYMBOL(__start___imv) = .;				\
+	*(__imv)		/* Immediate values: pointers */	\
+	VMLINUX_SYMBOL(__stop___imv) = .;				\
 	/* implement dynamic printk debug */				\
 	. = ALIGN(8);							\
 	VMLINUX_SYMBOL(__start___verbose) = .;                          \
diff --git a/stblinux-2.6.31/include/asm-x86/call_64.h b/stblinux-2.6.31/include/asm-x86/call_64.h
new file mode 100644
index 0000000..b10cc58
--- /dev/null
+++ b/stblinux-2.6.31/include/asm-x86/call_64.h
@@ -0,0 +1,72 @@
+#ifndef __ASM_X86_CALL_64_H
+#define __ASM_X86_CALL_64_H
+
+/*
+ * asm-x86/call_64.h
+ *
+ * Use rax as first argument for the call. Useful when already returned by the
+ * previous instruction, such as cmpxchg.
+ * Leave rdi free to mov rax to rdi in the trampoline.
+ * Return value in rax.
+ *
+ * Saving the registers in the original caller because we cannot restore them in
+ * the trampoline. Save the same as "SAVE_ARGS".
+ *
+ * Copyright (C) 2008 Mathieu Desnoyers
+ */
+
+#define call_rax_rsi(symbol, rax, rsi)				\
+	({							\
+		unsigned long ret, modrsi;			\
+		asm volatile("callq asm_" #symbol "\n\t"	\
+			     : "=a" (ret), "=S" (modrsi)	\
+			     : "a" (rax), "S" (rsi)		\
+			     : "rdi", "rcx", "rdx",		\
+			       "%r8", "%r9", "%r10", "%r11",	\
+			       "cc", "memory");			\
+		ret;						\
+	})
+
+#define call_rbx_rsi(symbol, rbx, rsi)				\
+	({							\
+		unsigned long ret, modrsi;			\
+		asm volatile("callq asm_" #symbol "\n\t"	\
+			     : "=a" (ret), "=S" (modrsi)	\
+			     : "b" (rbx), "S" (rsi)		\
+			     : "rdi", "rcx", "rdx",		\
+			       "%r8", "%r9", "%r10", "%r11",	\
+			       "cc", "memory");			\
+		ret;						\
+	})
+
+#define psread_lock_slow_irq(v, rwlock)				\
+	call_rax_rsi(psread_lock_slow_irq, v, rwlock)
+#define psread_trylock_slow_irq(v, rwlock)			\
+	call_rax_rsi(psread_trylock_slow_irq, v, rwlock)
+#define psread_lock_slow_bh(v, rwlock)				\
+	call_rax_rsi(psread_lock_slow_bh, v, rwlock)
+#define psread_trylock_slow_bh(v, rwlock)			\
+	call_rax_rsi(psread_trylock_slow_bh, v, rwlock)
+#define psread_lock_slow_inatomic(v, rwlock)			\
+	call_rax_rsi(psread_lock_slow_inatomic, v, rwlock)
+#define psread_trylock_slow_inatomic(v, rwlock)			\
+	call_rax_rsi(psread_trylock_slow_inatomic, v, rwlock)
+#define psread_lock_slow(v, rwlock)				\
+	call_rax_rsi(psread_lock_slow, v, rwlock)
+#define psread_lock_interruptible_slow(v, rwlock)		\
+	call_rax_rsi(psread_lock_interruptible_slow, v, rwlock)
+#define psread_trylock_slow(v, rwlock)				\
+	call_rax_rsi(psread_trylock_slow, v, rwlock)
+
+#define pswrite_lock_slow(v, rwlock)				\
+	call_rax_rsi(pswrite_lock_slow, v, rwlock)
+#define pswrite_lock_interruptible_slow(v, rwlock)		\
+	call_rax_rsi(pswrite_lock_interruptible_slow, v, rwlock)
+#define pswrite_trylock_slow(v, rwlock)				\
+	call_rax_rsi(pswrite_trylock_slow, v, rwlock)
+#define pswrite_unlock_slow(v, rwlock)				\
+	call_rax_rsi(pswrite_unlock_slow, v, rwlock)
+#define psrwlock_wakeup(v, rwlock)				\
+	call_rbx_rsi(psrwlock_wakeup, v, rwlock)
+
+#endif
diff --git a/stblinux-2.6.31/include/linux/idle.h b/stblinux-2.6.31/include/linux/idle.h
new file mode 100644
index 0000000..75bd2a4
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/idle.h
@@ -0,0 +1,19 @@
+/*
+ * include/linux/idle.h - generic idle definition
+ *
+ */
+#ifndef _LINUX_IDLE_H_
+#define _LINUX_IDLE_H_
+
+#include <linux/notifier.h>
+
+enum idle_val {
+	IDLE_START = 1,
+	IDLE_END = 2,
+};
+
+int notify_idle(enum idle_val val);
+void register_idle_notifier(struct notifier_block *n);
+void unregister_idle_notifier(struct notifier_block *n);
+
+#endif /* _LINUX_IDLE_H_ */
diff --git a/stblinux-2.6.31/include/linux/immediate.h b/stblinux-2.6.31/include/linux/immediate.h
new file mode 100644
index 0000000..28d51fd
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/immediate.h
@@ -0,0 +1,85 @@
+#ifndef _LINUX_IMMEDIATE_H
+#define _LINUX_IMMEDIATE_H
+
+/*
+ * Immediate values, can be updated at runtime and save cache lines.
+ *
+ * (C) Copyright 2007 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * Dual BSD/GPL v2 license.
+ */
+
+#ifdef USE_IMMEDIATE
+
+#include <asm/immediate.h>
+
+/**
+ * imv_set - set immediate variable (with locking)
+ * @name: immediate value name
+ * @i: required value
+ *
+ * Sets the value of @name, taking the module_mutex if required by
+ * the architecture.
+ */
+#define imv_set(name, i)						\
+	do {								\
+		name##__imv = (i);					\
+		core_imv_update();					\
+		module_imv_update();					\
+	} while (0)
+
+/*
+ * Internal update functions.
+ */
+extern void core_imv_update(void);
+extern void imv_update_range(const struct __imv *begin,
+	const struct __imv *end);
+extern void imv_unref_core_init(void);
+extern void imv_unref(struct __imv *begin, struct __imv *end, void *start,
+		unsigned long size);
+
+#else
+
+/*
+ * Generic immediate values: a simple, standard, memory load.
+ */
+
+/**
+ * imv_read - read immediate variable
+ * @name: immediate value name
+ *
+ * Reads the value of @name.
+ */
+#define imv_read(name)			_imv_read(name)
+
+/**
+ * imv_set - set immediate variable (with locking)
+ * @name: immediate value name
+ * @i: required value
+ *
+ * Sets the value of @name, taking the module_mutex if required by
+ * the architecture.
+ */
+#define imv_set(name, i)		(name##__imv = (i))
+
+static inline void core_imv_update(void) { }
+static inline void imv_unref_core_init(void) { }
+
+#endif
+
+#define DECLARE_IMV(type, name) extern __typeof__(type) name##__imv
+#define DEFINE_IMV(type, name)  __typeof__(type) name##__imv
+
+#define EXPORT_IMV_SYMBOL(name) EXPORT_SYMBOL(name##__imv)
+#define EXPORT_IMV_SYMBOL_GPL(name) EXPORT_SYMBOL_GPL(name##__imv)
+
+/**
+ * _imv_read - Read immediate value with standard memory load.
+ * @name: immediate value name
+ *
+ * Force a data read of the immediate value instead of the immediate value
+ * based mechanism. Useful for __init and __exit section data read.
+ */
+#define _imv_read(name)		(name##__imv)
+
+#endif
diff --git a/stblinux-2.6.31/include/linux/irqnr.h b/stblinux-2.6.31/include/linux/irqnr.h
index ec87b21..dd8eec4 100644
--- a/stblinux-2.6.31/include/linux/irqnr.h
+++ b/stblinux-2.6.31/include/linux/irqnr.h
@@ -24,6 +24,7 @@
 #else /* CONFIG_GENERIC_HARDIRQS */
 
 extern int nr_irqs;
+struct irq_desc;
 extern struct irq_desc *irq_to_desc(unsigned int irq);
 
 # define for_each_irq_desc(irq, desc)					\
diff --git a/stblinux-2.6.31/include/linux/kvm_host.h b/stblinux-2.6.31/include/linux/kvm_host.h
index 3060bdc..301553b 100644
--- a/stblinux-2.6.31/include/linux/kvm_host.h
+++ b/stblinux-2.6.31/include/linux/kvm_host.h
@@ -460,22 +460,22 @@ extern struct kvm_stats_debugfs_item debugfs_entries[];
 extern struct dentry *kvm_debugfs_dir;
 
 #define KVMTRACE_5D(evt, vcpu, d1, d2, d3, d4, d5, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+	trace_mark(kvm, trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt,\
 						vcpu, 5, d1, d2, d3, d4, d5)
 #define KVMTRACE_4D(evt, vcpu, d1, d2, d3, d4, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+	trace_mark(kvm, trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt,\
 						vcpu, 4, d1, d2, d3, d4, 0)
 #define KVMTRACE_3D(evt, vcpu, d1, d2, d3, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+	trace_mark(kvm, trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt,\
 						vcpu, 3, d1, d2, d3, 0, 0)
 #define KVMTRACE_2D(evt, vcpu, d1, d2, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+	trace_mark(kvm, trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt,\
 						vcpu, 2, d1, d2, 0, 0, 0)
 #define KVMTRACE_1D(evt, vcpu, d1, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+	trace_mark(kvm, trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt,\
 						vcpu, 1, d1, 0, 0, 0, 0)
 #define KVMTRACE_0D(evt, vcpu, name) \
-	trace_mark(kvm_trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt, \
+	trace_mark(kvm, trace_##name, "%u %p %u %u %u %u %u %u", KVM_TRC_##evt,\
 						vcpu, 0, 0, 0, 0, 0, 0)
 
 #ifdef CONFIG_KVM_TRACE
diff --git a/stblinux-2.6.31/include/linux/lockdep.h b/stblinux-2.6.31/include/linux/lockdep.h
index b25d1b5..0027091 100644
--- a/stblinux-2.6.31/include/linux/lockdep.h
+++ b/stblinux-2.6.31/include/linux/lockdep.h
@@ -518,4 +518,19 @@ do {									\
 # define might_lock_read(lock) do { } while (0)
 #endif
 
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# ifdef CONFIG_PROVE_LOCKING
+#  define psrwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
+#  define psrwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 2, NULL, i)
+# else
+#  define psrwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
+#  define psrwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 1, NULL, i)
+# endif
+# define psrwlock_release(l, n, i)		lock_release(l, n, i)
+#else
+# define psrwlock_acquire(l, s, t, i)		do { } while (0)
+# define psrwlock_acquire_read(l, s, t, i)	do { } while (0)
+# define psrwlock_release(l, n, i)		do { } while (0)
+#endif
+
 #endif /* __LINUX_LOCKDEP_H */
diff --git a/stblinux-2.6.31/include/linux/ltt-channels.h b/stblinux-2.6.31/include/linux/ltt-channels.h
new file mode 100644
index 0000000..f51ba4f
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/ltt-channels.h
@@ -0,0 +1,62 @@
+#ifndef _LTT_CHANNELS_H
+#define _LTT_CHANNELS_H
+
+/*
+ * Copyright (C) 2008 Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Dynamic tracer channel allocation.
+
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/ltt-relay.h>
+#include <linux/limits.h>
+#include <linux/kref.h>
+#include <linux/list.h>
+#include <linux/timer.h>
+
+#define EVENTS_PER_CHANNEL	65536
+
+struct ltt_chan {
+	struct ltt_chan_alloc a;		/* Parent. First field. */
+	int overwrite:1;
+	int active:1;
+	unsigned long commit_count_mask;	/*
+						 * Commit count mask, removing
+						 * the MSBs corresponding to
+						 * bits used to represent the
+						 * subbuffer index.
+						 */
+	unsigned long switch_timer_interval;
+};
+
+struct ltt_channel_setting {
+	unsigned int sb_size;
+	unsigned int n_sb;
+	struct kref kref;	/* Number of references to structure content */
+	struct list_head list;
+	unsigned int index;	/* index of channel in trace channel array */
+	u16 free_event_id;	/* Next event ID to allocate */
+	char name[PATH_MAX];
+};
+
+int ltt_channels_register(const char *name);
+int ltt_channels_unregister(const char *name, int compacting);
+int ltt_channels_set_default(const char *name,
+			     unsigned int subbuf_size,
+			     unsigned int subbuf_cnt);
+const char *ltt_channels_get_name_from_index(unsigned int index);
+int ltt_channels_get_index_from_name(const char *name);
+int ltt_channels_trace_ref(void);
+struct ltt_chan *ltt_channels_trace_alloc(unsigned int *nr_channels,
+					  int overwrite, int active);
+void ltt_channels_trace_free(struct ltt_chan *channels,
+			     unsigned int nr_channels);
+void ltt_channels_trace_set_timer(struct ltt_chan *chan,
+				  unsigned long interval);
+
+int _ltt_channels_get_event_id(const char *channel, const char *name);
+int ltt_channels_get_event_id(const char *channel, const char *name);
+void _ltt_channels_reset_event_ids(void);
+
+#endif /* _LTT_CHANNELS_H */
diff --git a/stblinux-2.6.31/include/linux/ltt-core.h b/stblinux-2.6.31/include/linux/ltt-core.h
new file mode 100644
index 0000000..d1918d3
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/ltt-core.h
@@ -0,0 +1,95 @@
+/*
+ * Copyright (C) 2005,2006 Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * This contains the core definitions for the Linux Trace Toolkit.
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#ifndef LTT_CORE_H
+#define LTT_CORE_H
+
+#include <linux/list.h>
+#include <linux/percpu.h>
+
+/* ltt's root dir in debugfs */
+#define LTT_ROOT        "ltt"
+
+/*
+ * All modifications of ltt_traces must be done by ltt-tracer.c, while holding
+ * the semaphore. Only reading of this information can be done elsewhere, with
+ * the RCU mechanism : the preemption must be disabled while reading the
+ * list.
+ */
+struct ltt_traces {
+	struct list_head setup_head;	/* Pre-allocated traces list */
+	struct list_head head;		/* Allocated Traces list */
+	unsigned int num_active_traces;	/* Number of active traces */
+} ____cacheline_aligned;
+
+extern struct ltt_traces ltt_traces;
+
+/*
+ * get dentry of ltt's root dir
+ */
+struct dentry *get_ltt_root(void);
+
+void put_ltt_root(void);
+
+/* Keep track of trap nesting inside LTT */
+DECLARE_PER_CPU(unsigned int, ltt_nesting);
+
+typedef int (*ltt_run_filter_functor)(void *trace, uint16_t eID);
+
+extern ltt_run_filter_functor ltt_run_filter;
+
+extern void ltt_filter_register(ltt_run_filter_functor func);
+extern void ltt_filter_unregister(void);
+
+#if defined(CONFIG_LTT) && defined(CONFIG_LTT_ALIGNMENT)
+
+/*
+ * Calculate the offset needed to align the type.
+ * size_of_type must be non-zero.
+ *
+ * Do not limit alignment on architecture size anymore, because uint64_t types
+ * are aligned on 64-bit even on 32-bit archs.
+ */
+static inline unsigned int ltt_align(size_t align_drift, size_t size_of_type)
+{
+	return (size_of_type - align_drift) & (size_of_type - 1);
+}
+/* Default arch alignment */
+#define LTT_ALIGN
+
+static inline int ltt_get_alignment(void)
+{
+	return sizeof(void *);
+}
+
+extern unsigned int ltt_fmt_largest_align(size_t align_drift, const char *fmt);
+
+#else
+
+static inline unsigned int ltt_align(size_t align_drift,
+		 size_t size_of_type)
+{
+	return 0;
+}
+
+#define LTT_ALIGN __attribute__((packed))
+
+static inline int ltt_get_alignment(void)
+{
+	return 0;
+}
+
+static inline unsigned int ltt_fmt_largest_align(size_t align_drift,
+		const char *fmt)
+{
+	return 0;
+}
+
+#endif /* defined(CONFIG_LTT) && defined(CONFIG_LTT_ALIGNMENT) */
+
+#endif /* LTT_CORE_H */
diff --git a/stblinux-2.6.31/include/linux/ltt-relay.h b/stblinux-2.6.31/include/linux/ltt-relay.h
new file mode 100644
index 0000000..cdef02b
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/ltt-relay.h
@@ -0,0 +1,304 @@
+ /*
+ * include/linux/ltt-relay.h
+ *
+ * Copyright (C) 2008,2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ *
+ * Credits to Steven Rostedt for proposing to use an extra-subbuffer owned by
+ * the reader in flight recorder mode.
+ */
+
+#ifndef _LINUX_LTT_RELAY_H
+#define _LINUX_LTT_RELAY_H
+
+#include <linux/types.h>
+#include <linux/sched.h>
+#include <linux/timer.h>
+#include <linux/wait.h>
+#include <linux/fs.h>
+#include <linux/poll.h>
+#include <linux/kref.h>
+#include <linux/mm.h>
+#include <linux/ltt-core.h>
+
+/* Use lowest pointer bit to show the sub-buffer has no reference. */
+#define RCHAN_NOREF_FLAG	0x1UL
+
+#define RCHAN_SB_IS_NOREF(x)	((unsigned long)(x) & RCHAN_NOREF_FLAG)
+#define RCHAN_SB_SET_NOREF(x)	\
+	(x = (struct chanbuf_page *)((unsigned long)(x) | RCHAN_NOREF_FLAG))
+#define RCHAN_SB_CLEAR_NOREF(x)	\
+	(x = (struct chanbuf_page *)((unsigned long)(x) & ~RCHAN_NOREF_FLAG))
+
+struct ltt_trace;
+
+struct chanbuf_page {
+	void *virt;			/* page virtual address (cached) */
+	struct page *page;		/* pointer to page structure */
+};
+
+struct chanbuf_sb {
+	struct chanbuf_page *pages;	/* Pointer to rchan pages for subbuf */
+};
+
+struct ltt_chanbuf_alloc {
+	struct chanbuf_sb *buf_wsb;	/* Array of rchan_sb for writer */
+	struct chanbuf_sb buf_rsb;	/* chanbuf_sb for reader */
+	void **_virt;			/* Array of pointers to page addr */
+	struct page **_pages;		/* Array of pointers to pages */
+	struct dentry *dentry;		/* Associated file dentry */
+	unsigned int nr_pages;		/* Number pages in buffer */
+
+	struct ltt_chan_alloc *chan;	/* Associated channel */
+	unsigned int cpu;		/* This buffer's cpu */
+	unsigned int allocated:1;	/* Bool: is buffer allocated ? */
+};
+
+/*
+ * Forward declaration of locking-specific per-cpu buffer structure.
+ */
+struct ltt_chanbuf;
+
+struct ltt_chan_alloc {
+	unsigned long buf_size;		/* Size of the buffer */
+	unsigned long sb_size;		/* Sub-buffer size */
+	unsigned int sb_size_order;	/* Order of sub-buffer size */
+	unsigned int n_sb_order;	/* Number of sub-buffers per buffer */
+	int extra_reader_sb:1;		/* Bool: has extra reader subbuffer */
+	struct ltt_chanbuf *buf;	/* Channel per-cpu buffers */
+
+	struct kref kref;		/* Reference count */
+	unsigned long n_sb;		/* Number of sub-buffers */
+	struct dentry *parent;		/* Associated parent dentry */
+	struct dentry *ascii_dentry;	/* Text output dentry */
+	struct ltt_trace *trace;	/* Associated trace */
+	char filename[NAME_MAX];	/* Filename for channel files */
+};
+
+int ltt_chanbuf_alloc_create(struct ltt_chanbuf_alloc *buf,
+			     struct ltt_chan_alloc *chan, int cpu);
+void ltt_chanbuf_alloc_free(struct ltt_chanbuf_alloc *buf);
+int ltt_chan_alloc_init(struct ltt_chan_alloc *chan, struct ltt_trace *trace,
+			const char *base_filename,
+			struct dentry *parent, size_t sb_size,
+			size_t n_sb, int extra_reader_sb, int overwrite);
+void ltt_chan_alloc_free(struct ltt_chan_alloc *chan);
+int ltt_chanbuf_create_file(const char *filename, struct dentry *parent,
+			    int mode, struct ltt_chanbuf *buf);
+int ltt_chanbuf_remove_file(struct ltt_chanbuf *buf);
+
+void ltt_chan_for_each_channel(void (*cb) (struct ltt_chanbuf *buf), int cpu);
+
+extern void _ltt_relay_write(struct ltt_chanbuf_alloc *bufa,
+			     size_t offset, const void *src, size_t len,
+			     ssize_t pagecpy);
+
+extern int ltt_relay_read(struct ltt_chanbuf_alloc *bufa,
+			  size_t offset, void *dest, size_t len);
+
+extern int ltt_relay_read_cstr(struct ltt_chanbuf_alloc *bufa,
+			       size_t offset, void *dest, size_t len);
+
+extern struct page *ltt_relay_read_get_page(struct ltt_chanbuf_alloc *bufa,
+					    size_t offset);
+
+/*
+ * Return the address where a given offset is located.
+ * Should be used to get the current subbuffer header pointer. Given we know
+ * it's never on a page boundary, it's safe to write directly to this address,
+ * as long as the write is never bigger than a page size.
+ */
+extern void *ltt_relay_offset_address(struct ltt_chanbuf_alloc *bufa,
+				      size_t offset);
+extern void *ltt_relay_read_offset_address(struct ltt_chanbuf_alloc *bufa,
+					   size_t offset);
+
+#ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
+static __inline__
+void ltt_relay_do_copy(void *dest, const void *src, size_t len)
+{
+	switch (len) {
+	case 0:
+		break;
+	case 1:
+		*(u8 *)dest = *(const u8 *)src;
+		break;
+	case 2:
+		*(u16 *)dest = *(const u16 *)src;
+		break;
+	case 4:
+		*(u32 *)dest = *(const u32 *)src;
+		break;
+	case 8:
+		*(u64 *)dest = *(const u64 *)src;
+		break;
+	default:
+		/*
+		 * What we really want here is an __inline__ memcpy, but we don't
+		 * have constants, so gcc generally uses a function call.
+		 */
+		for (; len > 0; len--)
+			*(u8 *)dest++ = *(const u8 *)src++;
+	}
+}
+#else
+/*
+ * Returns whether the dest and src addresses are aligned on
+ * min(sizeof(void *), len). Call this with statically known len for efficiency.
+ */
+static __inline__
+int addr_aligned(const void *dest, const void *src, size_t len)
+{
+	if (ltt_align((size_t)dest, len))
+		return 0;
+	if (ltt_align((size_t)src, len))
+		return 0;
+	return 1;
+}
+
+static __inline__
+void ltt_relay_do_copy(void *dest, const void *src, size_t len)
+{
+	switch (len) {
+	case 0:
+		break;
+	case 1:
+		*(u8 *)dest = *(const u8 *)src;
+		break;
+	case 2:
+		if (unlikely(!addr_aligned(dest, src, 2)))
+			goto memcpy_fallback;
+		*(u16 *)dest = *(const u16 *)src;
+		break;
+	case 4:
+		if (unlikely(!addr_aligned(dest, src, 4)))
+			goto memcpy_fallback;
+		*(u32 *)dest = *(const u32 *)src;
+		break;
+	case 8:
+		if (unlikely(!addr_aligned(dest, src, 8)))
+			goto memcpy_fallback;
+		*(u64 *)dest = *(const u64 *)src;
+		break;
+	default:
+		goto memcpy_fallback;
+	}
+	return;
+
+memcpy_fallback:
+	/*
+	 * What we really want here is an inline memcpy, but we don't
+	 * have constants, so gcc generally uses a function call.
+	 */
+	for (; len > 0; len--)
+		*(u8 *)dest++ = *(const u8 *)src++;
+}
+#endif
+
+static __inline__
+int ltt_relay_write(struct ltt_chanbuf_alloc *bufa,
+		    struct ltt_chan_alloc *chana, size_t offset,
+		    const void *src, size_t len)
+{
+	size_t sbidx, index;
+	ssize_t pagecpy;
+	struct chanbuf_page *rpages;
+
+	offset &= chana->buf_size - 1;
+	sbidx = offset >> chana->sb_size_order;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+	pagecpy = min_t(size_t, len, (- offset) & ~PAGE_MASK);
+	rpages = bufa->buf_wsb[sbidx].pages;
+	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
+	ltt_relay_do_copy(rpages[index].virt + (offset & ~PAGE_MASK),
+			  src, pagecpy);
+
+	if (unlikely(len != pagecpy))
+		_ltt_relay_write(bufa, offset, src, len, pagecpy);
+	return len;
+}
+
+/**
+ * ltt_clear_noref_flag - Clear the noref subbuffer flag, for writer.
+ */
+static __inline__
+void ltt_clear_noref_flag(struct ltt_chanbuf_alloc *bufa, long idx)
+{
+	struct chanbuf_page *sb_pages, *new_sb_pages;
+
+	sb_pages = bufa->buf_wsb[idx].pages;
+	for (;;) {
+		if (!RCHAN_SB_IS_NOREF(sb_pages))
+			return;	/* Already writing to this buffer */
+		new_sb_pages = sb_pages;
+		RCHAN_SB_CLEAR_NOREF(new_sb_pages);
+		new_sb_pages = cmpxchg(&bufa->buf_wsb[idx].pages,
+			sb_pages, new_sb_pages);
+		if (likely(new_sb_pages == sb_pages))
+			break;
+		sb_pages = new_sb_pages;
+	}
+}
+
+/**
+ * ltt_set_noref_flag - Set the noref subbuffer flag, for writer.
+ */
+static __inline__
+void ltt_set_noref_flag(struct ltt_chanbuf_alloc *bufa, long idx)
+{
+	struct chanbuf_page *sb_pages, *new_sb_pages;
+
+	sb_pages = bufa->buf_wsb[idx].pages;
+	for (;;) {
+		if (RCHAN_SB_IS_NOREF(sb_pages))
+			return;	/* Already set */
+		new_sb_pages = sb_pages;
+		RCHAN_SB_SET_NOREF(new_sb_pages);
+		new_sb_pages = cmpxchg(&bufa->buf_wsb[idx].pages,
+			sb_pages, new_sb_pages);
+		if (likely(new_sb_pages == sb_pages))
+			break;
+		sb_pages = new_sb_pages;
+	}
+}
+
+/**
+ * update_read_sb_index - Read-side subbuffer index update.
+ */
+static __inline__
+int update_read_sb_index(struct ltt_chanbuf_alloc *bufa,
+			 struct ltt_chan_alloc *chana,
+			 long consumed_idx)
+{
+	struct chanbuf_page *old_wpage, *new_wpage;
+
+	if (unlikely(chana->extra_reader_sb)) {
+		/*
+		 * Exchange the target writer subbuffer with our own unused
+		 * subbuffer.
+		 */
+		old_wpage = bufa->buf_wsb[consumed_idx].pages;
+		if (unlikely(!RCHAN_SB_IS_NOREF(old_wpage)))
+			return -EAGAIN;
+		WARN_ON_ONCE(!RCHAN_SB_IS_NOREF(bufa->buf_rsb.pages));
+		new_wpage = cmpxchg(&bufa->buf_wsb[consumed_idx].pages,
+				old_wpage,
+				bufa->buf_rsb.pages);
+		if (unlikely(old_wpage != new_wpage))
+			return -EAGAIN;
+		bufa->buf_rsb.pages = new_wpage;
+		RCHAN_SB_CLEAR_NOREF(bufa->buf_rsb.pages);
+	} else {
+		/* No page exchange, use the writer page directly */
+		bufa->buf_rsb.pages = bufa->buf_wsb[consumed_idx].pages;
+		RCHAN_SB_CLEAR_NOREF(bufa->buf_rsb.pages);
+	}
+	return 0;
+}
+
+ssize_t ltt_relay_file_splice_read(struct file *in, loff_t *ppos,
+				   struct pipe_inode_info *pipe, size_t len,
+				   unsigned int flags);
+
+#endif /* _LINUX_LTT_RELAY_H */
diff --git a/stblinux-2.6.31/include/linux/ltt-tracer.h b/stblinux-2.6.31/include/linux/ltt-tracer.h
new file mode 100644
index 0000000..e141b85
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/ltt-tracer.h
@@ -0,0 +1,718 @@
+/*
+ * Copyright (C) 2005,2006,2008 Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * This contains the definitions for the Linux Trace Toolkit tracer.
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#ifndef _LTT_TRACER_H
+#define _LTT_TRACER_H
+
+#include <stdarg.h>
+#include <linux/types.h>
+#include <linux/limits.h>
+#include <linux/list.h>
+#include <linux/cache.h>
+#include <linux/kernel.h>
+#include <linux/timex.h>
+#include <linux/wait.h>
+#include <linux/ltt-relay.h>
+#include <linux/ltt-channels.h>
+#include <linux/ltt-core.h>
+#include <linux/marker.h>
+#include <linux/trace-clock.h>
+#include <asm/atomic.h>
+#include <asm/local.h>
+
+/* Number of bytes to log with a read/write event */
+#define LTT_LOG_RW_SIZE			32L
+
+/* Interval (in jiffies) at which the LTT per-CPU timer fires */
+#define LTT_PERCPU_TIMER_INTERVAL	1
+
+#ifndef LTT_ARCH_TYPE
+#define LTT_ARCH_TYPE			LTT_ARCH_TYPE_UNDEFINED
+#endif
+
+#ifndef LTT_ARCH_VARIANT
+#define LTT_ARCH_VARIANT		LTT_ARCH_VARIANT_NONE
+#endif
+
+struct ltt_active_marker;
+
+/* Maximum number of callbacks per marker */
+#define LTT_NR_CALLBACKS	10
+
+struct ltt_serialize_closure;
+struct ltt_probe_private_data;
+
+/* Serialization callback '%k' */
+typedef size_t (*ltt_serialize_cb)(struct ltt_chanbuf *buf, size_t buf_offset,
+				   struct ltt_serialize_closure *closure,
+				   void *serialize_private, int *largest_align,
+				   const char *fmt, va_list *args);
+
+struct ltt_serialize_closure {
+	ltt_serialize_cb *callbacks;
+	long cb_args[LTT_NR_CALLBACKS];
+	unsigned int cb_idx;
+};
+
+size_t ltt_serialize_data(struct ltt_chanbuf *buf, size_t buf_offset,
+			  struct ltt_serialize_closure *closure,
+			  void *serialize_private, int *largest_align,
+			  const char *fmt, va_list *args);
+
+struct ltt_available_probe {
+	const char *name;		/* probe name */
+	const char *format;
+	marker_probe_func *probe_func;
+	ltt_serialize_cb callbacks[LTT_NR_CALLBACKS];
+	struct list_head node;		/* registered probes list */
+};
+
+struct ltt_probe_private_data {
+	struct ltt_trace *trace;	/*
+					 * Target trace, for metadata
+					 * or statedump.
+					 */
+	ltt_serialize_cb serializer;	/*
+					 * Serialization function override.
+					 */
+	void *serialize_private;	/*
+					 * Private data for serialization
+					 * functions.
+					 */
+};
+
+enum ltt_channels {
+	LTT_CHANNEL_METADATA,
+	LTT_CHANNEL_FD_STATE,
+	LTT_CHANNEL_GLOBAL_STATE,
+	LTT_CHANNEL_IRQ_STATE,
+	LTT_CHANNEL_MODULE_STATE,
+	LTT_CHANNEL_NETIF_STATE,
+	LTT_CHANNEL_SOFTIRQ_STATE,
+	LTT_CHANNEL_SWAP_STATE,
+	LTT_CHANNEL_SYSCALL_STATE,
+	LTT_CHANNEL_TASK_STATE,
+	LTT_CHANNEL_VM_STATE,
+	LTT_CHANNEL_FS,
+	LTT_CHANNEL_INPUT,
+	LTT_CHANNEL_IPC,
+	LTT_CHANNEL_KERNEL,
+	LTT_CHANNEL_MM,
+	LTT_CHANNEL_RCU,
+	LTT_CHANNEL_DEFAULT,
+};
+
+struct ltt_active_marker {
+	struct list_head node;		/* active markers list */
+	const char *channel;
+	const char *name;
+	const char *format;
+	struct ltt_available_probe *probe;
+};
+
+extern void ltt_vtrace(const struct marker *mdata, void *probe_data,
+		       void *call_data, const char *fmt, va_list *args);
+extern void ltt_trace(const struct marker *mdata, void *probe_data,
+		      void *call_data, const char *fmt, ...);
+
+size_t ltt_serialize_printf(struct ltt_chanbuf *buf, unsigned long buf_offset,
+			    size_t *msg_size, char *output, size_t outlen,
+			    const char *fmt);
+
+/*
+ * Unique ID assigned to each registered probe.
+ */
+enum marker_id {
+	MARKER_ID_SET_MARKER_ID = 0,	/* Static IDs available (range 0-7) */
+	MARKER_ID_SET_MARKER_FORMAT,
+	MARKER_ID_COMPACT,		/* Compact IDs (range: 8-127)	    */
+	MARKER_ID_DYNAMIC,		/* Dynamic IDs (range: 128-65535)   */
+};
+
+/* static ids 0-1 reserved for internal use. */
+#define MARKER_CORE_IDS		2
+static __inline__ enum marker_id marker_id_type(uint16_t id)
+{
+	if (id < MARKER_CORE_IDS)
+		return (enum marker_id)id;
+	else
+		return MARKER_ID_DYNAMIC;
+}
+
+#ifdef CONFIG_LTT
+
+struct user_dbg_data {
+	unsigned long avail_size;
+	unsigned long write;
+	unsigned long read;
+};
+
+struct ltt_trace_ops {
+	/* First 32 bytes cache-hot cacheline */
+	void (*wakeup_channel) (struct ltt_chan *chan);
+	int (*user_blocking) (struct ltt_trace *trace, unsigned int index,
+			      size_t data_size, struct user_dbg_data *dbg);
+	/* End of first 32 bytes cacheline */
+	int (*create_dirs) (struct ltt_trace *new_trace);
+	void (*remove_dirs) (struct ltt_trace *new_trace);
+	int (*create_channel) (const char *channel_name, struct ltt_chan *chan,
+			       struct dentry *parent, size_t sb_size,
+			       size_t n_sb, int overwrite,
+			       struct ltt_trace *trace);
+	void (*finish_channel) (struct ltt_chan *chan);
+	void (*remove_channel) (struct kref *kref);
+	void (*user_errors) (struct ltt_trace *trace, unsigned int index,
+			     size_t data_size, struct user_dbg_data *dbg,
+			     int cpu);
+	void (*start_switch_timer) (struct ltt_chan *chan);
+	void (*stop_switch_timer) (struct ltt_chan *chan);
+#ifdef CONFIG_HOTPLUG_CPU
+	int (*handle_cpuhp) (struct notifier_block *nb, unsigned long action,
+			     void *hcpu, struct ltt_trace *trace);
+#endif
+};
+
+struct ltt_transport {
+	char *name;
+	struct module *owner;
+	struct list_head node;
+	struct ltt_trace_ops ops;
+};
+
+enum trace_mode { LTT_TRACE_NORMAL, LTT_TRACE_FLIGHT, LTT_TRACE_HYBRID };
+
+#define CHANNEL_FLAG_ENABLE	(1U<<0)
+#define CHANNEL_FLAG_OVERWRITE	(1U<<1)
+
+/* Per-trace information - each trace/flight recorder represented by one */
+struct ltt_trace {
+	/* First 32 bytes cache-hot cacheline */
+	struct list_head list;
+	struct ltt_chan *channels;
+	unsigned int nr_channels;
+	int active;
+	/* Second 32 bytes cache-hot cacheline */
+	struct ltt_trace_ops *ops;
+	u32 freq_scale;
+	u64 start_freq;
+	u64 start_tsc;
+	unsigned long long start_monotonic;
+	struct timeval		start_time;
+	struct ltt_channel_setting *settings;
+	struct {
+		struct dentry			*trace_root;
+		struct dentry			*ascii_root;
+	} dentry;
+	struct kref kref; /* Each channel has a kref of the trace struct */
+	struct ltt_transport *transport;
+	struct kref ltt_transport_kref;
+	wait_queue_head_t kref_wq; /* Place for ltt_trace_destroy to sleep */
+	char trace_name[NAME_MAX];
+} ____cacheline_aligned;
+
+/* Hardcoded event headers
+ *
+ * event header for a trace with active heartbeat : 27 bits timestamps
+ *
+ * headers are 32-bits aligned. In order to insure such alignment, a dynamic per
+ * trace alignment value must be done.
+ *
+ * Remember that the C compiler does align each member on the boundary
+ * equivalent to their own size.
+ *
+ * As relay subbuffers are aligned on pages, we are sure that they are 4 and 8
+ * bytes aligned, so the buffer header and trace header are aligned.
+ *
+ * Event headers are aligned depending on the trace alignment option.
+ *
+ * Note using C structure bitfields for cross-endianness and portability
+ * concerns.
+ */
+
+#define LTT_RESERVED_EVENTS	3
+#define LTT_EVENT_BITS		5
+#define LTT_FREE_EVENTS		((1 << LTT_EVENT_BITS) - LTT_RESERVED_EVENTS)
+#define LTT_TSC_BITS		27
+#define LTT_TSC_MASK		((1 << LTT_TSC_BITS) - 1)
+
+struct ltt_event_header {
+	u32 id_time;		/* 5 bits event id (MSB); 27 bits time (LSB) */
+};
+
+/* Reservation flags */
+#define	LTT_RFLAG_ID			(1 << 0)
+#define	LTT_RFLAG_ID_SIZE		(1 << 1)
+#define	LTT_RFLAG_ID_SIZE_TSC		(1 << 2)
+
+#define LTT_MAX_SMALL_SIZE		0xFFFFU
+
+/*
+ * We use asm/timex.h : cpu_khz/HZ variable in here : we might have to deal
+ * specifically with CPU frequency scaling someday, so using an interpolation
+ * between the start and end of buffer values is not flexible enough. Using an
+ * immediate frequency value permits to calculate directly the times for parts
+ * of a buffer that would be before a frequency change.
+ *
+ * Keep the natural field alignment for _each field_ within this structure if
+ * you ever add/remove a field from this header. Packed attribute is not used
+ * because gcc generates poor code on at least powerpc and mips. Don't ever
+ * let gcc add padding between the structure elements.
+ */
+struct ltt_subbuffer_header {
+	uint64_t cycle_count_begin;	/* Cycle count at subbuffer start */
+	uint64_t cycle_count_end;	/* Cycle count at subbuffer end */
+	uint32_t magic_number;		/*
+					 * Trace magic number.
+					 * contains endianness information.
+					 */
+	uint8_t major_version;
+	uint8_t minor_version;
+	uint8_t arch_size;		/* Architecture pointer size */
+	uint8_t alignment;		/* LTT data alignment */
+	uint64_t start_time_sec;	/* NTP-corrected start time */
+	uint64_t start_time_usec;
+	uint64_t start_freq;		/*
+					 * Frequency at trace start,
+					 * used all along the trace.
+					 */
+	uint32_t freq_scale;		/* Frequency scaling (divisor) */
+	uint32_t data_size;		/* Size of data in subbuffer */
+	uint32_t sb_size;		/* Subbuffer size (include padding) */
+	uint32_t events_lost;		/*
+					 * Events lost in this subbuffer since
+					 * the beginning of the trace.
+					 * (may overflow)
+					 */
+	uint32_t subbuf_corrupt;	/*
+					 * Corrupted (lost) subbuffers since
+					 * the begginig of the trace.
+					 * (may overflow)
+					 */
+	uint8_t header_end[0];		/* End of header */
+};
+
+/**
+ * ltt_sb_header_size - called on buffer-switch to a new sub-buffer
+ *
+ * Return header size without padding after the structure. Don't use packed
+ * structure because gcc generates inefficient code on some architectures
+ * (powerpc, mips..)
+ */
+static __inline__ size_t ltt_sb_header_size(void)
+{
+	return offsetof(struct ltt_subbuffer_header, header_end);
+}
+
+/*
+ * ltt_get_header_size
+ *
+ * Calculate alignment offset to 32-bits. This is the alignment offset of the
+ * event header.
+ *
+ * Important note :
+ * The event header must be 32-bits. The total offset calculated here :
+ *
+ * Alignment of header struct on 32 bits (min arch size, header size)
+ * + sizeof(header struct)  (32-bits)
+ * + (opt) u16 (ext. event id)
+ * + (opt) u16 (event_size)
+ *             (if event_size == LTT_MAX_SMALL_SIZE, has ext. event size)
+ * + (opt) u32 (ext. event size)
+ * + (opt) u64 full TSC (aligned on min(64-bits, arch size))
+ *
+ * The payload must itself determine its own alignment from the biggest type it
+ * contains.
+ * */
+static __inline__
+unsigned char ltt_get_header_size(struct ltt_chan *chan, size_t offset,
+				  size_t data_size, size_t *before_hdr_pad,
+				  unsigned int rflags)
+{
+	size_t orig_offset = offset;
+	size_t padding;
+
+	BUILD_BUG_ON(sizeof(struct ltt_event_header) != sizeof(u32));
+
+	padding = ltt_align(offset, sizeof(struct ltt_event_header));
+	offset += padding;
+	offset += sizeof(struct ltt_event_header);
+
+	if (unlikely(rflags)) {
+		switch (rflags) {
+		case LTT_RFLAG_ID_SIZE_TSC:
+			offset += sizeof(u16) + sizeof(u16);
+			if (data_size >= LTT_MAX_SMALL_SIZE)
+				offset += sizeof(u32);
+			offset += ltt_align(offset, sizeof(u64));
+			offset += sizeof(u64);
+			break;
+		case LTT_RFLAG_ID_SIZE:
+			offset += sizeof(u16) + sizeof(u16);
+			if (data_size >= LTT_MAX_SMALL_SIZE)
+				offset += sizeof(u32);
+			break;
+		case LTT_RFLAG_ID:
+			offset += sizeof(u16);
+			break;
+		}
+	}
+
+	*before_hdr_pad = padding;
+	return offset - orig_offset;
+}
+
+extern
+size_t ltt_write_event_header_slow(struct ltt_chanbuf_alloc *bufa,
+				   struct ltt_chan_alloc *chana,
+				   long buf_offset, u16 eID, u32 event_size,
+				   u64 tsc, unsigned int rflags);
+
+/*
+ * ltt_write_event_header
+ *
+ * Writes the event header to the offset (already aligned on 32-bits).
+ *
+ * @buf : buffer to write to.
+ * @chan : pointer to the channel structure..
+ * @buf_offset : buffer offset to write to (aligned on 32 bits).
+ * @eID : event ID
+ * @event_size : size of the event, excluding the event header.
+ * @tsc : time stamp counter.
+ * @rflags : reservation flags.
+ *
+ * returns : offset where the event data must be written.
+ */
+static __inline__
+size_t ltt_write_event_header(struct ltt_chanbuf_alloc *bufa,
+			      struct ltt_chan_alloc *chana,
+			      long buf_offset, u16 eID, u32 event_size, u64 tsc,
+			      unsigned int rflags)
+{
+	struct ltt_event_header header;
+
+	if (unlikely(rflags))
+		goto slow_path;
+
+	header.id_time = eID << LTT_TSC_BITS;
+	header.id_time |= (u32)tsc & LTT_TSC_MASK;
+	ltt_relay_write(bufa, chana, buf_offset, &header, sizeof(header));
+	buf_offset += sizeof(header);
+
+	return buf_offset;
+
+slow_path:
+	return ltt_write_event_header_slow(bufa, chana, buf_offset,
+					   eID, event_size, tsc, rflags);
+}
+
+/*
+ * ltt_read_event_header
+ * buf_offset must aligned on 32 bits
+ */
+static __inline__
+size_t ltt_read_event_header(struct ltt_chanbuf_alloc *bufa, long buf_offset,
+			     u64 *tsc, u32 *event_size, u16 *eID,
+			     unsigned int *rflags)
+{
+	struct ltt_event_header header;
+	u16 small_size;
+
+	ltt_relay_read(bufa, buf_offset, &header, sizeof(header));
+	buf_offset += sizeof(header);
+
+	*event_size = INT_MAX;
+	*eID = header.id_time >> LTT_TSC_BITS;
+	*tsc = header.id_time & LTT_TSC_MASK;
+
+	switch (*eID) {
+	case 29:
+		*rflags = LTT_RFLAG_ID_SIZE_TSC;
+		ltt_relay_read(bufa, buf_offset, eID, sizeof(u16));
+		buf_offset += sizeof(u16);
+		ltt_relay_read(bufa, buf_offset, &small_size, sizeof(u16));
+		buf_offset += sizeof(u16);
+		if (small_size == LTT_MAX_SMALL_SIZE) {
+			ltt_relay_read(bufa, buf_offset, event_size,
+					sizeof(u32));
+			buf_offset += sizeof(u32);
+		} else
+			*event_size = small_size;
+		buf_offset += ltt_align(buf_offset, sizeof(u64));
+		ltt_relay_read(bufa, buf_offset, tsc, sizeof(u64));
+		buf_offset += sizeof(u64);
+		break;
+	case 30:
+		*rflags = LTT_RFLAG_ID_SIZE;
+		ltt_relay_read(bufa, buf_offset, eID, sizeof(u16));
+		buf_offset += sizeof(u16);
+		ltt_relay_read(bufa, buf_offset, &small_size, sizeof(u16));
+		buf_offset += sizeof(u16);
+		if (small_size == LTT_MAX_SMALL_SIZE) {
+			ltt_relay_read(bufa, buf_offset, event_size,
+					sizeof(u32));
+			buf_offset += sizeof(u32);
+		} else
+			*event_size = small_size;
+		break;
+	case 31:
+		*rflags = LTT_RFLAG_ID;
+		ltt_relay_read(bufa, buf_offset, eID, sizeof(u16));
+		buf_offset += sizeof(u16);
+		break;
+	default:
+		*rflags = 0;
+		break;
+	}
+
+	return buf_offset;
+}
+
+/* Lockless LTTng */
+
+/* Buffer offset macros */
+
+/*
+ * BUFFER_TRUNC zeroes the subbuffer offset and the subbuffer number parts of
+ * the offset, which leaves only the buffer number.
+ */
+#define BUFFER_TRUNC(offset, chan) \
+	((offset) & (~((chan)->a.buf_size - 1)))
+#define BUFFER_OFFSET(offset, chan) ((offset) & ((chan)->a.buf_size - 1))
+#define SUBBUF_OFFSET(offset, chan) ((offset) & ((chan)->a.sb_size - 1))
+#define SUBBUF_ALIGN(offset, chan) \
+	(((offset) + (chan)->a.sb_size) & (~((chan)->a.sb_size - 1)))
+#define SUBBUF_TRUNC(offset, chan) \
+	((offset) & (~((chan)->a.sb_size - 1)))
+#define SUBBUF_INDEX(offset, chan) \
+	(BUFFER_OFFSET((offset), chan) >> (chan)->a.sb_size_order)
+
+/*
+ * Control channels :
+ * control/metadata
+ * control/interrupts
+ * control/...
+ *
+ * cpu channel :
+ * cpu
+ */
+#define LTT_RELAY_ROOT			"ltt"
+#define LTT_RELAY_LOCKED_ROOT		"ltt-locked"
+
+#define LTT_METADATA_CHANNEL		"metadata_state"
+#define LTT_FD_STATE_CHANNEL		"fd_state"
+#define LTT_GLOBAL_STATE_CHANNEL	"global_state"
+#define LTT_IRQ_STATE_CHANNEL		"irq_state"
+#define LTT_MODULE_STATE_CHANNEL	"module_state"
+#define LTT_NETIF_STATE_CHANNEL		"netif_state"
+#define LTT_SOFTIRQ_STATE_CHANNEL	"softirq_state"
+#define LTT_SWAP_STATE_CHANNEL		"swap_state"
+#define LTT_SYSCALL_STATE_CHANNEL	"syscall_state"
+#define LTT_TASK_STATE_CHANNEL		"task_state"
+#define LTT_VM_STATE_CHANNEL		"vm_state"
+#define LTT_FS_CHANNEL			"fs"
+#define LTT_INPUT_CHANNEL		"input"
+#define LTT_IPC_CHANNEL			"ipc"
+#define LTT_KERNEL_CHANNEL		"kernel"
+#define LTT_MM_CHANNEL			"mm"
+#define LTT_RCU_CHANNEL			"rcu"
+
+#define LTT_FLIGHT_PREFIX		"flight-"
+
+#define LTT_ASCII			"ascii"
+
+/* Tracer properties */
+#define LTT_DEFAULT_SUBBUF_SIZE_LOW	65536
+#define LTT_DEFAULT_N_SUBBUFS_LOW	2
+#define LTT_DEFAULT_SUBBUF_SIZE_MED	262144
+#define LTT_DEFAULT_N_SUBBUFS_MED	2
+#define LTT_DEFAULT_SUBBUF_SIZE_HIGH	1048576
+#define LTT_DEFAULT_N_SUBBUFS_HIGH	2
+#define LTT_TRACER_MAGIC_NUMBER		0x00D6B7ED
+#define LTT_TRACER_VERSION_MAJOR	2
+#define LTT_TRACER_VERSION_MINOR	5
+
+/**
+ * ltt_write_trace_header - Write trace header
+ * @trace: Trace information
+ * @header: Memory address where the information must be written to
+ */
+static __inline__
+void ltt_write_trace_header(struct ltt_trace *trace,
+			    struct ltt_subbuffer_header *header)
+{
+	header->magic_number = LTT_TRACER_MAGIC_NUMBER;
+	header->major_version = LTT_TRACER_VERSION_MAJOR;
+	header->minor_version = LTT_TRACER_VERSION_MINOR;
+	header->arch_size = sizeof(void *);
+	header->alignment = ltt_get_alignment();
+	header->start_time_sec = trace->start_time.tv_sec;
+	header->start_time_usec = trace->start_time.tv_usec;
+	header->start_freq = trace->start_freq;
+	header->freq_scale = trace->freq_scale;
+}
+
+/*
+ * Size reserved for high priority events (interrupts, NMI, BH) at the end of a
+ * nearly full buffer. User space won't use this last amount of space when in
+ * blocking mode. This space also includes the event header that would be
+ * written by this user space event.
+ */
+#define LTT_RESERVE_CRITICAL		4096
+
+/* Register and unregister function pointers */
+
+enum ltt_module_function {
+	LTT_FUNCTION_RUN_FILTER,
+	LTT_FUNCTION_FILTER_CONTROL,
+	LTT_FUNCTION_STATEDUMP
+};
+
+extern int ltt_module_register(enum ltt_module_function name, void *function,
+			       struct module *owner);
+extern void ltt_module_unregister(enum ltt_module_function name);
+
+void ltt_transport_register(struct ltt_transport *transport);
+void ltt_transport_unregister(struct ltt_transport *transport);
+
+/* Exported control function */
+
+enum ltt_control_msg {
+	LTT_CONTROL_START,
+	LTT_CONTROL_STOP,
+	LTT_CONTROL_CREATE_TRACE,
+	LTT_CONTROL_DESTROY_TRACE
+};
+
+union ltt_control_args {
+	struct {
+		enum trace_mode mode;
+		unsigned int subbuf_size_low;
+		unsigned int n_subbufs_low;
+		unsigned int subbuf_size_med;
+		unsigned int n_subbufs_med;
+		unsigned int subbuf_size_high;
+		unsigned int n_subbufs_high;
+	} new_trace;
+};
+
+int _ltt_trace_setup(const char *trace_name);
+int ltt_trace_setup(const char *trace_name);
+struct ltt_trace *_ltt_trace_find_setup(const char *trace_name);
+int ltt_trace_set_type(const char *trace_name, const char *trace_type);
+int ltt_trace_set_channel_subbufsize(const char *trace_name,
+				     const char *channel_name,
+				     unsigned int size);
+int ltt_trace_set_channel_subbufcount(const char *trace_name,
+				      const char *channel_name,
+				      unsigned int cnt);
+int ltt_trace_set_channel_switch_timer(const char *trace_name,
+				       const char *channel_name,
+				       unsigned long interval);
+int ltt_trace_set_channel_enable(const char *trace_name,
+				 const char *channel_name,
+				 unsigned int enable);
+int ltt_trace_set_channel_overwrite(const char *trace_name,
+				    const char *channel_name,
+				    unsigned int overwrite);
+int ltt_trace_alloc(const char *trace_name);
+int ltt_trace_destroy(const char *trace_name);
+int ltt_trace_start(const char *trace_name);
+int ltt_trace_stop(const char *trace_name);
+
+extern int ltt_control(enum ltt_control_msg msg, const char *trace_name,
+		       const char *trace_type, union ltt_control_args args);
+
+enum ltt_filter_control_msg {
+	LTT_FILTER_DEFAULT_ACCEPT,
+	LTT_FILTER_DEFAULT_REJECT
+};
+
+extern int ltt_filter_control(enum ltt_filter_control_msg msg,
+			      const char *trace_name);
+
+extern struct dentry *get_filter_root(void);
+
+void ltt_core_register(int (*function)(u8, void *));
+
+void ltt_core_unregister(void);
+
+void ltt_release_trace(struct kref *kref);
+void ltt_release_transport(struct kref *kref);
+
+extern int ltt_probe_register(struct ltt_available_probe *pdata);
+extern int ltt_probe_unregister(struct ltt_available_probe *pdata);
+extern int ltt_marker_connect(const char *channel, const char *mname,
+			      const char *pname);
+extern int ltt_marker_disconnect(const char *channel, const char *mname,
+				 const char *pname);
+extern void ltt_dump_marker_state(struct ltt_trace *trace);
+
+void ltt_lock_traces(void);
+void ltt_unlock_traces(void);
+
+#ifdef CONFIG_LTT_ASCII
+extern int ltt_ascii_create_dir(struct ltt_trace *new_trace);
+extern void ltt_ascii_remove_dir(struct ltt_trace *trace);
+extern int ltt_ascii_create(struct ltt_chan *chan);
+extern void ltt_ascii_remove(struct ltt_chan *chan);
+#else
+static inline int ltt_ascii_create_dir(struct ltt_trace *new_trace)
+{
+	return 0;
+}
+
+static inline void ltt_ascii_remove_dir(struct ltt_trace *trace)
+{
+}
+
+static inline int ltt_ascii_create(struct ltt_chan *chan)
+{
+	return 0;
+}
+
+static inline void ltt_ascii_remove(struct ltt_chan *chan)
+{
+}
+#endif
+
+extern
+void ltt_statedump_register_kprobes_dump(void (*callback)(void *call_data));
+extern
+void ltt_statedump_unregister_kprobes_dump(void (*callback)(void *call_data));
+
+extern void ltt_dump_softirq_vec(void *call_data);
+
+#ifdef CONFIG_HAVE_LTT_DUMP_TABLES
+extern void ltt_dump_sys_call_table(void *call_data);
+extern void ltt_dump_idt_table(void *call_data);
+#else
+static __inline__ void ltt_dump_sys_call_table(void *call_data)
+{
+}
+
+static __inline__ void ltt_dump_idt_table(void *call_data)
+{
+}
+#endif
+
+/* Relay IOCTL */
+
+/* Get the next sub-buffer that can be read. */
+#define RELAY_GET_SB			_IOR(0xF5, 0x00, __u32)
+/* Release the oldest reserved (by "get") sub-buffer. */
+#define RELAY_PUT_SB			_IOW(0xF5, 0x01, __u32)
+/* returns the number of sub-buffers in the per cpu channel. */
+#define RELAY_GET_N_SB			_IOR(0xF5, 0x02, __u32)
+/* returns the size of the current sub-buffer. */
+#define RELAY_GET_SB_SIZE		_IOR(0xF5, 0x03, __u32)
+/* returns the maximum size for sub-buffers. */
+#define RELAY_GET_MAX_SB_SIZE		_IOR(0xF5, 0x04, __u32)
+
+#endif /* CONFIG_LTT */
+
+#endif /* _LTT_TRACER_H */
diff --git a/stblinux-2.6.31/include/linux/ltt-type-serializer.h b/stblinux-2.6.31/include/linux/ltt-type-serializer.h
new file mode 100644
index 0000000..021a107
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/ltt-type-serializer.h
@@ -0,0 +1,186 @@
+#ifndef _LTT_TYPE_SERIALIZER_H
+#define _LTT_TYPE_SERIALIZER_H
+
+#include <linux/ltt-tracer.h>
+#include <linux/if.h>	/* For IFNAMSIZ */
+
+/*
+ * largest_align must be non-zero, equal to the minimum between the largest type
+ * and sizeof(void *).
+ */
+extern void _ltt_specialized_trace(const struct marker *mdata, void *probe_data,
+		void *serialize_private, unsigned int data_size,
+		unsigned int largest_align);
+
+/*
+ * Statically check that 0 < largest_align < sizeof(void *) to make sure it is
+ * dumb-proof. It will make sure 0 is changed into 1 and unsigned long long is
+ * changed into sizeof(void *) on 32-bit architectures.
+ */
+static inline void ltt_specialized_trace(const struct marker *mdata,
+		void *probe_data,
+		void *serialize_private, unsigned int data_size,
+		unsigned int largest_align)
+{
+	largest_align = min_t(unsigned int, largest_align, sizeof(void *));
+	largest_align = max_t(unsigned int, largest_align, 1);
+	_ltt_specialized_trace(mdata, probe_data, serialize_private, data_size,
+		largest_align);
+}
+
+/*
+ * Type serializer definitions.
+ */
+
+/*
+ * Return size of structure without end-of-structure padding.
+ */
+#define serialize_sizeof(type)	offsetof(typeof(type), end_field)
+
+struct serialize_long_int {
+	unsigned long f1;
+	unsigned int f2;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_int_int_long {
+	unsigned int f1;
+	unsigned int f2;
+	unsigned long f3;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_int_int_short {
+	unsigned int f1;
+	unsigned int f2;
+	unsigned short f3;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_long_long {
+	unsigned long f1;
+	unsigned long f2;
+	unsigned long f3;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_long_int {
+	unsigned long f1;
+	unsigned long f2;
+	unsigned int f3;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_long_short_char {
+	unsigned long f1;
+	unsigned long f2;
+	unsigned short f3;
+	unsigned char f4;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_long_short {
+	unsigned long f1;
+	unsigned long f2;
+	unsigned short f3;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_short_char {
+	unsigned long f1;
+	unsigned short f2;
+	unsigned char f3;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_short {
+	unsigned long f1;
+	unsigned short f2;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_char {
+	unsigned long f1;
+	unsigned char f2;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_ifname {
+	unsigned long f1;
+	unsigned char f2[IFNAMSIZ];
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_sizet_int {
+	size_t f1;
+	unsigned int f2;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_long_sizet_int {
+	unsigned long f1;
+	unsigned long f2;
+	size_t f3;
+	unsigned int f4;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_long_sizet_int_int {
+	unsigned long f1;
+	unsigned long f2;
+	size_t f3;
+	unsigned int f4;
+	unsigned int f5;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_l4421224411111 {
+	unsigned long f1;
+	uint32_t f2;
+	uint32_t f3;
+	uint16_t f4;
+	uint8_t f5;
+	uint16_t f6;
+	uint16_t f7;
+	uint32_t f8;
+	uint32_t f9;
+	uint8_t f10;
+	uint8_t f11;
+	uint8_t f12;
+	uint8_t f13;
+	uint8_t f14;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_l214421224411111 {
+	unsigned long f1;
+	uint16_t f2;
+	uint8_t f3;
+	uint32_t f4;
+	uint32_t f5;
+	uint16_t f6;
+	uint8_t f7;
+	uint16_t f8;
+	uint16_t f9;
+	uint32_t f10;
+	uint32_t f11;
+	uint8_t f12;
+	uint8_t f13;
+	uint8_t f14;
+	uint8_t f15;
+	uint8_t f16;
+	uint8_t end_field[0];
+} LTT_ALIGN;
+
+struct serialize_l4412228 {
+	unsigned long f1;
+	uint32_t f2;
+	uint32_t f3;
+	uint8_t f4;
+	uint16_t f5;
+	uint16_t f6;
+	uint16_t f7;
+	uint64_t f8;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+#endif /* _LTT_TYPE_SERIALIZER_H */
diff --git a/stblinux-2.6.31/include/linux/marker.h b/stblinux-2.6.31/include/linux/marker.h
index b85e74c..0a6d53f 100644
--- a/stblinux-2.6.31/include/linux/marker.h
+++ b/stblinux-2.6.31/include/linux/marker.h
@@ -14,12 +14,15 @@
 
 #include <stdarg.h>
 #include <linux/types.h>
+#include <linux/immediate.h>
 
 struct module;
 struct marker;
+struct marker_probe_array;
 
 /**
  * marker_probe_func - Type of a marker probe function
+ * @mdata: marker data
  * @probe_private: probe private data
  * @call_private: call site private data
  * @fmt: format string
@@ -30,7 +33,8 @@ struct marker;
  * Type of marker probe functions. They receive the mdata and need to parse the
  * format string to recover the variable argument list.
  */
-typedef void marker_probe_func(void *probe_private, void *call_private,
+typedef void marker_probe_func(const struct marker *mdata,
+		void *probe_private, void *call_private,
 		const char *fmt, va_list *args);
 
 struct marker_probe_closure {
@@ -39,41 +43,46 @@ struct marker_probe_closure {
 };
 
 struct marker {
+	const char *channel;	/* Name of channel where to send data */
 	const char *name;	/* Marker name */
 	const char *format;	/* Marker format string, describing the
 				 * variable argument list.
 				 */
-	char state;		/* Marker state. */
+	DEFINE_IMV(char, state);/* Immediate value state. */
 	char ptype;		/* probe type : 0 : single, 1 : multi */
 				/* Probe wrapper */
+	u16 channel_id;		/* Numeric channel identifier, dynamic */
+	u16 event_id;		/* Numeric event identifier, dynamic */
 	void (*call)(const struct marker *mdata, void *call_private, ...);
 	struct marker_probe_closure single;
-	struct marker_probe_closure *multi;
+	struct marker_probe_array *multi;
 	const char *tp_name;	/* Optional tracepoint name */
 	void *tp_cb;		/* Optional tracepoint callback */
 } __attribute__((aligned(8)));
 
 #ifdef CONFIG_MARKERS
 
-#define _DEFINE_MARKER(name, tp_name_str, tp_cb, format)		\
-		static const char __mstrtab_##name[]			\
+#define _DEFINE_MARKER(channel, name, tp_name_str, tp_cb, format)	\
+		static const char __mstrtab_##channel##_##name[]	\
 		__attribute__((section("__markers_strings")))		\
-		= #name "\0" format;					\
-		static struct marker __mark_##name			\
+		= #channel "\0" #name "\0" format;			\
+		static struct marker __mark_##channel##_##name		\
 		__attribute__((section("__markers"), aligned(8))) =	\
-		{ __mstrtab_##name, &__mstrtab_##name[sizeof(#name)],	\
-		  0, 0, marker_probe_cb, { __mark_empty_function, NULL},\
+		{ __mstrtab_##channel##_##name,				\
+		  &__mstrtab_##channel##_##name[sizeof(#channel)],	\
+		  &__mstrtab_##channel##_##name[sizeof(#channel) +	\
+						sizeof(#name)],		\
+		  0, 0, 0, 0, marker_probe_cb,				\
+		  { __mark_empty_function, NULL},			\
 		  NULL, tp_name_str, tp_cb }
 
-#define DEFINE_MARKER(name, format)					\
-		_DEFINE_MARKER(name, NULL, NULL, format)
+#define DEFINE_MARKER(channel, name, format)				\
+		_DEFINE_MARKER(channel, name, NULL, NULL, format)
 
-#define DEFINE_MARKER_TP(name, tp_name, tp_cb, format)			\
-		_DEFINE_MARKER(name, #tp_name, tp_cb, format)
+#define DEFINE_MARKER_TP(channel, name, tp_name, tp_cb, format)		\
+		_DEFINE_MARKER(channel, name, #tp_name, tp_cb, format)
 
 /*
- * Note : the empty asm volatile with read constraint is used here instead of a
- * "used" attribute to fix a gcc 4.1.x bug.
  * Make sure the alignment of the structure in the __markers section will
  * not add unwanted padding between the beginning of the section and the
  * structure. Force alignment to the same alignment as the section start.
@@ -82,38 +91,49 @@ struct marker {
  * If generic is true, a variable read is used.
  * If generic is false, immediate values are used.
  */
-#define __trace_mark(generic, name, call_private, format, args...)	\
+#define __trace_mark(generic, channel, name, call_private, format, args...) \
 	do {								\
-		DEFINE_MARKER(name, format);				\
+		DEFINE_MARKER(channel, name, format);			\
 		__mark_check_format(format, ## args);			\
-		if (unlikely(__mark_##name.state)) {			\
-			(*__mark_##name.call)				\
-				(&__mark_##name, call_private, ## args);\
+		if (!generic) {						\
+			if (unlikely(imv_read(				\
+					__mark_##channel##_##name.state))) \
+				(*__mark_##channel##_##name.call)	\
+					(&__mark_##channel##_##name,	\
+					call_private, ## args);		\
+		} else {						\
+			if (unlikely(_imv_read(				\
+					__mark_##channel##_##name.state))) \
+				(*__mark_##channel##_##name.call)	\
+					(&__mark_##channel##_##name,	\
+					call_private, ## args);		\
 		}							\
 	} while (0)
 
-#define __trace_mark_tp(name, call_private, tp_name, tp_cb, format, args...) \
+#define __trace_mark_tp(channel, name, call_private, tp_name, tp_cb,	\
+			format, args...)				\
 	do {								\
 		void __check_tp_type(void)				\
 		{							\
 			register_trace_##tp_name(tp_cb);		\
 		}							\
-		DEFINE_MARKER_TP(name, tp_name, tp_cb, format);		\
+		DEFINE_MARKER_TP(channel, name, tp_name, tp_cb, format);\
 		__mark_check_format(format, ## args);			\
-		(*__mark_##name.call)(&__mark_##name, call_private,	\
-					## args);			\
+		(*__mark_##channel##_##name.call)(&__mark_##channel##_##name, \
+			call_private, ## args);				\
 	} while (0)
 
 extern void marker_update_probe_range(struct marker *begin,
 	struct marker *end);
 
-#define GET_MARKER(name)	(__mark_##name)
+#define GET_MARKER(channel, name)	(__mark_##channel##_##name)
 
 #else /* !CONFIG_MARKERS */
-#define DEFINE_MARKER(name, tp_name, tp_cb, format)
-#define __trace_mark(generic, name, call_private, format, args...) \
+#define DEFINE_MARKER(channel, name, tp_name, tp_cb, format)
+#define __trace_mark(generic, channel, name, call_private, format, args...) \
 		__mark_check_format(format, ## args)
-#define __trace_mark_tp(name, call_private, tp_name, tp_cb, format, args...) \
+#define __trace_mark_tp(channel, name, call_private, tp_name, tp_cb,	\
+		format, args...)					\
 	do {								\
 		void __check_tp_type(void)				\
 		{							\
@@ -124,11 +144,12 @@ extern void marker_update_probe_range(struct marker *begin,
 static inline void marker_update_probe_range(struct marker *begin,
 	struct marker *end)
 { }
-#define GET_MARKER(name)
+#define GET_MARKER(channel, name)
 #endif /* CONFIG_MARKERS */
 
 /**
  * trace_mark - Marker using code patching
+ * @channel: marker channel (where to send the data), not quoted.
  * @name: marker name, not quoted.
  * @format: format string
  * @args...: variable argument list
@@ -136,11 +157,12 @@ static inline void marker_update_probe_range(struct marker *begin,
  * Places a marker using optimized code patching technique (imv_read())
  * to be enabled when immediate values are present.
  */
-#define trace_mark(name, format, args...) \
-	__trace_mark(0, name, NULL, format, ## args)
+#define trace_mark(channel, name, format, args...) \
+	__trace_mark(0, channel, name, NULL, format, ## args)
 
 /**
  * _trace_mark - Marker using variable read
+ * @channel: marker channel (where to send the data), not quoted.
  * @name: marker name, not quoted.
  * @format: format string
  * @args...: variable argument list
@@ -150,11 +172,12 @@ static inline void marker_update_probe_range(struct marker *begin,
  * modification based enabling is not welcome. (__init and __exit functions,
  * lockdep, some traps, printk).
  */
-#define _trace_mark(name, format, args...) \
-	__trace_mark(1, name, NULL, format, ## args)
+#define _trace_mark(channel, name, format, args...) \
+	__trace_mark(1, channel, name, NULL, format, ## args)
 
 /**
  * trace_mark_tp - Marker in a tracepoint callback
+ * @channel: marker channel (where to send the data), not quoted.
  * @name: marker name, not quoted.
  * @tp_name: tracepoint name, not quoted.
  * @tp_cb: tracepoint callback. Should have an associated global symbol so it
@@ -164,14 +187,19 @@ static inline void marker_update_probe_range(struct marker *begin,
  *
  * Places a marker in a tracepoint callback.
  */
-#define trace_mark_tp(name, tp_name, tp_cb, format, args...)	\
-	__trace_mark_tp(name, NULL, tp_name, tp_cb, format, ## args)
+#define trace_mark_tp(channel, name, tp_name, tp_cb, format, args...)	\
+	__trace_mark_tp(channel, name, NULL, tp_name, tp_cb, format, ## args)
 
 /**
  * MARK_NOARGS - Format string for a marker with no argument.
  */
 #define MARK_NOARGS " "
 
+extern void lock_markers(void);
+extern void unlock_markers(void);
+
+extern void markers_compact_event_ids(void);
+
 /* To be used for string format validity checking with gcc */
 static inline void __printf(1, 2) ___mark_check_format(const char *fmt, ...)
 {
@@ -192,13 +220,13 @@ extern void marker_probe_cb(const struct marker *mdata,
  * Connect a probe to a marker.
  * private data pointer must be a valid allocated memory address, or NULL.
  */
-extern int marker_probe_register(const char *name, const char *format,
-				marker_probe_func *probe, void *probe_private);
+extern int marker_probe_register(const char *channel, const char *name,
+	const char *format, marker_probe_func *probe, void *probe_private);
 
 /*
  * Returns the private data given to marker_probe_register.
  */
-extern int marker_probe_unregister(const char *name,
+extern int marker_probe_unregister(const char *channel, const char *name,
 	marker_probe_func *probe, void *probe_private);
 /*
  * Unregister a marker by providing the registered private data.
@@ -206,8 +234,11 @@ extern int marker_probe_unregister(const char *name,
 extern int marker_probe_unregister_private_data(marker_probe_func *probe,
 	void *probe_private);
 
-extern void *marker_get_private_data(const char *name, marker_probe_func *probe,
-	int num);
+extern void *marker_get_private_data(const char *channel, const char *name,
+	marker_probe_func *probe, int num);
+
+const char *marker_get_name_from_id(u16 channel_id, u16 event_id);
+const char *marker_get_fmt_from_id(u16 channel_id, u16 event_id);
 
 /*
  * marker_synchronize_unregister must be called between the last marker probe
@@ -218,4 +249,20 @@ extern void *marker_get_private_data(const char *name, marker_probe_func *probe,
  */
 #define marker_synchronize_unregister() synchronize_sched()
 
+struct marker_iter {
+	struct module *module;
+	struct marker *marker;
+};
+
+extern void marker_iter_start(struct marker_iter *iter);
+extern void marker_iter_next(struct marker_iter *iter);
+extern void marker_iter_stop(struct marker_iter *iter);
+extern void marker_iter_reset(struct marker_iter *iter);
+extern int marker_get_iter_range(struct marker **marker, struct marker *begin,
+	struct marker *end);
+extern int _is_marker_enabled(const char *channel, const char *name);
+extern int is_marker_enabled(const char *channel, const char *name);
+extern int is_marker_present(const char *channel, const char *name);
+extern void marker_update_probes(void);
+
 #endif
diff --git a/stblinux-2.6.31/include/linux/module.h b/stblinux-2.6.31/include/linux/module.h
index 098bdb7..c9e393c 100644
--- a/stblinux-2.6.31/include/linux/module.h
+++ b/stblinux-2.6.31/include/linux/module.h
@@ -15,6 +15,7 @@
 #include <linux/stringify.h>
 #include <linux/kobject.h>
 #include <linux/moduleparam.h>
+#include <linux/immediate.h>
 #include <linux/marker.h>
 #include <linux/tracepoint.h>
 #include <asm/local.h>
@@ -325,6 +326,10 @@ struct module
 	/* The command line arguments (may be mangled).  People like
 	   keeping pointers to this stuff */
 	char *args;
+#ifdef USE_IMMEDIATE
+	struct __imv *immediate;
+	unsigned int num_immediate;
+#endif
 #ifdef CONFIG_MARKERS
 	struct marker *markers;
 	unsigned int num_markers;
@@ -526,8 +531,10 @@ int register_module_notifier(struct notifier_block * nb);
 int unregister_module_notifier(struct notifier_block * nb);
 
 extern void print_modules(void);
+extern void list_modules(void *call_data);
 
 extern void module_update_markers(void);
+extern int module_get_iter_markers(struct marker_iter *iter);
 
 extern void module_update_tracepoints(void);
 extern int module_get_iter_tracepoints(struct tracepoint_iter *iter);
@@ -643,6 +650,10 @@ static inline void print_modules(void)
 {
 }
 
+static inline void list_modules(void *call_data)
+{
+}
+
 static inline void module_update_markers(void)
 {
 }
@@ -656,8 +667,26 @@ static inline int module_get_iter_tracepoints(struct tracepoint_iter *iter)
 	return 0;
 }
 
+static inline int module_get_iter_markers(struct marker_iter *iter)
+{
+	return 0;
+}
+
 #endif /* CONFIG_MODULES */
 
+#if defined(CONFIG_MODULES) && defined(USE_IMMEDIATE)
+extern void _module_imv_update(void);
+extern void module_imv_update(void);
+#else
+static inline void _module_imv_update(void)
+{
+}
+
+static inline void module_imv_update(void)
+{
+}
+#endif
+
 struct device_driver;
 #ifdef CONFIG_SYSFS
 struct module;
diff --git a/stblinux-2.6.31/include/linux/netdevice.h b/stblinux-2.6.31/include/linux/netdevice.h
index d4a4d98..08a1dd8 100644
--- a/stblinux-2.6.31/include/linux/netdevice.h
+++ b/stblinux-2.6.31/include/linux/netdevice.h
@@ -42,6 +42,7 @@
 #include <linux/rculist.h>
 #include <linux/dmaengine.h>
 #include <linux/workqueue.h>
+#include <trace/net.h>
 
 #include <linux/ethtool.h>
 #include <net/net_namespace.h>
diff --git a/stblinux-2.6.31/include/linux/poison.h b/stblinux-2.6.31/include/linux/poison.h
index 6729f7d..4f13b13 100644
--- a/stblinux-2.6.31/include/linux/poison.h
+++ b/stblinux-2.6.31/include/linux/poison.h
@@ -65,6 +65,10 @@
 #define MUTEX_DEBUG_INIT	0x11
 #define MUTEX_DEBUG_FREE	0x22
 
+/********** Priority-Sifting Reader-Writer Locks **********/
+#define PSRWLOCK_DEBUG_INIT	0x33
+#define PSRWLOCK_DEBUG_FREE	0x44
+
 /********** security/ **********/
 #define KEY_DESTROY		0xbd
 
diff --git a/stblinux-2.6.31/include/linux/poll.h b/stblinux-2.6.31/include/linux/poll.h
index fa287f2..33231d6 100644
--- a/stblinux-2.6.31/include/linux/poll.h
+++ b/stblinux-2.6.31/include/linux/poll.h
@@ -77,6 +77,8 @@ static inline int poll_schedule(struct poll_wqueues *pwq, int state)
 	return poll_schedule_timeout(pwq, state, NULL, 0);
 }
 
+extern void poll_wait_set_exclusive(poll_table *p);
+
 /*
  * Scaleable version of the fd_set.
  */
diff --git a/stblinux-2.6.31/include/linux/profile.h b/stblinux-2.6.31/include/linux/profile.h
index a0fc322..d9a0fd1 100644
--- a/stblinux-2.6.31/include/linux/profile.h
+++ b/stblinux-2.6.31/include/linux/profile.h
@@ -5,6 +5,7 @@
 #include <linux/init.h>
 #include <linux/cpumask.h>
 #include <linux/cache.h>
+#include <linux/immediate.h>
 
 #include <asm/errno.h>
 
@@ -38,7 +39,7 @@ enum profile_type {
 
 #ifdef CONFIG_PROFILING
 
-extern int prof_on __read_mostly;
+DECLARE_IMV(char, prof_on) __read_mostly;
 
 /* init basic kernel profiler */
 int profile_init(void);
@@ -58,7 +59,7 @@ static inline void profile_hit(int type, void *ip)
 	/*
 	 * Speedup for the common (no profiling enabled) case:
 	 */
-	if (unlikely(prof_on == type))
+	if (unlikely(imv_read(prof_on) == type))
 		profile_hits(type, ip, 1);
 }
 
diff --git a/stblinux-2.6.31/include/linux/psrwlock-api.h b/stblinux-2.6.31/include/linux/psrwlock-api.h
new file mode 100644
index 0000000..890993b
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/psrwlock-api.h
@@ -0,0 +1,243 @@
+#ifndef _LINUX_PSRWLOCK_API_H
+#define _LINUX_PSRWLOCK_API_H
+
+/* Reader lock */
+
+/*
+ * many readers, from irq/softirq/non preemptable and preemptable thread
+ * context. Protects against writers.
+ *
+ * Read lock fastpath :
+ *
+ * A cmpxchg is used here and _not_ a simple add because a lower-priority reader
+ * could block the writer while it is waiting for readers to clear the
+ * uncontended path. This would happen if, for instance, the reader gets
+ * interrupted between the add and the moment it gets to the slow path.
+ */
+
+/*
+ * Called from any context.
+ * Statically check for preemptable writer to compile-out the check if all the
+ * contexts accessing the lock are non-preemptable.
+ */
+static inline void psread_unlock(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc = atomic_sub_return(UC_READER_OFFSET, &rwlock->uc);
+	if (wctx == PSRW_PRIO_P || (rctx & PSR_PTHREAD))
+		psrwlock_preempt_check(uc, rwlock);
+}
+
+/*
+ * Called from interrupt disabled or interrupt context.
+ */
+static inline void psread_lock_irq(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_IRQ));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return;
+	psread_lock_slow_irq(uc, rwlock);
+}
+
+static inline int psread_trylock_irq(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_IRQ));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return 1;
+	return psread_trylock_slow_irq(uc, rwlock);
+}
+
+/*
+ * Called from softirq context.
+ */
+
+static inline void psread_lock_bh(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_BH));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return;
+	psread_lock_slow_bh(uc, rwlock);
+}
+
+static inline int psread_trylock_bh(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_BH));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return 1;
+	return psread_trylock_slow_bh(uc, rwlock);
+}
+
+
+/*
+ * Called from non-preemptable thread context.
+ */
+
+static inline void psread_lock_inatomic(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_NPTHREAD));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return;
+	psread_lock_slow_inatomic(uc, rwlock);
+}
+
+static inline int psread_trylock_inatomic(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_NPTHREAD));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return 1;
+	return psread_trylock_slow_inatomic(uc, rwlock);
+}
+
+
+/*
+ * Called from preemptable thread context.
+ */
+
+static inline void psread_lock(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return;
+	psread_lock_slow(uc, rwlock);
+}
+
+static inline int psread_lock_interruptible(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return 0;
+	return psread_lock_interruptible_slow(uc, rwlock);
+}
+
+static inline int psread_trylock(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return 1;
+	return psread_trylock_slow(uc, rwlock);
+}
+
+
+/* Writer Lock */
+
+/*
+ * ctx is the context map showing which contexts can take the read lock and
+ * which context is using the write lock.
+ *
+ * Write lock use example, where the lock is used by readers in interrupt,
+ * preemptable context and non-preemptable context. The writer lock is taken in
+ * preemptable context.
+ *
+ * static DEFINE_PSRWLOCK(lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ * CHECK_PSRWLOCK_MAP(lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ *
+ *  pswrite_lock(&lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ *  ...
+ *  pswrite_unlock(&lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ */
+static inline
+void pswrite_lock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	/* no other reader nor writer present, try to take the lock */
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_WRITER);
+	if (likely(!uc))
+		return;
+	else
+		pswrite_lock_slow(uc, rwlock);
+}
+
+static inline
+int pswrite_lock_interruptible(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	/* no other reader nor writer present, try to take the lock */
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_WRITER);
+	if (likely(!uc))
+		return 0;
+	else
+		return pswrite_lock_interruptible_slow(uc, rwlock);
+}
+
+static inline
+int pswrite_trylock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	/* no other reader nor writer present, try to take the lock */
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_WRITER);
+	if (likely(!uc))
+		return 1;
+	else
+		return pswrite_trylock_slow(uc, rwlock);
+}
+
+static inline
+void pswrite_unlock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	/*
+	 * atomic_cmpxchg makes sure we commit the data before reenabling
+	 * the lock. Will take the slow path if there are active readers, if
+	 * UC_SLOW_WRITER is set or if there are threads in the wait queue.
+	 */
+	uc = atomic_cmpxchg(&rwlock->uc, UC_WRITER, 0);
+	if (likely(uc == UC_WRITER)) {
+		write_context_enable(wctx, rctx);
+		/*
+		 * no need to check preempt because all wait queue masks
+		 * were 0. An active wait queue would trigger the slow path.
+		 */
+		return;
+	}
+	/*
+	 * Go through the slow unlock path to check if we must clear the
+	 * UC_SLOW_WRITER bit.
+	 */
+	pswrite_unlock_slow(uc, rwlock);
+}
+
+#endif /* _LINUX_PSRWLOCK_API_H */
diff --git a/stblinux-2.6.31/include/linux/psrwlock-debug-api.h b/stblinux-2.6.31/include/linux/psrwlock-debug-api.h
new file mode 100644
index 0000000..93e1e82
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/psrwlock-debug-api.h
@@ -0,0 +1,189 @@
+#ifndef _LINUX_PSRWLOCK_DEBUG_API_H
+#define _LINUX_PSRWLOCK_DEBUG_API_H
+
+#include <linux/lockdep.h>
+
+/*
+ * Priority-Sifting reader-writer lock debugging API. Using the slow path.
+ */
+
+/* Reader lock */
+
+/*
+ * many readers, from irq/softirq/non preemptable and preemptable thread
+ * context. Protects against writers.
+ */
+
+/*
+ * Called from any context.
+ * Statically check for preemptable writer to compile-out the check if all the
+ * contexts accessing the lock are non-preemptable.
+ */
+extern void psread_unlock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx);
+
+/*
+ * Called from interrupt disabled or interrupt context.
+ */
+static inline void psread_lock_irq(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_IRQ));
+	uc = atomic_read(&rwlock->uc);
+	psread_lock_slow_irq(uc, rwlock);
+}
+
+static inline int psread_trylock_irq(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_IRQ));
+	uc = atomic_read(&rwlock->uc);
+	return psread_trylock_slow_irq(uc, rwlock);
+}
+
+/*
+ * Called from softirq context.
+ */
+
+static inline void psread_lock_bh(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_BH));
+	uc = atomic_read(&rwlock->uc);
+	psread_lock_slow_bh(uc, rwlock);
+}
+
+static inline int psread_trylock_bh(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_BH));
+	uc = atomic_read(&rwlock->uc);
+	return psread_trylock_slow_bh(uc, rwlock);
+}
+
+
+/*
+ * Called from non-preemptable thread context.
+ */
+
+static inline void psread_lock_inatomic(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_NPTHREAD));
+	uc = atomic_read(&rwlock->uc);
+	psread_lock_slow_inatomic(uc, rwlock);
+}
+
+static inline int psread_trylock_inatomic(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_NPTHREAD));
+	uc = atomic_read(&rwlock->uc);
+	return psread_trylock_slow_inatomic(uc, rwlock);
+}
+
+
+/*
+ * Called from preemptable thread context.
+ */
+
+static inline void psread_lock(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_read(&rwlock->uc);
+	psread_lock_slow(uc, rwlock);
+}
+
+static inline int psread_lock_interruptible(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_read(&rwlock->uc);
+	return psread_lock_interruptible_slow(uc, rwlock);
+}
+
+static inline int psread_trylock(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_read(&rwlock->uc);
+	return psread_trylock_slow(uc, rwlock);
+}
+
+
+/* Writer Lock */
+
+/*
+ * ctx is the context map showing which contexts can take the read lock and
+ * which context is using the write lock.
+ *
+ * Write lock use example, where the lock is used by readers in interrupt,
+ * preemptable context and non-preemptable context. The writer lock is taken in
+ * preemptable context.
+ *
+ * static DEFINE_PSRWLOCK(lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ * CHECK_PSRWLOCK_MAP(lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ *
+ *  pswrite_lock(&lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ *  ...
+ *  pswrite_unlock(&lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ */
+static inline
+void pswrite_lock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	uc = atomic_read(&rwlock->uc);
+	pswrite_lock_slow(uc, rwlock);
+}
+
+static inline
+int pswrite_lock_interruptible(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	uc = atomic_read(&rwlock->uc);
+	return pswrite_lock_interruptible_slow(uc, rwlock);
+}
+
+static inline
+int pswrite_trylock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	uc = atomic_read(&rwlock->uc);
+	return pswrite_trylock_slow(uc, rwlock);
+}
+
+static inline
+void pswrite_unlock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	uc = atomic_read(&rwlock->uc);
+	pswrite_unlock_slow(uc, rwlock);
+}
+
+#endif /* _LINUX_PSRWLOCK_DEBUG_API_H */
diff --git a/stblinux-2.6.31/include/linux/psrwlock-debug.h b/stblinux-2.6.31/include/linux/psrwlock-debug.h
new file mode 100644
index 0000000..e71e61f
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/psrwlock-debug.h
@@ -0,0 +1,22 @@
+#ifndef _LINUX_PSRWLOCK_DEBUG_H
+#define _LINUX_PSRWLOCK_DEBUG_H
+
+#include <linux/lockdep.h>
+
+/*
+ * Priority-Sifting Reader-Writer Locks : debugging helpers:
+ */
+
+#define __DEBUG_PSRWLOCK_INITIALIZER(lockname)				\
+	.magic = &lockname,
+
+#define psrwlock_init(psrwlock, _rctx, _wctx)				\
+do {									\
+	static struct lock_class_key __key;				\
+									\
+	__psrwlock_init((psrwlock), #psrwlock, &__key, _rctx, _wctx);	\
+} while (0)
+
+extern void psrwlock_destroy(struct psrwlock *lock);
+
+#endif /* _LINUX_PSRWLOCK_DEBUG_H */
diff --git a/stblinux-2.6.31/include/linux/psrwlock-latency-trace.h b/stblinux-2.6.31/include/linux/psrwlock-latency-trace.h
new file mode 100644
index 0000000..9312ab5
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/psrwlock-latency-trace.h
@@ -0,0 +1,104 @@
+#ifndef _LINUX_PSRWLOCK_LATENCY_TRACE_H
+#define _LINUX_PSRWLOCK_LATENCY_TRACE_H
+
+/*
+ * Priority Sifting Reader-Writer Lock Latency Tracer
+ *
+ * Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * August 2008
+ */
+
+#include <linux/hardirq.h>
+
+#ifdef CONFIG_PSRWLOCK_LATENCY_TEST
+
+extern void psrwlock_profile_latency_reset(void);
+extern void psrwlock_profile_latency_print(void);
+
+extern void psrwlock_profile_irq_disable(void);
+extern void psrwlock_profile_irq_enable(void);
+extern void psrwlock_profile_bh_disable(void);
+extern void psrwlock_profile_bh_enable(void);
+
+#define psrwlock_irq_save(flags)				\
+do {								\
+	local_irq_save(flags);					\
+	if (!irqs_disabled_flags(flags))			\
+		psrwlock_profile_irq_disable();		\
+} while (0)
+
+#define psrwlock_irq_restore(flags)				\
+do {								\
+	if (irqs_disabled() && !irqs_disabled_flags(flags))	\
+		psrwlock_profile_irq_enable();		\
+	local_irq_restore(flags);				\
+} while (0)
+
+static inline void psrwlock_irq_disable(void)
+{
+	unsigned long flags;
+
+	local_save_flags(flags);
+	local_irq_disable();
+	if (!irqs_disabled_flags(flags))
+		psrwlock_profile_irq_disable();
+}
+static inline void psrwlock_irq_enable(void)
+{
+	if (irqs_disabled())
+		psrwlock_profile_irq_enable();
+	local_irq_enable();
+}
+static inline void psrwlock_bh_disable(void)
+{
+	local_bh_disable();
+	if (softirq_count() == SOFTIRQ_OFFSET)
+		psrwlock_profile_bh_disable();
+}
+static inline void psrwlock_bh_enable(void)
+{
+	if (softirq_count() == SOFTIRQ_OFFSET)
+		psrwlock_profile_bh_enable();
+	local_bh_enable();
+}
+static inline void psrwlock_bh_enable_ip(unsigned long ip)
+{
+	if (softirq_count() == SOFTIRQ_OFFSET)
+		psrwlock_profile_bh_enable();
+	local_bh_enable_ip(ip);
+}
+
+#ifdef CONFIG_PREEMPT
+extern void psrwlock_profile_preempt_disable(void);
+extern void psrwlock_profile_preempt_enable(void);
+
+static inline void psrwlock_preempt_disable(void)
+{
+	preempt_disable();
+	if (preempt_count() == PREEMPT_OFFSET)
+		psrwlock_profile_preempt_disable();
+}
+static inline void psrwlock_preempt_enable(void)
+{
+	if (preempt_count() == PREEMPT_OFFSET)
+		psrwlock_profile_preempt_enable();
+	preempt_enable();
+}
+static inline void psrwlock_preempt_enable_no_resched(void)
+{
+	/*
+	 * Not exactly true, since we really re-preempt at the next preempt
+	 * check, but gives a good idea (lower-bound).
+	 */
+	if (preempt_count() == PREEMPT_OFFSET)
+		psrwlock_profile_preempt_enable();
+	preempt_enable_no_resched();
+}
+#else
+#define psrwlock_preempt_disable()		preempt_disable()
+#define psrwlock_preempt_enable()		preempt_enable()
+#define psrwlock_preempt_enable_no_resched()	preempt_enable_no_resched()
+#endif
+
+#endif	/* CONFIG_PSRWLOCK_LATENCY_TEST */
+#endif	/* _LINUX_PSRWLOCK_LATENCY_TRACE_H */
diff --git a/stblinux-2.6.31/include/linux/psrwlock-types.h b/stblinux-2.6.31/include/linux/psrwlock-types.h
new file mode 100644
index 0000000..5606857
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/psrwlock-types.h
@@ -0,0 +1,164 @@
+#ifndef _LINUX_PSRWLOCK_TYPES_H
+#define _LINUX_PSRWLOCK_TYPES_H
+
+/*
+ * Priority Sifting Reader-Writer Lock types definition
+ *
+ * Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * August 2008
+ */
+
+#include <linux/list.h>
+#include <linux/linkage.h>
+#include <linux/lockdep.h>
+
+#include <asm/atomic.h>
+
+/*
+ * This table represents which is the lowest read priority context can be used
+ * given the highest read priority context and the context in which the write
+ * lock is taken.
+ *
+ * e.g. given the highest priority context from which we take the read lock is
+ * interrupt context (IRQ) and the context where the write lock is taken is
+ * non-preemptable (NP), we should never have a reader in context lower than
+ * NP.
+ *
+ * X means : don't !
+ *
+ * X axis : Priority of writer
+ * Y axis : Max priority of reader
+ * Maps to :  Minimum priority of a reader.
+ *
+ * Highest Read Prio / Write Prio    | P     NP    BH    IRQ
+ * ------------------------------------------------------------------------
+ * P                                 | P     X     X     X
+ * NP                                | P     NP    X     X
+ * BH                                | P     NP    BH    X
+ * IRQ                               | P     NP    BH    IRQ
+ *
+ * This table is verified by the CHECK_PSRWLOCK_MAP macro.
+ */
+
+enum psrw_prio {
+	PSRW_PRIO_P,
+	PSRW_PRIO_NP,
+	PSRW_PRIO_BH,
+	PSRW_PRIO_IRQ,
+	PSRW_NR_PRIO,
+};
+
+/*
+ * Possible execution contexts for readers.
+ */
+#define PSR_PTHREAD	(1U << PSRW_PRIO_P)
+#define PSR_NPTHREAD	(1U << PSRW_PRIO_NP)
+#define PSR_BH		(1U << PSRW_PRIO_BH)
+#define PSR_IRQ		(1U << PSRW_PRIO_IRQ)
+#define PSR_NR		PSRW_NR_PRIO
+#define PSR_MASK	(PSR_PTHREAD | PSR_NPTHREAD | PSR_BH | PSR_IRQ)
+
+typedef struct psrwlock {
+	atomic_t uc;			/* Uncontended word	*/
+	atomic_t ws;			/* Writers in the slow path count */
+	atomic_long_t prio[PSRW_NR_PRIO]; /* Per priority slow path counts */
+	u32 rctx_bitmap;		/* Allowed read execution ctx */
+	enum psrw_prio wctx;		/* Allowed write execution ctx */
+	struct list_head wait_list_r;	/* Preemptable readers wait queue */
+	struct list_head wait_list_w;	/* Preemptable writers wait queue */
+#ifdef CONFIG_DEBUG_PSRWLOCK
+	struct thread_info	*owner;
+	const char 		*name;
+	void			*magic;
+#endif
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map      dep_map;
+#endif
+} psrwlock_t;
+
+/*
+ * This is the control structure for tasks blocked on psrwlock,
+ * which resides on the blocked task's kernel stack:
+ */
+struct psrwlock_waiter {
+	struct list_head	list;
+	struct task_struct	*task;
+#ifdef CONFIG_DEBUG_PSRWLOCK
+	struct psrwlock		*lock;
+	void			*magic;
+#endif
+};
+
+#ifdef CONFIG_DEBUG_PSRWLOCK
+# include <linux/psrwlock-debug.h>
+#else
+# define __DEBUG_PSRWLOCK_INITIALIZER(lockname)
+# define psrwlock_init(psrwlock, _rctx, _wctx)				\
+do {									\
+	static struct lock_class_key __key;				\
+									\
+	__psrwlock_init((psrwlock), #psrwlock, &__key, _rctx, _wctx);	\
+} while (0)
+# define psrwlock_destroy(psrwlock)		do { } while (0)
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define __DEP_MAP_PSRWLOCK_INITIALIZER(lockname)			\
+		.dep_map = { .name = #lockname },
+#else
+# define __DEP_MAP_PSRWLOCK_INITIALIZER(lockname)
+#endif
+
+#define __PSRWLOCK_UNLOCKED(x, _wctx, _rctx)				\
+	{								\
+		.uc = { 0 },						\
+		.ws = { 0 },						\
+		.prio[0 ... (PSRW_NR_PRIO - 1)] = { 0 },		\
+		.rctx_bitmap = (_rctx),					\
+		.wctx = (_wctx),					\
+		.wait_list_r = LIST_HEAD_INIT((x).wait_list_r),		\
+		.wait_list_w = LIST_HEAD_INIT((x).wait_list_w),		\
+		__DEBUG_PSRWLOCK_INITIALIZER(x)				\
+		__DEP_MAP_PSRWLOCK_INITIALIZER(x)			\
+	}
+
+#define DEFINE_PSRWLOCK(x, wctx, rctx)					\
+	psrwlock_t x = __PSRWLOCK_UNLOCKED(x, wctx, rctx)
+
+/*
+ * Statically check that no reader with priority lower than the writer is
+ * possible.
+ */
+#define CHECK_PSRWLOCK_MAP(x, wctx, rctx)				\
+	static inline void __psrwlock_bad_context_map_##x(void)		\
+	{								\
+		BUILD_BUG_ON((~(~0UL << (wctx))) & (rctx));		\
+	}
+
+extern void __psrwlock_init(struct psrwlock *lock, const char *name,
+			    struct lock_class_key *key,
+			    u32 rctx, enum psrw_prio wctx);
+
+/**
+ * psrwlock_is_locked - is the psrwlock locked
+ * @lock: the psrwlock to be queried
+ *
+ * Returns 1 if the psrwlock is locked or if any accessor is waiting for it,
+ * else returns 0.
+ * Also check the per-priority counts to make sure no reader nor writer is
+ * within the per-priority slow path waiting period, where they do not appear
+ * in the fastpath "uc".
+ */
+static inline int psrwlock_is_locked(struct psrwlock *lock)
+{
+	unsigned int i;
+
+	if (atomic_read(&lock->uc))
+		return 1;
+	for (i = 0; i < PSRW_NR_PRIO; i++)
+		if (atomic_long_read(&lock->prio[i]))
+			return 1;
+	return 0;
+}
+
+#endif /* _LINUX_PSRWLOCK_TYPES_H */
diff --git a/stblinux-2.6.31/include/linux/psrwlock.h b/stblinux-2.6.31/include/linux/psrwlock.h
new file mode 100644
index 0000000..f65ef33
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/psrwlock.h
@@ -0,0 +1,221 @@
+#ifndef _LINUX_PSRWLOCK_H
+#define _LINUX_PSRWLOCK_H
+
+/*
+ * Priority Sifting Reader-Writer Lock
+ *
+ * Priority Sifting Reader-Writer Lock (psrwlock) excludes reader execution
+ * contexts one at a time, thus increasing the writer priority in stages. It
+ * favors writers against reader threads, but lets higher priority readers in
+ * even when there are subscribed writers waiting for the lock at a given lower
+ * priority. Very frequent writers could starve reader threads.
+ *
+ * See psrwlock-types.h for types definitions.
+ * See psrwlock.c for algorithmic details.
+ *
+ * Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * August 2008
+ */
+
+#include <linux/hardirq.h>
+#include <linux/bottom_half.h>
+#include <linux/list.h>
+#include <linux/linkage.h>
+#include <linux/psrwlock-types.h>
+
+#include <asm/atomic.h>
+
+#ifdef CONFIG_M68K	/* system.h local_irq_enable depends on sched.h */
+#include <linux/sched.h>
+#endif
+
+#define NR_PREEMPT_BUSY_LOOPS	100
+
+/*
+ * Uncontended word bits (32 bits)
+ *
+ * Because we deal with overflow by busy-looping waiting for the counter to
+ * decrement, make sure the maximum allowed for lower-priority execution
+ * contexts is lower than the maximum for higher priority execution contexts.
+ * Therefore, all contexts use the same counter bits, but they reach their
+ * overflow capacity one bit apart from each other (only used in the slow path).
+ *
+ * 3 bits for status
+ * 29 bits for reader count
+ *   reserve 1 high bit for irqs
+ *   reserve 1 high bit for bh
+ *   reserve 1 high bit for non-preemptable threads
+ *   26 bits left for preemptable readers count
+ */
+#define UC_READER_MAX		(1U << 29)
+#define UC_HARDIRQ_READER_MAX	UC_READER_MAX
+#define UC_SOFTIRQ_READER_MAX	(UC_HARDIRQ_READER_MAX >> 1)
+#define UC_NPTHREAD_READER_MAX	(UC_SOFTIRQ_READER_MAX >> 1)
+#define UC_PTHREAD_READER_MAX	(UC_NPTHREAD_READER_MAX >> 1)
+
+#define UC_WRITER		(1U << 0)
+#define UC_SLOW_WRITER		(1U << 1)
+#define UC_WQ_ACTIVE		(1U << 2)
+#define UC_READER_OFFSET	(1U << 3)
+#define UC_HARDIRQ_READER_MASK	((UC_HARDIRQ_READER_MAX - 1) * UC_READER_OFFSET)
+#define UC_SOFTIRQ_READER_MASK	((UC_SOFTIRQ_READER_MAX - 1) * UC_READER_OFFSET)
+#define UC_NPTHREAD_READER_MASK		\
+	((UC_NPTHREAD_READER_MAX - 1) * UC_READER_OFFSET)
+#define UC_PTHREAD_READER_MASK	((UC_PTHREAD_READER_MAX - 1) * UC_READER_OFFSET)
+#define UC_READER_MASK		UC_HARDIRQ_READER_MASK
+
+
+/*
+ * Writers in slow path count and mutexes (32 bits)
+ *
+ * 1 bit for WS_WQ_MUTEX (wait queue mutex, always taken with irqs off)
+ * 1 bit for WS_COUNT_MUTEX (protects writer count and UC_SLOW_WRITER updates,
+ *                           taken in initial writer context).
+ * 1 bit for WS_LOCK_MUTEX (single writer in critical section)
+ * 29 bits for writer count.
+ */
+#define WS_WQ_MUTEX		(1U << 0)
+#define WS_COUNT_MUTEX		(1U << 1)
+#define WS_LOCK_MUTEX		(1U << 2)
+
+#define WS_MAX			(1U << 29)
+#define WS_OFFSET		(1U << 3)
+#define WS_MASK			((WS_MAX - 1) * WS_OFFSET)
+
+
+/*
+ * Per-context slow path reader and writer count maximum, offset and mask.
+ * unsigned long type. Used to atomically detect that there is no contention in
+ * a given slow path context and subscribe a writer or let a reader take the
+ * slow path context lock.
+ */
+#define CTX_WOFFSET		(1UL << 0)
+#define CTX_WMAX		(1UL << (BITS_PER_LONG/2))
+#define CTX_WMASK		((CTX_WMAX - 1) * CTX_WOFFSET)
+
+#define CTX_ROFFSET		CTX_WMAX
+#define CTX_RMAX		(1UL << (BITS_PER_LONG/2))
+#define CTX_RMASK		((CTX_RMAX - 1) * CTX_ROFFSET)
+
+
+/*
+ * Internal slow paths.
+ */
+extern asmregparm
+void _psread_lock_slow_irq(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+int _psread_trylock_slow_irq(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+void _psread_lock_slow_bh(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+int _psread_trylock_slow_bh(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+void _psread_lock_slow_inatomic(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+int _psread_trylock_slow_inatomic(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+void _psread_lock_slow(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+int _psread_lock_interruptible_slow(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+int _psread_trylock_slow(unsigned int uc, psrwlock_t *rwlock);
+
+extern asmregparm
+void _pswrite_lock_slow(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+int _pswrite_lock_interruptible_slow(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+int _pswrite_trylock_slow(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+void _pswrite_unlock_slow(unsigned int uc, psrwlock_t *rwlock);
+extern asmregparm
+void _psrwlock_wakeup(unsigned int uc, psrwlock_t *rwlock);
+
+#ifdef CONFIG_HAVE_PSRWLOCK_ASM_CALL
+#include <asm/call_64.h>
+#else
+#define psread_lock_slow_irq		_psread_lock_slow_irq
+#define psread_trylock_slow_irq		_psread_trylock_slow_irq
+#define psread_lock_slow_bh		_psread_lock_slow_bh
+#define psread_trylock_slow_bh		_psread_trylock_slow_bh
+#define psread_lock_slow_inatomic	_psread_lock_slow_inatomic
+#define psread_trylock_slow_inatomic	_psread_trylock_slow_inatomic
+#define psread_lock_slow		_psread_lock_slow
+#define psread_lock_interruptible_slow	_psread_lock_interruptible_slow
+#define psread_trylock_slow		_psread_trylock_slow
+
+#define pswrite_lock_slow		_pswrite_lock_slow
+#define pswrite_lock_interruptible_slow	_pswrite_lock_interruptible_slow
+#define pswrite_trylock_slow		_pswrite_trylock_slow
+#define pswrite_unlock_slow		_pswrite_unlock_slow
+#define psrwlock_wakeup			_psrwlock_wakeup
+#endif
+
+/*
+ * psrwlock-specific latency tracing, maps to standard macros by default.
+ */
+#ifdef CONFIG_PSRWLOCK_LATENCY_TEST
+#include <linux/psrwlock-latency-trace.h>
+#else
+static inline void psrwlock_profile_latency_reset(void)
+{ }
+static inline void psrwlock_profile_latency_print(void)
+{ }
+
+#define psrwlock_irq_save(flags)		local_irq_save(flags)
+#define psrwlock_irq_restore(flags)		local_irq_restore(flags)
+#define psrwlock_irq_disable()			local_irq_disable()
+#define psrwlock_irq_enable()			local_irq_enable()
+#define psrwlock_bh_disable()			local_bh_disable()
+#define psrwlock_bh_enable()			local_bh_enable()
+#define psrwlock_bh_enable_ip(ip)		local_bh_enable_ip(ip)
+#define psrwlock_preempt_disable()		preempt_disable()
+#define psrwlock_preempt_enable()		preempt_enable()
+#define psrwlock_preempt_enable_no_resched()	preempt_enable_no_resched()
+#endif
+
+/*
+ * Internal preemption/softirq/irq disabling helpers. Optimized into simple use
+ * of standard local_irq_disable, local_bh_disable, preempt_disable by the
+ * compiler since wctx and rctx are constant.
+ */
+
+static inline void write_context_disable(enum psrw_prio wctx, u32 rctx)
+{
+	if (wctx != PSRW_PRIO_IRQ && (rctx & PSR_IRQ))
+		psrwlock_irq_disable();
+	else if (wctx != PSRW_PRIO_BH && (rctx & PSR_BH))
+		psrwlock_bh_disable();
+	else if (wctx != PSRW_PRIO_NP && (rctx & PSR_NPTHREAD))
+		psrwlock_preempt_disable();
+}
+
+static inline void write_context_enable(enum psrw_prio wctx, u32 rctx)
+{
+	if (wctx != PSRW_PRIO_IRQ && (rctx & PSR_IRQ))
+		psrwlock_irq_enable();
+	else if (wctx != PSRW_PRIO_BH && (rctx & PSR_BH))
+		psrwlock_bh_enable();
+	else if (wctx != PSRW_PRIO_NP && (rctx & PSR_NPTHREAD))
+		psrwlock_preempt_enable();
+}
+
+/*
+ * psrwlock_preempt_check must have a uc parameter read with a memory
+ * barrier making sure the slow path variable writes and the UC_WQ_ACTIVE flag
+ * read are done in this order (either a smp_mb() or a atomic_sub_return()).
+ */
+static __always_inline void psrwlock_preempt_check(unsigned int uc,
+						   psrwlock_t *rwlock)
+{
+	if (unlikely(uc & UC_WQ_ACTIVE))
+		psrwlock_wakeup(uc, rwlock);
+}
+
+#ifdef CONFIG_DEBUG_PSRWLOCK
+# include <linux/psrwlock-debug-api.h>
+#else
+# include <linux/psrwlock-api.h>
+#endif
+
+#endif /* _LINUX_PSRWLOCK_H */
diff --git a/stblinux-2.6.31/include/linux/rculist.h b/stblinux-2.6.31/include/linux/rculist.h
index 5710f43..fb62b1e 100644
--- a/stblinux-2.6.31/include/linux/rculist.h
+++ b/stblinux-2.6.31/include/linux/rculist.h
@@ -229,6 +229,11 @@ static inline void list_splice_init_rcu(struct list_head *list,
 		pos != (head); \
 		pos = rcu_dereference(pos->next))
 
+#define __list_for_each_entry_rcu(pos, head, member) \
+	for (pos = list_entry(rcu_dereference((head)->next), typeof(*pos), member); \
+		&pos->member != (head); \
+		pos = list_entry(rcu_dereference(pos->member.next), typeof(*pos), member))
+
 /**
  * list_for_each_entry_rcu	-	iterate over rcu list of given type
  * @pos:	the type * to use as a loop cursor.
diff --git a/stblinux-2.6.31/include/linux/rcupdate.h b/stblinux-2.6.31/include/linux/rcupdate.h
index 15fbb3c..f5b5e62 100644
--- a/stblinux-2.6.31/include/linux/rcupdate.h
+++ b/stblinux-2.6.31/include/linux/rcupdate.h
@@ -49,6 +49,9 @@
 struct rcu_head {
 	struct rcu_head *next;
 	void (*func)(struct rcu_head *head);
+#ifdef CONFIG_DEBUG_RCU_HEAD
+	struct rcu_head *debug;
+#endif
 };
 
 /* Internal to kernel, but needed by rcupreempt.h. */
@@ -64,11 +67,19 @@ extern int rcu_scheduler_active;
 #error "Unknown RCU implementation specified to kernel configuration"
 #endif /* #else #if defined(CONFIG_CLASSIC_RCU) */
 
+#ifdef CONFIG_DEBUG_RCU_HEAD
+#define RCU_HEAD_INIT 	{ .next = NULL, .func = NULL, .debug = NULL }
+#define RCU_HEAD(head) struct rcu_head head = RCU_HEAD_INIT
+#define INIT_RCU_HEAD(ptr) do { \
+       (ptr)->next = NULL; (ptr)->func = NULL; (ptr)->debug = NULL; \
+} while (0)
+#else
 #define RCU_HEAD_INIT 	{ .next = NULL, .func = NULL }
 #define RCU_HEAD(head) struct rcu_head head = RCU_HEAD_INIT
 #define INIT_RCU_HEAD(ptr) do { \
        (ptr)->next = NULL; (ptr)->func = NULL; \
 } while (0)
+#endif
 
 /**
  * rcu_read_lock - mark the beginning of an RCU read-side critical section.
diff --git a/stblinux-2.6.31/include/linux/sched.h b/stblinux-2.6.31/include/linux/sched.h
index 1e174ca..e1c59d4 100644
--- a/stblinux-2.6.31/include/linux/sched.h
+++ b/stblinux-2.6.31/include/linux/sched.h
@@ -1361,6 +1361,9 @@ struct task_struct {
 	/* mutex deadlock detection */
 	struct mutex_waiter *blocked_on;
 #endif
+#ifdef CONFIG_DEBUG_PSRWLOCK
+	struct psrwlock_waiter *psrwlock_blocked_on;
+#endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	unsigned int irq_events;
 	int hardirqs_enabled;
@@ -2507,6 +2510,9 @@ static inline unsigned long rlimit_max(unsigned int limit)
 	return task_rlimit_max(current, limit);
 }
 
+extern void clear_kernel_trace_flag_all_tasks(void);
+extern void set_kernel_trace_flag_all_tasks(void);
+
 #endif /* __KERNEL__ */
 
 #endif
diff --git a/stblinux-2.6.31/include/linux/seq_file.h b/stblinux-2.6.31/include/linux/seq_file.h
index 0c6a86b..cd7f9e2 100644
--- a/stblinux-2.6.31/include/linux/seq_file.h
+++ b/stblinux-2.6.31/include/linux/seq_file.h
@@ -97,4 +97,24 @@ extern struct list_head *seq_list_start_head(struct list_head *head,
 extern struct list_head *seq_list_next(void *v, struct list_head *head,
 		loff_t *ppos);
 
+/*
+ * Helpers for iteration over a list sorted by ascending head pointer address.
+ * To be used in contexts where preemption cannot be disabled to insure to
+ * continue iteration on a modified list starting at the same location where it
+ * stopped, or at a following location. It insures that the lost information
+ * will only be in elements added/removed from the list between iterations.
+ * void *pos is only used to get the next list element and may not be a valid
+ * list_head anymore when given to seq_sorted_list_start() or
+ * seq_sorted_list_start_head().
+ */
+extern struct list_head *seq_sorted_list_start(struct list_head *head,
+		loff_t *ppos);
+extern struct list_head *seq_sorted_list_start_head(struct list_head *head,
+		loff_t *ppos);
+/*
+ * next must be called with an existing p node
+ */
+extern struct list_head *seq_sorted_list_next(void *p, struct list_head *head,
+		loff_t *ppos);
+
 #endif
diff --git a/stblinux-2.6.31/include/linux/swap.h b/stblinux-2.6.31/include/linux/swap.h
index 7c15334..1366818 100644
--- a/stblinux-2.6.31/include/linux/swap.h
+++ b/stblinux-2.6.31/include/linux/swap.h
@@ -335,6 +335,8 @@ static inline void mem_cgroup_uncharge_swap(swp_entry_t ent)
 }
 #endif
 
+extern void ltt_dump_swap_files(void *call_data);
+
 #else /* CONFIG_SWAP */
 
 #define nr_swap_pages				0L
@@ -429,6 +431,10 @@ mem_cgroup_uncharge_swapcache(struct page *page, swp_entry_t ent)
 {
 }
 
+static inline void ltt_dump_swap_files(void *call_data)
+{
+}
+
 #endif /* CONFIG_SWAP */
 #endif /* __KERNEL__*/
 #endif /* _LINUX_SWAP_H */
diff --git a/stblinux-2.6.31/include/linux/swapops.h b/stblinux-2.6.31/include/linux/swapops.h
index 6ec39ab..fb40dac 100644
--- a/stblinux-2.6.31/include/linux/swapops.h
+++ b/stblinux-2.6.31/include/linux/swapops.h
@@ -76,6 +76,14 @@ static inline pte_t swp_entry_to_pte(swp_entry_t entry)
 	return __swp_entry_to_pte(arch_entry);
 }
 
+static inline swp_entry_t page_swp_entry(struct page *page)
+{
+	swp_entry_t entry;
+	VM_BUG_ON(!PageSwapCache(page));
+	entry.val = page_private(page);
+	return entry;
+}
+
 #ifdef CONFIG_MIGRATION
 static inline swp_entry_t make_migration_entry(struct page *page, int write)
 {
diff --git a/stblinux-2.6.31/include/linux/trace-clock.h b/stblinux-2.6.31/include/linux/trace-clock.h
new file mode 100644
index 0000000..273991a
--- /dev/null
+++ b/stblinux-2.6.31/include/linux/trace-clock.h
@@ -0,0 +1,17 @@
+#ifndef _LINUX_TRACE_CLOCK_H
+#define _LINUX_TRACE_CLOCK_H
+
+/*
+ * Trace clock
+ *
+ * Chooses between an architecture specific clock or an atomic logical clock.
+ *
+ * Copyright (C) 2007,2008 Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ */
+
+#ifdef CONFIG_HAVE_TRACE_CLOCK
+#include <asm/trace-clock.h>
+#else
+#include <asm-generic/trace-clock.h>
+#endif /* CONFIG_HAVE_TRACE_CLOCK */
+#endif /* _LINUX_TRACE_CLOCK_H */
diff --git a/stblinux-2.6.31/include/linux/tracepoint.h b/stblinux-2.6.31/include/linux/tracepoint.h
index b9dc4ca..b75cd01 100644
--- a/stblinux-2.6.31/include/linux/tracepoint.h
+++ b/stblinux-2.6.31/include/linux/tracepoint.h
@@ -14,6 +14,7 @@
  * See the file COPYING for more details.
  */
 
+#include <linux/immediate.h>
 #include <linux/types.h>
 #include <linux/rcupdate.h>
 
@@ -22,7 +23,7 @@ struct tracepoint;
 
 struct tracepoint {
 	const char *name;		/* Tracepoint name */
-	int state;			/* State. */
+	DEFINE_IMV(char, state);	/* State. */
 	void **funcs;
 } __attribute__((aligned(32)));		/*
 					 * Aligned on 32 bytes because it is
@@ -36,6 +37,24 @@ struct tracepoint {
 #define TP_PROTO(args...)	args
 #define TP_ARGS(args...)		args
 
+#define DECLARE_TRACE_NOP(name, proto, args)				\
+	static inline void trace_##name(proto)				\
+	{ }								\
+	static inline void _trace_##name(proto)				\
+	{ }								\
+	static inline int register_trace_##name(void (*probe)(proto))	\
+	{								\
+		return -ENOSYS;						\
+	}								\
+	static inline int unregister_trace_##name(void (*probe)(proto))	\
+	{								\
+		return -ENOSYS;						\
+	}
+
+#define DEFINE_TRACE_NOP(name)
+#define EXPORT_TRACEPOINT_SYMBOL_GPL_NOP(name)
+#define EXPORT_TRACEPOINT_SYMBOL_NOP(name)
+
 #ifdef CONFIG_TRACEPOINTS
 
 /*
@@ -56,18 +75,38 @@ struct tracepoint {
 		rcu_read_unlock_sched_notrace();			\
 	} while (0)
 
+#define __CHECK_TRACE(name, generic, proto, args)			\
+	do {								\
+		if (!generic) {						\
+			if (unlikely(imv_read(__tracepoint_##name.state))) \
+				__DO_TRACE(&__tracepoint_##name,	\
+					TP_PROTO(proto), TP_ARGS(args));\
+		} else {						\
+			if (unlikely(_imv_read(__tracepoint_##name.state))) \
+				__DO_TRACE(&__tracepoint_##name,	\
+					TP_PROTO(proto), TP_ARGS(args));\
+		}							\
+	} while (0)
+
 /*
  * Make sure the alignment of the structure in the __tracepoints section will
  * not add unwanted padding between the beginning of the section and the
  * structure. Force alignment to the same alignment as the section start.
+ *
+ * The "generic" argument, passed to the declared __trace_##name inline
+ * function controls which tracepoint enabling mechanism must be used.
+ * If generic is true, a variable read is used.
+ * If generic is false, immediate values are used.
  */
 #define DECLARE_TRACE(name, proto, args)				\
 	extern struct tracepoint __tracepoint_##name;			\
 	static inline void trace_##name(proto)				\
 	{								\
-		if (unlikely(__tracepoint_##name.state))		\
-			__DO_TRACE(&__tracepoint_##name,		\
-				TP_PROTO(proto), TP_ARGS(args));	\
+		__CHECK_TRACE(name, 0, TP_PROTO(proto), TP_ARGS(args));	\
+	}								\
+	static inline void _trace_##name(proto)				\
+	{								\
+		__CHECK_TRACE(name, 1, TP_PROTO(proto), TP_ARGS(args));	\
 	}								\
 	static inline int register_trace_##name(void (*probe)(proto))	\
 	{								\
@@ -94,23 +133,11 @@ extern void tracepoint_update_probe_range(struct tracepoint *begin,
 	struct tracepoint *end);
 
 #else /* !CONFIG_TRACEPOINTS */
-#define DECLARE_TRACE(name, proto, args)				\
-	static inline void _do_trace_##name(struct tracepoint *tp, proto) \
-	{ }								\
-	static inline void trace_##name(proto)				\
-	{ }								\
-	static inline int register_trace_##name(void (*probe)(proto))	\
-	{								\
-		return -ENOSYS;						\
-	}								\
-	static inline int unregister_trace_##name(void (*probe)(proto))	\
-	{								\
-		return -ENOSYS;						\
-	}
 
-#define DEFINE_TRACE(name)
-#define EXPORT_TRACEPOINT_SYMBOL_GPL(name)
-#define EXPORT_TRACEPOINT_SYMBOL(name)
+#define DECLARE_TRACE			DECLARE_TRACE_NOP
+#define DEFINE_TRACE			DEFINE_TRACE_NOP
+#define EXPORT_TRACEPOINT_SYMBOL_GPL	EXPORT_TRACEPOINT_SYMBOL_GPL_NOP
+#define EXPORT_TRACEPOINT_SYMBOL	EXPORT_TRACEPOINT_SYMBOL_NOP
 
 static inline void tracepoint_update_probe_range(struct tracepoint *begin,
 	struct tracepoint *end)
diff --git a/stblinux-2.6.31/include/linux/vmalloc.h b/stblinux-2.6.31/include/linux/vmalloc.h
index a43ebec..948e1a0 100644
--- a/stblinux-2.6.31/include/linux/vmalloc.h
+++ b/stblinux-2.6.31/include/linux/vmalloc.h
@@ -67,7 +67,7 @@ extern void vunmap(const void *addr);
 
 extern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
 							unsigned long pgoff);
-void vmalloc_sync_all(void);
+extern void vmalloc_sync_all(void);
  
 /*
  *	Lowlevel-APIs (not for driver use!)
diff --git a/stblinux-2.6.31/include/net/dst.h b/stblinux-2.6.31/include/net/dst.h
index 7fc409c..f8d01a5 100644
--- a/stblinux-2.6.31/include/net/dst.h
+++ b/stblinux-2.6.31/include/net/dst.h
@@ -175,7 +175,9 @@ static inline void dst_hold(struct dst_entry * dst)
 	 * If your kernel compilation stops here, please check
 	 * __pad_to_align_refcnt declaration in struct dst_entry
 	 */
+#ifndef CONFIG_DEBUG_RCU_HEAD
 	BUILD_BUG_ON(offsetof(struct dst_entry, __refcnt) & 63);
+#endif
 	atomic_inc(&dst->__refcnt);
 }
 
diff --git a/stblinux-2.6.31/include/trace/fault.h b/stblinux-2.6.31/include/trace/fault.h
new file mode 100644
index 0000000..d23e551
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/fault.h
@@ -0,0 +1,27 @@
+#ifndef _TRACE_FAULT_H
+#define _TRACE_FAULT_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(page_fault_entry,
+	TP_PROTO(struct pt_regs *regs, int trapnr,
+			struct mm_struct *mm, struct vm_area_struct *vma,
+			unsigned long address, int write_access),
+		TP_ARGS(regs, trapnr, mm, vma, address, write_access));
+DECLARE_TRACE(page_fault_exit,
+	TP_PROTO(int res),
+		TP_ARGS(res));
+DECLARE_TRACE(page_fault_nosem_entry,
+	TP_PROTO(struct pt_regs *regs, int trapnr, unsigned long address),
+		TP_ARGS(regs, trapnr, address));
+DECLARE_TRACE(page_fault_nosem_exit,
+	TP_PROTO(void),
+		TP_ARGS());
+DECLARE_TRACE(page_fault_get_user_entry,
+	TP_PROTO(struct mm_struct *mm, struct vm_area_struct *vma,
+			unsigned long address, int write_access),
+		TP_ARGS(mm, vma, address, write_access));
+DECLARE_TRACE(page_fault_get_user_exit,
+	TP_PROTO(int res),
+		TP_ARGS(res));
+#endif
diff --git a/stblinux-2.6.31/include/trace/filemap.h b/stblinux-2.6.31/include/trace/filemap.h
new file mode 100644
index 0000000..14e90ba
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/filemap.h
@@ -0,0 +1,19 @@
+#ifndef _TRACE_FILEMAP_H
+#define _TRACE_FILEMAP_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(wait_on_page_start,
+	TP_PROTO(struct page *page, int bit_nr),
+		TP_ARGS(page, bit_nr));
+DECLARE_TRACE(wait_on_page_end,
+	TP_PROTO(struct page *page, int bit_nr),
+		TP_ARGS(page, bit_nr));
+DECLARE_TRACE(add_to_page_cache,
+	TP_PROTO(struct address_space *mapping, pgoff_t offset),
+		TP_ARGS(mapping, offset));
+DECLARE_TRACE(remove_from_page_cache,
+	TP_PROTO(struct address_space *mapping),
+		TP_ARGS(mapping));
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/fs.h b/stblinux-2.6.31/include/trace/fs.h
new file mode 100644
index 0000000..29c4ca6
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/fs.h
@@ -0,0 +1,66 @@
+#ifndef _TRACE_FS_H
+#define _TRACE_FS_H
+
+#include <linux/buffer_head.h>
+#include <linux/time.h>
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(fs_buffer_wait_start,
+	TP_PROTO(struct buffer_head *bh),
+	TP_ARGS(bh));
+DECLARE_TRACE(fs_buffer_wait_end,
+	TP_PROTO(struct buffer_head *bh),
+	TP_ARGS(bh));
+DECLARE_TRACE(fs_exec,
+	TP_PROTO(char *filename),
+	TP_ARGS(filename));
+DECLARE_TRACE(fs_ioctl,
+	TP_PROTO(unsigned int fd, unsigned int cmd, unsigned long arg),
+	TP_ARGS(fd, cmd, arg));
+DECLARE_TRACE(fs_open,
+	TP_PROTO(int fd, char *filename),
+	TP_ARGS(fd, filename));
+DECLARE_TRACE(fs_close,
+	TP_PROTO(unsigned int fd),
+	TP_ARGS(fd));
+DECLARE_TRACE(fs_lseek,
+	TP_PROTO(unsigned int fd, long offset, unsigned int origin),
+	TP_ARGS(fd, offset, origin));
+DECLARE_TRACE(fs_llseek,
+	TP_PROTO(unsigned int fd, loff_t offset, unsigned int origin),
+	TP_ARGS(fd, offset, origin));
+
+/*
+ * Probes must be aware that __user * may be modified by concurrent userspace
+ * or kernel threads.
+ */
+DECLARE_TRACE(fs_read,
+	TP_PROTO(unsigned int fd, char __user *buf, size_t count, ssize_t ret),
+	TP_ARGS(fd, buf, count, ret));
+DECLARE_TRACE(fs_write,
+	TP_PROTO(unsigned int fd, const char __user *buf, size_t count,
+		ssize_t ret),
+	TP_ARGS(fd, buf, count, ret));
+DECLARE_TRACE(fs_pread64,
+	TP_PROTO(unsigned int fd, char __user *buf, size_t count, loff_t pos,
+		ssize_t ret),
+	TP_ARGS(fd, buf, count, pos, ret));
+DECLARE_TRACE(fs_pwrite64,
+	TP_PROTO(unsigned int fd, const char __user *buf, size_t count,
+		loff_t pos, ssize_t ret),
+	TP_ARGS(fd, buf, count, pos, ret));
+DECLARE_TRACE(fs_readv,
+	TP_PROTO(unsigned long fd, const struct iovec __user *vec,
+		unsigned long vlen, ssize_t ret),
+	TP_ARGS(fd, vec, vlen, ret));
+DECLARE_TRACE(fs_writev,
+	TP_PROTO(unsigned long fd, const struct iovec __user *vec,
+		unsigned long vlen, ssize_t ret),
+	TP_ARGS(fd, vec, vlen, ret));
+DECLARE_TRACE(fs_select,
+	TP_PROTO(int fd, struct timespec *end_time),
+	TP_ARGS(fd, end_time));
+DECLARE_TRACE(fs_poll,
+	TP_PROTO(int fd),
+	TP_ARGS(fd));
+#endif
diff --git a/stblinux-2.6.31/include/trace/hugetlb.h b/stblinux-2.6.31/include/trace/hugetlb.h
new file mode 100644
index 0000000..c18944e
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/hugetlb.h
@@ -0,0 +1,28 @@
+#ifndef _TRACE_HUGETLB_H
+#define _TRACE_HUGETLB_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(hugetlb_page_release,
+	TP_PROTO(struct page *page),
+	TP_ARGS(page));
+DECLARE_TRACE(hugetlb_page_grab,
+	TP_PROTO(struct page *page),
+	TP_ARGS(page));
+DECLARE_TRACE(hugetlb_buddy_pgalloc,
+	TP_PROTO(struct page *page),
+	TP_ARGS(page));
+DECLARE_TRACE(hugetlb_page_alloc,
+	TP_PROTO(struct page *page),
+	TP_ARGS(page));
+DECLARE_TRACE(hugetlb_page_free,
+	TP_PROTO(struct page *page),
+	TP_ARGS(page));
+DECLARE_TRACE(hugetlb_pages_reserve,
+	TP_PROTO(struct inode *inode, long from, long to, int ret),
+	TP_ARGS(inode, from, to, ret));
+DECLARE_TRACE(hugetlb_pages_unreserve,
+	TP_PROTO(struct inode *inode, long offset, long freed),
+	TP_ARGS(inode, offset, freed));
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/ipc.h b/stblinux-2.6.31/include/trace/ipc.h
new file mode 100644
index 0000000..ea9dac1
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/ipc.h
@@ -0,0 +1,18 @@
+#ifndef _TRACE_IPC_H
+#define _TRACE_IPC_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(ipc_msg_create,
+	TP_PROTO(long id, int flags),
+	TP_ARGS(id, flags));
+DECLARE_TRACE(ipc_sem_create,
+	TP_PROTO(long id, int flags),
+	TP_ARGS(id, flags));
+DECLARE_TRACE(ipc_shm_create,
+	TP_PROTO(long id, int flags),
+	TP_ARGS(id, flags));
+DECLARE_TRACE(ipc_call,
+	TP_PROTO(unsigned int call, unsigned int first),
+	TP_ARGS(call, first));
+#endif
diff --git a/stblinux-2.6.31/include/trace/ipv4.h b/stblinux-2.6.31/include/trace/ipv4.h
new file mode 100644
index 0000000..388908a
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/ipv4.h
@@ -0,0 +1,14 @@
+#ifndef _TRACE_IPV4_H
+#define _TRACE_IPV4_H
+
+#include <linux/inetdevice.h>
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(ipv4_addr_add,
+	TP_PROTO(struct in_ifaddr *ifa),
+	TP_ARGS(ifa));
+DECLARE_TRACE(ipv4_addr_del,
+	TP_PROTO(struct in_ifaddr *ifa),
+	TP_ARGS(ifa));
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/ipv6.h b/stblinux-2.6.31/include/trace/ipv6.h
new file mode 100644
index 0000000..ffb9b11
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/ipv6.h
@@ -0,0 +1,14 @@
+#ifndef _TRACE_IPV6_H
+#define _TRACE_IPV6_H
+
+#include <net/if_inet6.h>
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(ipv6_addr_add,
+	TP_PROTO(struct inet6_ifaddr *ifa),
+	TP_ARGS(ifa));
+DECLARE_TRACE(ipv6_addr_del,
+	TP_PROTO(struct inet6_ifaddr *ifa),
+	TP_ARGS(ifa));
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/irq.h b/stblinux-2.6.31/include/trace/irq.h
new file mode 100644
index 0000000..da53828
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/irq.h
@@ -0,0 +1,35 @@
+#ifndef _LTTNG_TRACE_IRQ_H
+#define _LTTNG_TRACE_IRQ_H
+
+#include <linux/kdebug.h>
+#include <linux/interrupt.h>
+
+/*
+ * action can be NULL if not available.
+ */
+DECLARE_TRACE(irq_entry,
+	TP_PROTO(unsigned int id, struct pt_regs *regs,
+			struct irqaction *action),
+		TP_ARGS(id, regs, action));
+DECLARE_TRACE(irq_exit,
+	TP_PROTO(irqreturn_t retval),
+		TP_ARGS(retval));
+
+DECLARE_TRACE(irq_tasklet_low_entry,
+	TP_PROTO(struct tasklet_struct *t),
+		TP_ARGS(t));
+DECLARE_TRACE(irq_tasklet_low_exit,
+	TP_PROTO(struct tasklet_struct *t),
+		TP_ARGS(t));
+DECLARE_TRACE(irq_tasklet_high_entry,
+	TP_PROTO(struct tasklet_struct *t),
+		TP_ARGS(t));
+DECLARE_TRACE(irq_tasklet_high_exit,
+	TP_PROTO(struct tasklet_struct *t),
+		TP_ARGS(t));
+
+DECLARE_TRACE(softirq_raise,
+	TP_PROTO(unsigned int nr),
+		TP_ARGS(nr));
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/kernel.h b/stblinux-2.6.31/include/trace/kernel.h
new file mode 100644
index 0000000..ca61c54
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/kernel.h
@@ -0,0 +1,31 @@
+#ifndef _TRACE_KERNEL_H
+#define _TRACE_KERNEL_H
+
+#include <linux/tracepoint.h>
+#include <linux/kexec.h>
+
+struct kimage;
+
+DECLARE_TRACE(kernel_printk,
+	TP_PROTO(unsigned long retaddr),
+		TP_ARGS(retaddr));
+DECLARE_TRACE(kernel_vprintk,
+	TP_PROTO(unsigned long retaddr, char *buf, int len),
+		TP_ARGS(retaddr, buf, len));
+DECLARE_TRACE(kernel_module_free,
+	TP_PROTO(struct module *mod),
+		TP_ARGS(mod));
+DECLARE_TRACE(kernel_module_load,
+	TP_PROTO(struct module *mod),
+		TP_ARGS(mod));
+DECLARE_TRACE(kernel_panic,
+	TP_PROTO(const char *fmt, va_list args),
+		TP_ARGS(fmt, args));
+DECLARE_TRACE(kernel_kernel_kexec,
+	TP_PROTO(struct kimage *image),
+		TP_ARGS(image));
+DECLARE_TRACE(kernel_crash_kexec,
+	TP_PROTO(struct kimage *image, struct pt_regs *regs),
+		TP_ARGS(image, regs));
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/lockdep.h b/stblinux-2.6.31/include/trace/lockdep.h
new file mode 100644
index 0000000..dbd4629
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/lockdep.h
@@ -0,0 +1,37 @@
+#ifndef _LTTNG_TRACE_LOCKDEP_H
+#define _LTTNG_TRACE_LOCKDEP_H
+
+#include <linux/lockdep.h>
+#include <linux/tracepoint.h>
+
+/*
+ * lockdep tracing must be very careful with respect to reentrancy.
+ *
+ * It should not use immediate values for activation because it involves
+ * traps called when the code patching is done.
+ */
+DECLARE_TRACE(lockdep_hardirqs_on,
+	TP_PROTO(unsigned long retaddr),
+		TP_ARGS(retaddr));
+DECLARE_TRACE(lockdep_hardirqs_off,
+	TP_PROTO(unsigned long retaddr),
+		TP_ARGS(retaddr));
+DECLARE_TRACE(lockdep_softirqs_on,
+	TP_PROTO(unsigned long retaddr),
+		TP_ARGS(retaddr));
+DECLARE_TRACE(lockdep_softirqs_off,
+	TP_PROTO(unsigned long retaddr),
+		TP_ARGS(retaddr));
+
+/* FIXME : some duplication with lockdep TRACE EVENTs */
+DECLARE_TRACE(lockdep_lock_acquire,
+	TP_PROTO(unsigned long retaddr, unsigned int subclass,
+			struct lockdep_map *lock, int trylock, int read,
+			int hardirqs_off),
+		TP_ARGS(retaddr, subclass, lock, trylock, read, hardirqs_off));
+DECLARE_TRACE(lockdep_lock_release,
+	TP_PROTO(unsigned long retaddr, struct lockdep_map *lock, int nested),
+		TP_ARGS(retaddr, lock, nested));
+
+
+#endif /* _LTTNG_TRACE_LOCKDEP_H */
diff --git a/stblinux-2.6.31/include/trace/net.h b/stblinux-2.6.31/include/trace/net.h
new file mode 100644
index 0000000..5e774eb
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/net.h
@@ -0,0 +1,40 @@
+#ifndef _TRACE_NET_H
+#define _TRACE_NET_H
+
+#include <linux/tracepoint.h>
+
+struct sk_buff;
+DECLARE_TRACE(net_dev_xmit,
+	TP_PROTO(struct sk_buff *skb),
+	TP_ARGS(skb));
+DECLARE_TRACE(net_dev_receive,
+	TP_PROTO(struct sk_buff *skb),
+	TP_ARGS(skb));
+DECLARE_TRACE(net_tcpv4_rcv,
+	TP_PROTO(struct sk_buff *skb),
+	TP_ARGS(skb));
+DECLARE_TRACE(net_udpv4_rcv,
+	TP_PROTO(struct sk_buff *skb),
+	TP_ARGS(skb));
+
+/*
+ * Note these first 2 traces are actually in __napi_schedule and net_rx_action
+ * respectively.  The former is in __napi_schedule because it uses at-most-once
+ * logic and placing it in the calling routine (napi_schedule) would produce
+ * countless trace events that were effectively  no-ops.  napi_poll is
+ * implemented in net_rx_action, because thats where we do our polling on
+ * devices.  The last trace point is in napi_complete, right where you would
+ * think it would be.
+ */
+struct napi_struct;
+DECLARE_TRACE(net_napi_schedule,
+	TP_PROTO(struct napi_struct *n),
+	TP_ARGS(n));
+DECLARE_TRACE(net_napi_poll,
+	TP_PROTO(struct napi_struct *n),
+	TP_ARGS(n));
+DECLARE_TRACE(net_napi_complete,
+	TP_PROTO(struct napi_struct *n),
+	TP_ARGS(n));
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/page_alloc.h b/stblinux-2.6.31/include/trace/page_alloc.h
new file mode 100644
index 0000000..c30a389
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/page_alloc.h
@@ -0,0 +1,16 @@
+#ifndef _TRACE_PAGE_ALLOC_H
+#define _TRACE_PAGE_ALLOC_H
+
+#include <linux/tracepoint.h>
+
+/*
+ * mm_page_alloc : page can be NULL.
+ */
+DECLARE_TRACE(page_alloc,
+	TP_PROTO(struct page *page, unsigned int order),
+	TP_ARGS(page, order));
+DECLARE_TRACE(page_free,
+	TP_PROTO(struct page *page, unsigned int order),
+	TP_ARGS(page, order));
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/pm.h b/stblinux-2.6.31/include/trace/pm.h
new file mode 100644
index 0000000..c3529fa
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/pm.h
@@ -0,0 +1,19 @@
+#ifndef _TRACE_PM_H
+#define _TRACE_PM_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(pm_idle_entry,
+	TP_PROTO(void),
+		TP_ARGS());
+DECLARE_TRACE(pm_idle_exit,
+	TP_PROTO(void),
+		TP_ARGS());
+DECLARE_TRACE(pm_suspend_entry,
+	TP_PROTO(void),
+		TP_ARGS());
+DECLARE_TRACE(pm_suspend_exit,
+	TP_PROTO(void),
+		TP_ARGS());
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/rcu.h b/stblinux-2.6.31/include/trace/rcu.h
new file mode 100644
index 0000000..f551c2c
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/rcu.h
@@ -0,0 +1,43 @@
+#ifndef _TRACE_RCU_H
+#define _TRACE_RCU_H
+
+#include <linux/tracepoint.h>
+#include <linux/rcupdate.h>
+
+DECLARE_TRACE(rcu_classic_callback,
+	TP_PROTO(struct rcu_head *head),
+	TP_ARGS(head));
+
+DECLARE_TRACE(rcu_classic_call_rcu,
+	TP_PROTO(struct rcu_head *head, unsigned long ip),
+	TP_ARGS(head, ip));
+
+DECLARE_TRACE(rcu_classic_call_rcu_bh,
+	TP_PROTO(struct rcu_head *head, unsigned long ip),
+	TP_ARGS(head, ip));
+
+DECLARE_TRACE(rcu_preempt_callback,
+	TP_PROTO(struct rcu_head *head),
+	TP_ARGS(head));
+
+DECLARE_TRACE(rcu_preempt_call_rcu,
+	TP_PROTO(struct rcu_head *head, unsigned long ip),
+	TP_ARGS(head, ip));
+
+DECLARE_TRACE(rcu_preempt_call_rcu_sched,
+	TP_PROTO(struct rcu_head *head, unsigned long ip),
+	TP_ARGS(head, ip));
+
+DECLARE_TRACE(rcu_tree_callback,
+	TP_PROTO(struct rcu_head *head),
+	TP_ARGS(head));
+
+DECLARE_TRACE(rcu_tree_call_rcu,
+	TP_PROTO(struct rcu_head *head, unsigned long ip),
+	TP_ARGS(head, ip));
+
+DECLARE_TRACE(rcu_tree_call_rcu_bh,
+	TP_PROTO(struct rcu_head *head, unsigned long ip),
+	TP_ARGS(head, ip));
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/sched.h b/stblinux-2.6.31/include/trace/sched.h
new file mode 100644
index 0000000..a4b0307
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/sched.h
@@ -0,0 +1,11 @@
+#ifndef _LTTNG_TRACE_SCHED_H
+#define _LTTNG_TRACE_SCHED_H
+
+#include <linux/sched.h>
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(sched_kthread_create,
+	TP_PROTO(void *fn, int pid),
+		TP_ARGS(fn, pid));
+
+#endif /* _LTTNG_TRACE_SCHED_H */
diff --git a/stblinux-2.6.31/include/trace/socket.h b/stblinux-2.6.31/include/trace/socket.h
new file mode 100644
index 0000000..4e8a324
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/socket.h
@@ -0,0 +1,77 @@
+#ifndef _TRACE_SOCKET_H
+#define _TRACE_SOCKET_H
+
+#include <net/sock.h>
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(socket_create,
+	TP_PROTO(int family, int type, int protocol, struct socket *sock,
+	int ret),
+	TP_ARGS(family, type, protocol, sock, ret));
+
+DECLARE_TRACE(socket_bind,
+	TP_PROTO(int fd, struct sockaddr __user *umyaddr, int addrlen, int ret),
+	TP_ARGS(fd, umyaddr, addrlen, ret));
+
+DECLARE_TRACE(socket_connect,
+	TP_PROTO(int fd, struct sockaddr __user *uservaddr, int addrlen,
+	int ret),
+	TP_ARGS(fd, uservaddr, addrlen, ret));
+
+DECLARE_TRACE(socket_listen,
+	TP_PROTO(int fd, int backlog, int ret),
+	TP_ARGS(fd, backlog, ret));
+
+DECLARE_TRACE(socket_accept,
+	TP_PROTO(int fd, struct sockaddr __user *upeer_sockaddr,
+	int __user *upeer_addrlen, int flags, int ret),
+	TP_ARGS(fd, upeer_sockaddr, upeer_addrlen, flags, ret));
+
+DECLARE_TRACE(socket_getsockname,
+	TP_PROTO(int fd, struct sockaddr __user *usockaddr,
+	int __user *usockaddr_len, int ret),
+	TP_ARGS(fd, usockaddr, usockaddr_len, ret));
+
+DECLARE_TRACE(socket_getpeername,
+	TP_PROTO(int fd, struct sockaddr __user *usockaddr,
+	int __user *usockaddr_len, int ret),
+	TP_ARGS(fd, usockaddr, usockaddr_len, ret));
+
+DECLARE_TRACE(socket_socketpair,
+	TP_PROTO(int family, int type, int protocol, int __user *usockvec,
+	int ret),
+	TP_ARGS(family, type, protocol, usockvec, ret));
+
+DECLARE_TRACE(socket_sendmsg,
+	TP_PROTO(struct socket *sock, struct msghdr *msg, size_t size, int ret),
+	TP_ARGS(sock, msg, size, ret));
+
+DECLARE_TRACE(socket_recvmsg,
+	TP_PROTO(struct socket *sock, struct msghdr *msg, size_t size,
+		int flags, int ret),
+	TP_ARGS(sock, msg, size, flags, ret));
+
+DECLARE_TRACE(socket_setsockopt,
+	TP_PROTO(int fd, int level, int optname, char __user *optval,
+	int optlen, int ret),
+	TP_ARGS(fd, level, optname, optval, optlen, ret));
+
+DECLARE_TRACE(socket_getsockopt,
+	TP_PROTO(int fd, int level, int optname, char __user *optval,
+	int __user *optlen, int ret),
+	TP_ARGS(fd, level, optname, optval, optlen, ret));
+
+DECLARE_TRACE(socket_shutdown,
+	TP_PROTO(int fd, int how, int ret),
+	TP_ARGS(fd, how, ret));
+
+/*
+ * socket_call
+ *
+ * We also trace socket_call so we can know which syscall is used by user
+ * (socket_call or sock_send...)
+ */
+DECLARE_TRACE(socket_call,
+	TP_PROTO(int call, unsigned long a0),
+	TP_ARGS(call, a0));
+#endif
diff --git a/stblinux-2.6.31/include/trace/swap.h b/stblinux-2.6.31/include/trace/swap.h
new file mode 100644
index 0000000..bd035a7
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/swap.h
@@ -0,0 +1,20 @@
+#ifndef _TRACE_SWAP_H
+#define _TRACE_SWAP_H
+
+#include <linux/swap.h>
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(swap_in,
+	TP_PROTO(struct page *page, swp_entry_t entry),
+		TP_ARGS(page, entry));
+DECLARE_TRACE(swap_out,
+	TP_PROTO(struct page *page),
+		TP_ARGS(page));
+DECLARE_TRACE(swap_file_open,
+	TP_PROTO(struct file *file, char *filename),
+		TP_ARGS(file, filename));
+DECLARE_TRACE(swap_file_close,
+	TP_PROTO(struct file *file),
+		TP_ARGS(file));
+
+#endif
diff --git a/stblinux-2.6.31/include/trace/syscall.h b/stblinux-2.6.31/include/trace/syscall.h
index 8cfe515..dd5f366 100644
--- a/stblinux-2.6.31/include/trace/syscall.h
+++ b/stblinux-2.6.31/include/trace/syscall.h
@@ -2,6 +2,7 @@
 #define _TRACE_SYSCALL_H
 
 #include <asm/ptrace.h>
+#include <linux/tracepoint.h>
 
 /*
  * A syscall entry in the ftrace syscalls array.
@@ -32,4 +33,11 @@ static inline void ftrace_syscall_enter(struct pt_regs *regs)	{ }
 static inline void ftrace_syscall_exit(struct pt_regs *regs)	{ }
 #endif
 
+DECLARE_TRACE(syscall_entry,
+	TP_PROTO(struct pt_regs *regs, long id),
+	TP_ARGS(regs, id));
+DECLARE_TRACE(syscall_exit,
+	TP_PROTO(long ret),
+	TP_ARGS(ret));
+
 #endif /* _TRACE_SYSCALL_H */
diff --git a/stblinux-2.6.31/include/trace/timer.h b/stblinux-2.6.31/include/trace/timer.h
new file mode 100644
index 0000000..9b2a852
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/timer.h
@@ -0,0 +1,24 @@
+#ifndef _TRACE_TIMER_H
+#define _TRACE_TIMER_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(timer_itimer_expired,
+	TP_PROTO(struct signal_struct *sig),
+		TP_ARGS(sig));
+DECLARE_TRACE(timer_itimer_set,
+	TP_PROTO(int which, struct itimerval *value),
+		TP_ARGS(which, value));
+DECLARE_TRACE(timer_set,
+	TP_PROTO(struct timer_list *timer),
+		TP_ARGS(timer));
+/*
+ * xtime_lock is taken when kernel_timer_update_time tracepoint is reached.
+ */
+DECLARE_TRACE(timer_update_time,
+	TP_PROTO(struct timespec *_xtime, struct timespec *_wall_to_monotonic),
+		TP_ARGS(_xtime, _wall_to_monotonic));
+DECLARE_TRACE(timer_timeout,
+	TP_PROTO(struct task_struct *p),
+		TP_ARGS(p));
+#endif
diff --git a/stblinux-2.6.31/include/trace/trap.h b/stblinux-2.6.31/include/trace/trap.h
new file mode 100644
index 0000000..aa98463
--- /dev/null
+++ b/stblinux-2.6.31/include/trace/trap.h
@@ -0,0 +1,13 @@
+#ifndef _TRACE_TRAP_H
+#define _TRACE_TRAP_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_TRACE(trap_entry,
+	TP_PROTO(struct pt_regs *regs, long id),
+	TP_ARGS(regs, id));
+DECLARE_TRACE(trap_exit,
+	TP_PROTO(void),
+	TP_ARGS());
+
+#endif
diff --git a/stblinux-2.6.31/init/Kconfig b/stblinux-2.6.31/init/Kconfig
index 3f7e609..e8ebeb0 100644
--- a/stblinux-2.6.31/init/Kconfig
+++ b/stblinux-2.6.31/init/Kconfig
@@ -441,6 +441,41 @@ config LOG_BUF_SHIFT
 config HAVE_UNSTABLE_SCHED_CLOCK
 	bool
 
+#
+# Architectures with a 64-bits get_cycles() should select this.
+# They should also define
+# get_cycles_barrier() : instruction synchronization barrier if required
+# get_cycles_rate() : cycle counter rate, in HZ. If 0, TSC are not synchronized
+# across CPUs or their frequency may vary due to frequency scaling.
+#
+config HAVE_GET_CYCLES
+	def_bool n
+
+#
+# Architectures with a specialized tracing clock should select this.
+#
+config HAVE_TRACE_CLOCK
+	def_bool n
+
+config HAVE_TRACE_CLOCK_GENERIC
+	bool
+	default y if (!HAVE_TRACE_CLOCK)
+	default n if HAVE_TRACE_CLOCK
+	select HAVE_TRACE_CLOCK_32_TO_64 if (!64BIT)
+
+#
+# Architectures with only a 32-bits clock source should select this.
+#
+config HAVE_TRACE_CLOCK_32_TO_64
+	def_bool n
+
+#
+# Architectures which need to dynamically detect if their TSC is unsynchronized
+# across cpus should select this.
+#
+config HAVE_UNSYNCHRONIZED_TSC
+	def_bool n
+
 config GROUP_SCHED
 	bool "Group CPU scheduler"
 	depends on EXPERIMENTAL
@@ -1093,6 +1128,26 @@ config SLOW_WORK
 
 	  See Documentation/slow-work.txt.
 
+source "ltt/Kconfig"
+
+config HAVE_IMMEDIATE
+	def_bool n
+
+config IMMEDIATE
+	default y
+	depends on HAVE_IMMEDIATE
+	bool "Immediate value optimization" if EMBEDDED
+	help
+	  Immediate values are used as read-mostly variables that are rarely
+	  updated. They use code patching to modify the values inscribed in the
+	  instruction stream. It provides a way to save precious cache lines
+	  that would otherwise have to be used by these variables. They can be
+	  disabled through the EMBEDDED menu.
+
+	  It consumes slightly more memory and modifies the instruction stream
+	  each time any specially-marked variable is updated. Should really be
+	  disabled for embedded systems with read-only text.
+
 endmenu		# General setup
 
 config HAVE_GENERIC_DMA_COHERENT
diff --git a/stblinux-2.6.31/init/main.c b/stblinux-2.6.31/init/main.c
index 7901959..eda6055 100644
--- a/stblinux-2.6.31/init/main.c
+++ b/stblinux-2.6.31/init/main.c
@@ -96,6 +96,11 @@ static inline void mark_rodata_ro(void) { }
 #ifdef CONFIG_TC
 extern void tc_init(void);
 #endif
+#ifdef USE_IMMEDIATE
+extern void imv_init_complete(void);
+#else
+static inline void imv_init_complete(void) { }
+#endif
 
 enum system_states system_state __read_mostly;
 EXPORT_SYMBOL(system_state);
@@ -562,6 +567,7 @@ asmlinkage void __init start_kernel(void)
 	boot_init_stack_canary();
 
 	cgroup_init_early();
+	core_imv_update();
 
 	local_irq_disable();
 	early_boot_irqs_off();
@@ -703,6 +709,7 @@ asmlinkage void __init start_kernel(void)
 	cpuset_init();
 	taskstats_init_early();
 	delayacct_init();
+	imv_init_complete();
 
 	check_bugs();
 
@@ -832,6 +839,7 @@ static noinline int init_post(void)
 {
 	/* need to finish all async __init code before freeing the memory */
 	async_synchronize_full();
+	imv_unref_core_init();
 	free_initmem();
 	unlock_kernel();
 	mark_rodata_ro();
diff --git a/stblinux-2.6.31/ipc/msg.c b/stblinux-2.6.31/ipc/msg.c
index 779f762..0920538 100644
--- a/stblinux-2.6.31/ipc/msg.c
+++ b/stblinux-2.6.31/ipc/msg.c
@@ -38,6 +38,7 @@
 #include <linux/rwsem.h>
 #include <linux/nsproxy.h>
 #include <linux/ipc_namespace.h>
+#include <trace/ipc.h>
 
 #include <asm/current.h>
 #include <asm/uaccess.h>
@@ -72,6 +73,8 @@ struct msg_sender {
 
 #define msg_unlock(msq)		ipc_unlock(&(msq)->q_perm)
 
+DEFINE_TRACE(ipc_msg_create);
+
 static void freeque(struct ipc_namespace *, struct kern_ipc_perm *);
 static int newque(struct ipc_namespace *, struct ipc_params *);
 #ifdef CONFIG_PROC_FS
@@ -315,6 +318,7 @@ SYSCALL_DEFINE2(msgget, key_t, key, int, msgflg)
 	struct ipc_namespace *ns;
 	struct ipc_ops msg_ops;
 	struct ipc_params msg_params;
+	long ret;
 
 	ns = current->nsproxy->ipc_ns;
 
@@ -325,7 +329,9 @@ SYSCALL_DEFINE2(msgget, key_t, key, int, msgflg)
 	msg_params.key = key;
 	msg_params.flg = msgflg;
 
-	return ipcget(ns, &msg_ids(ns), &msg_ops, &msg_params);
+	ret = ipcget(ns, &msg_ids(ns), &msg_ops, &msg_params);
+	trace_ipc_msg_create(ret, msgflg);
+	return ret;
 }
 
 static inline unsigned long
diff --git a/stblinux-2.6.31/ipc/sem.c b/stblinux-2.6.31/ipc/sem.c
index 2f2a479..9dc0809 100644
--- a/stblinux-2.6.31/ipc/sem.c
+++ b/stblinux-2.6.31/ipc/sem.c
@@ -83,6 +83,7 @@
 #include <linux/rwsem.h>
 #include <linux/nsproxy.h>
 #include <linux/ipc_namespace.h>
+#include <trace/ipc.h>
 
 #include <asm/uaccess.h>
 #include "util.h"
@@ -115,6 +116,8 @@ static int sysvipc_sem_proc_show(struct seq_file *s, void *it);
 #define sc_semopm	sem_ctls[2]
 #define sc_semmni	sem_ctls[3]
 
+DEFINE_TRACE(ipc_sem_create);
+
 void sem_init_ns(struct ipc_namespace *ns)
 {
 	ns->sc_semmsl = SEMMSL;
@@ -314,6 +317,7 @@ SYSCALL_DEFINE3(semget, key_t, key, int, nsems, int, semflg)
 	struct ipc_namespace *ns;
 	struct ipc_ops sem_ops;
 	struct ipc_params sem_params;
+	long err;
 
 	ns = current->nsproxy->ipc_ns;
 
@@ -328,7 +332,9 @@ SYSCALL_DEFINE3(semget, key_t, key, int, nsems, int, semflg)
 	sem_params.flg = semflg;
 	sem_params.u.nsems = nsems;
 
-	return ipcget(ns, &sem_ids(ns), &sem_ops, &sem_params);
+	err = ipcget(ns, &sem_ids(ns), &sem_ops, &sem_params);
+	trace_ipc_sem_create(err, semflg);
+	return err;
 }
 
 /*
diff --git a/stblinux-2.6.31/ipc/shm.c b/stblinux-2.6.31/ipc/shm.c
index 9f779ab..747d355 100644
--- a/stblinux-2.6.31/ipc/shm.c
+++ b/stblinux-2.6.31/ipc/shm.c
@@ -40,6 +40,7 @@
 #include <linux/mount.h>
 #include <linux/ipc_namespace.h>
 #include <linux/ima.h>
+#include <trace/ipc.h>
 
 #include <asm/uaccess.h>
 
@@ -57,6 +58,8 @@ struct shm_file_data {
 static const struct file_operations shm_file_operations;
 static struct vm_operations_struct shm_vm_ops;
 
+DEFINE_TRACE(ipc_shm_create);
+
 #define shm_ids(ns)	((ns)->ids[IPC_SHM_IDS])
 
 #define shm_unlock(shp)			\
@@ -451,6 +454,7 @@ SYSCALL_DEFINE3(shmget, key_t, key, size_t, size, int, shmflg)
 	struct ipc_namespace *ns;
 	struct ipc_ops shm_ops;
 	struct ipc_params shm_params;
+	long err;
 
 	ns = current->nsproxy->ipc_ns;
 
@@ -462,7 +466,9 @@ SYSCALL_DEFINE3(shmget, key_t, key, size_t, size, int, shmflg)
 	shm_params.flg = shmflg;
 	shm_params.u.size = size;
 
-	return ipcget(ns, &shm_ids(ns), &shm_ops, &shm_params);
+	err = ipcget(ns, &shm_ids(ns), &shm_ops, &shm_params);
+	trace_ipc_shm_create(err, shmflg);
+	return err;
 }
 
 static inline unsigned long copy_shmid_to_user(void __user *buf, struct shmid64_ds *in, int version)
diff --git a/stblinux-2.6.31/kernel/Makefile b/stblinux-2.6.31/kernel/Makefile
index 2093a69..7167c3f 100644
--- a/stblinux-2.6.31/kernel/Makefile
+++ b/stblinux-2.6.31/kernel/Makefile
@@ -89,6 +89,7 @@ obj-$(CONFIG_RELAY) += relay.o
 obj-$(CONFIG_SYSCTL) += utsname_sysctl.o
 obj-$(CONFIG_TASK_DELAY_ACCT) += delayacct.o
 obj-$(CONFIG_TASKSTATS) += taskstats.o tsacct.o
+obj-$(USE_IMMEDIATE) += immediate.o
 obj-$(CONFIG_MARKERS) += marker.o
 obj-$(CONFIG_TRACEPOINTS) += tracepoint.o
 obj-$(CONFIG_LATENCYTOP) += latencytop.o
@@ -97,6 +98,8 @@ obj-$(CONFIG_FUNCTION_TRACER) += trace/
 obj-$(CONFIG_TRACING) += trace/
 obj-$(CONFIG_X86_DS) += trace/
 obj-$(CONFIG_RING_BUFFER) += trace/
+obj-$(CONFIG_HAVE_TRACE_CLOCK_32_TO_64) += trace/
+obj-$(CONFIG_HAVE_TRACE_CLOCK_GENERIC) += trace/
 obj-$(CONFIG_SMP) += sched_cpupri.o
 obj-$(CONFIG_SLOW_WORK) += slow-work.o
 obj-$(CONFIG_PERF_COUNTERS) += perf_counter.o
diff --git a/stblinux-2.6.31/kernel/exit.c b/stblinux-2.6.31/kernel/exit.c
index b8606f0..09aa546 100644
--- a/stblinux-2.6.31/kernel/exit.c
+++ b/stblinux-2.6.31/kernel/exit.c
@@ -508,6 +508,8 @@ struct files_struct *get_files_struct(struct task_struct *task)
 	return files;
 }
 
+EXPORT_SYMBOL(get_files_struct);
+
 void put_files_struct(struct files_struct *files)
 {
 	struct fdtable *fdt;
@@ -527,6 +529,8 @@ void put_files_struct(struct files_struct *files)
 	}
 }
 
+EXPORT_SYMBOL(put_files_struct);
+
 void reset_files_struct(struct files_struct *files)
 {
 	struct task_struct *tsk = current;
diff --git a/stblinux-2.6.31/kernel/fork.c b/stblinux-2.6.31/kernel/fork.c
index 4b36858..5bca054 100644
--- a/stblinux-2.6.31/kernel/fork.c
+++ b/stblinux-2.6.31/kernel/fork.c
@@ -83,6 +83,7 @@ int max_threads;		/* tunable limit on nr_threads */
 DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 
 __cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
+EXPORT_SYMBOL(tasklist_lock);
 
 int nr_processes(void)
 {
@@ -1018,6 +1019,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->rcu_read_lock_nesting = 0;
 	p->rcu_flipctr_idx = 0;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
+	INIT_RCU_HEAD(&p->rcu);
 	p->vfork_done = NULL;
 	spin_lock_init(&p->alloc_lock);
 
@@ -1204,6 +1206,15 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			!cpu_online(task_cpu(p))))
 		set_task_cpu(p, smp_processor_id());
 
+	/*
+	 * The state of the parent's TIF_KTRACE flag may have changed
+	 * since it was copied in dup_task_struct() so we re-copy it here.
+	 */
+	if (test_thread_flag(TIF_KERNEL_TRACE))
+		set_tsk_thread_flag(p, TIF_KERNEL_TRACE);
+	else
+		clear_tsk_thread_flag(p, TIF_KERNEL_TRACE);
+
 	/* CLONE_PARENT re-uses the old parent */
 	if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
 		p->real_parent = current->real_parent;
diff --git a/stblinux-2.6.31/kernel/immediate.c b/stblinux-2.6.31/kernel/immediate.c
new file mode 100644
index 0000000..ca8ae2c
--- /dev/null
+++ b/stblinux-2.6.31/kernel/immediate.c
@@ -0,0 +1,137 @@
+/*
+ * Copyright (C) 2007 Mathieu Desnoyers
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/immediate.h>
+#include <linux/memory.h>
+#include <linux/cpu.h>
+
+#include <asm/sections.h>
+
+/*
+ * Kernel ready to execute the SMP update that may depend on trap and ipi.
+ */
+static int imv_early_boot_complete;
+
+extern struct __imv __start___imv[];
+extern struct __imv __stop___imv[];
+
+/*
+ * imv_mutex nests inside module_mutex. imv_mutex protects builtin
+ * immediates and module immediates.
+ */
+static DEFINE_MUTEX(imv_mutex);
+
+/**
+ * imv_update_range - Update immediate values in a range
+ * @begin: pointer to the beginning of the range
+ * @end: pointer to the end of the range
+ *
+ * Updates a range of immediates.
+ */
+void imv_update_range(const struct __imv *begin,
+		const struct __imv *end)
+{
+	const struct __imv *iter;
+	int ret;
+
+	mutex_lock(&imv_mutex);
+	for (iter = begin; iter < end; iter++) {
+		if (!iter->imv) /* Skip removed __init immediate values */
+			continue;
+		/* workaround on_each_cpu cpu hotplug race */
+		get_online_cpus();
+		mutex_lock(&text_mutex);
+		ret = arch_imv_update(iter, !imv_early_boot_complete);
+		mutex_unlock(&text_mutex);
+		put_online_cpus();
+		if (imv_early_boot_complete && ret)
+			printk(KERN_WARNING
+				"Invalid immediate value. "
+				"Variable at %p, "
+				"instruction at %p, size %hu\n",
+				(void *)iter->imv,
+				(void *)iter->var, iter->size);
+	}
+	mutex_unlock(&imv_mutex);
+}
+EXPORT_SYMBOL_GPL(imv_update_range);
+
+/**
+ * imv_update - update all immediate values in the kernel
+ *
+ * Iterate on the kernel core and modules to update the immediate values.
+ */
+void core_imv_update(void)
+{
+	/* Core kernel imvs */
+	imv_update_range(__start___imv, __stop___imv);
+}
+EXPORT_SYMBOL_GPL(core_imv_update);
+
+/**
+ * imv_unref
+ * @begin: pointer to the beginning of the range
+ * @end: pointer to the end of the range
+ * @start: beginning of the region to consider
+ * @size: size of the region to consider
+ *
+ * Deactivate any immediate value reference pointing into the code region in the
+ * range start to start + size.
+ */
+void imv_unref(struct __imv *begin, struct __imv *end, void *start,
+		unsigned long size)
+{
+	struct __imv *iter;
+
+	for (iter = begin; iter < end; iter++)
+		if (iter->imv >= (unsigned long)start
+			&& iter->imv < (unsigned long)start + size)
+			iter->imv = 0UL;
+}
+
+void imv_unref_core_init(void)
+{
+	imv_unref(__start___imv, __stop___imv, __init_begin,
+		(unsigned long)__init_end - (unsigned long)__init_begin);
+}
+
+void __init imv_init_complete(void)
+{
+	imv_early_boot_complete = 1;
+}
+
+#ifdef CONFIG_MODULES
+
+int imv_module_notify(struct notifier_block *self,
+		      unsigned long val, void *data)
+{
+	struct module *mod = data;
+
+	switch (val) {
+	case MODULE_STATE_COMING:
+		imv_update_range(mod->immediate,
+				 mod->immediate + mod->num_immediate);
+		break;
+	case MODULE_STATE_GOING:
+		/* All references will be gone, no update required. */
+		break;
+	}
+	return 0;
+}
+
+struct notifier_block imv_module_nb = {
+	.notifier_call = imv_module_notify,
+	.priority = 0,
+};
+
+static int init_imv(void)
+{
+	return register_module_notifier(&imv_module_nb);
+}
+__initcall(init_imv);
+
+#endif /* CONFIG_MODULES */
diff --git a/stblinux-2.6.31/kernel/irq/handle.c b/stblinux-2.6.31/kernel/irq/handle.c
index 065205b..6c324b3 100644
--- a/stblinux-2.6.31/kernel/irq/handle.c
+++ b/stblinux-2.6.31/kernel/irq/handle.c
@@ -20,6 +20,7 @@
 #include <linux/hash.h>
 #include <linux/bootmem.h>
 #include <trace/events/irq.h>
+#include <trace/irq.h>
 
 #include "internals.h"
 
@@ -184,6 +185,7 @@ int __init early_irq_init(void)
 
 	return arch_early_irq_init();
 }
+EXPORT_SYMBOL(irq_to_desc);
 
 struct irq_desc *irq_to_desc(unsigned int irq)
 {
@@ -269,6 +271,7 @@ int __init early_irq_init(void)
 	}
 	return arch_early_irq_init();
 }
+EXPORT_SYMBOL(irq_to_desc);
 
 struct irq_desc *irq_to_desc(unsigned int irq)
 {
@@ -356,6 +359,9 @@ static void warn_no_thread(unsigned int irq, struct irqaction *action)
 	       "but no thread function available.", irq, action->name);
 }
 
+DEFINE_TRACE(irq_entry);
+DEFINE_TRACE(irq_exit);
+
 /**
  * handle_IRQ_event - irq action chain handler
  * @irq:	the interrupt number
@@ -368,6 +374,8 @@ irqreturn_t handle_IRQ_event(unsigned int irq, struct irqaction *action)
 	irqreturn_t ret, retval = IRQ_NONE;
 	unsigned int status = 0;
 
+	trace_irq_entry(irq, NULL, action);
+
 	if (!(action->flags & IRQF_DISABLED))
 		local_irq_enable_in_hardirq();
 
@@ -424,6 +432,8 @@ irqreturn_t handle_IRQ_event(unsigned int irq, struct irqaction *action)
 		add_interrupt_randomness(irq);
 	local_irq_disable();
 
+	trace_irq_exit(retval);
+
 	return retval;
 }
 
diff --git a/stblinux-2.6.31/kernel/itimer.c b/stblinux-2.6.31/kernel/itimer.c
index 58762f7..0913601 100644
--- a/stblinux-2.6.31/kernel/itimer.c
+++ b/stblinux-2.6.31/kernel/itimer.c
@@ -12,9 +12,13 @@
 #include <linux/time.h>
 #include <linux/posix-timers.h>
 #include <linux/hrtimer.h>
+#include <trace/timer.h>
 
 #include <asm/uaccess.h>
 
+DEFINE_TRACE(timer_itimer_expired);
+DEFINE_TRACE(timer_itimer_set);
+
 /**
  * itimer_get_remtime - get remaining time for the timer
  *
@@ -123,6 +127,8 @@ enum hrtimer_restart it_real_fn(struct hrtimer *timer)
 	struct signal_struct *sig =
 		container_of(timer, struct signal_struct, real_timer);
 
+	trace_timer_itimer_expired(sig);
+
 	kill_pid_info(SIGALRM, SEND_SIG_PRIV, sig->leader_pid);
 
 	return HRTIMER_NORESTART;
@@ -148,6 +154,8 @@ int do_setitimer(int which, struct itimerval *value, struct itimerval *ovalue)
 	    !timeval_valid(&value->it_interval))
 		return -EINVAL;
 
+	trace_timer_itimer_set(which, value);
+
 	switch (which) {
 	case ITIMER_REAL:
 again:
diff --git a/stblinux-2.6.31/kernel/kallsyms.c b/stblinux-2.6.31/kernel/kallsyms.c
index 3a29dbe..c1a1609 100644
--- a/stblinux-2.6.31/kernel/kallsyms.c
+++ b/stblinux-2.6.31/kernel/kallsyms.c
@@ -180,6 +180,7 @@ unsigned long kallsyms_lookup_name(const char *name)
 	}
 	return module_kallsyms_lookup_name(name);
 }
+EXPORT_SYMBOL_GPL(kallsyms_lookup_name);
 
 int kallsyms_on_each_symbol(int (*fn)(void *, const char *, struct module *,
 				      unsigned long),
diff --git a/stblinux-2.6.31/kernel/kexec.c b/stblinux-2.6.31/kernel/kexec.c
index f336e21..f84d040 100644
--- a/stblinux-2.6.31/kernel/kexec.c
+++ b/stblinux-2.6.31/kernel/kexec.c
@@ -31,6 +31,7 @@
 #include <linux/cpu.h>
 #include <linux/console.h>
 #include <linux/vmalloc.h>
+#include <trace/kernel.h>
 
 #include <asm/page.h>
 #include <asm/uaccess.h>
@@ -38,6 +39,9 @@
 #include <asm/system.h>
 #include <asm/sections.h>
 
+DEFINE_TRACE(kernel_kernel_kexec);
+DEFINE_TRACE(kernel_crash_kexec);
+
 /* Per cpu memory for storing cpu states in case of system crash. */
 note_buf_t* crash_notes;
 
@@ -1062,6 +1066,8 @@ asmlinkage long compat_sys_kexec_load(unsigned long entry,
 
 void crash_kexec(struct pt_regs *regs)
 {
+	trace_kernel_crash_kexec(kexec_crash_image, regs);
+
 	/* Take the kexec_mutex here to prevent sys_kexec_load
 	 * running on one cpu from replacing the crash kernel
 	 * we are using after a panic on a different cpu.
@@ -1431,6 +1437,8 @@ int kernel_kexec(void)
 {
 	int error = 0;
 
+	trace_kernel_kernel_kexec(kexec_image);
+
 	if (!mutex_trylock(&kexec_mutex))
 		return -EBUSY;
 	if (!kexec_image) {
diff --git a/stblinux-2.6.31/kernel/ksysfs.c b/stblinux-2.6.31/kernel/ksysfs.c
index 528dd78..5c7b1f1 100644
--- a/stblinux-2.6.31/kernel/ksysfs.c
+++ b/stblinux-2.6.31/kernel/ksysfs.c
@@ -58,7 +58,7 @@ KERNEL_ATTR_RW(uevent_helper);
 static ssize_t profiling_show(struct kobject *kobj,
 				  struct kobj_attribute *attr, char *buf)
 {
-	return sprintf(buf, "%d\n", prof_on);
+	return sprintf(buf, "%d\n", _imv_read(prof_on));
 }
 static ssize_t profiling_store(struct kobject *kobj,
 				   struct kobj_attribute *attr,
@@ -66,7 +66,7 @@ static ssize_t profiling_store(struct kobject *kobj,
 {
 	int ret;
 
-	if (prof_on)
+	if (_imv_read(prof_on))
 		return -EEXIST;
 	/*
 	 * This eventually calls into get_option() which
diff --git a/stblinux-2.6.31/kernel/lockdep.c b/stblinux-2.6.31/kernel/lockdep.c
index 8bbeef9..c66583c 100644
--- a/stblinux-2.6.31/kernel/lockdep.c
+++ b/stblinux-2.6.31/kernel/lockdep.c
@@ -47,6 +47,8 @@
 
 #include "lockdep_internals.h"
 
+#include <trace/lockdep.h>
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/lockdep.h>
 
@@ -64,6 +66,13 @@ module_param(lock_stat, int, 0644);
 #define lock_stat 0
 #endif
 
+DEFINE_TRACE(lockdep_hardirqs_on);
+DEFINE_TRACE(lockdep_hardirqs_off);
+DEFINE_TRACE(lockdep_softirqs_on);
+DEFINE_TRACE(lockdep_softirqs_off);
+DEFINE_TRACE(lockdep_lock_acquire);
+DEFINE_TRACE(lockdep_lock_release);
+
 /*
  * lockdep_lock: protects the lockdep graph, the hashes and the
  *               class/list/hash allocators.
@@ -2122,6 +2131,8 @@ void trace_hardirqs_on_caller(unsigned long ip)
 
 	time_hardirqs_on(CALLER_ADDR0, ip);
 
+	_trace_lockdep_hardirqs_on(ip);
+
 	if (unlikely(!debug_locks || current->lockdep_recursion))
 		return;
 
@@ -2175,6 +2186,8 @@ void trace_hardirqs_off_caller(unsigned long ip)
 
 	time_hardirqs_off(CALLER_ADDR0, ip);
 
+	_trace_lockdep_hardirqs_off(ip);
+
 	if (unlikely(!debug_locks || current->lockdep_recursion))
 		return;
 
@@ -2207,6 +2220,8 @@ void trace_softirqs_on(unsigned long ip)
 {
 	struct task_struct *curr = current;
 
+	_trace_lockdep_softirqs_on(ip);
+
 	if (unlikely(!debug_locks))
 		return;
 
@@ -2241,6 +2256,8 @@ void trace_softirqs_off(unsigned long ip)
 {
 	struct task_struct *curr = current;
 
+	_trace_lockdep_softirqs_off(ip);
+
 	if (unlikely(!debug_locks))
 		return;
 
@@ -2539,6 +2556,9 @@ static int __lock_acquire(struct lockdep_map *lock, unsigned int subclass,
 	int chain_head = 0;
 	u64 chain_key;
 
+	_trace_lockdep_lock_acquire(ip, subclass, lock, trylock, read,
+		hardirqs_off);
+
 	if (!prove_locking)
 		check = 1;
 
@@ -2867,6 +2887,8 @@ __lock_release(struct lockdep_map *lock, int nested, unsigned long ip)
 {
 	struct task_struct *curr = current;
 
+	_trace_lockdep_lock_release(ip, lock, nested);
+
 	if (!check_unlock(curr, lock, ip))
 		return;
 
diff --git a/stblinux-2.6.31/kernel/marker.c b/stblinux-2.6.31/kernel/marker.c
index ea54f26..b72d1e0 100644
--- a/stblinux-2.6.31/kernel/marker.c
+++ b/stblinux-2.6.31/kernel/marker.c
@@ -19,11 +19,14 @@
 #include <linux/mutex.h>
 #include <linux/types.h>
 #include <linux/jhash.h>
+#include <linux/hash.h>
 #include <linux/list.h>
 #include <linux/rcupdate.h>
 #include <linux/marker.h>
 #include <linux/err.h>
 #include <linux/slab.h>
+#include <linux/immediate.h>
+#include <linux/ltt-tracer.h>
 
 extern struct marker __start___markers[];
 extern struct marker __stop___markers[];
@@ -34,9 +37,23 @@ static const int marker_debug;
 /*
  * markers_mutex nests inside module_mutex. Markers mutex protects the builtin
  * and module markers and the hash table.
+ * markers_mutex nests inside the trace lock, to ensure event ID consistency
+ * between the hash table and the marker section.
  */
 static DEFINE_MUTEX(markers_mutex);
 
+void lock_markers(void)
+{
+	mutex_lock(&markers_mutex);
+}
+EXPORT_SYMBOL_GPL(lock_markers);
+
+void unlock_markers(void)
+{
+	mutex_unlock(&markers_mutex);
+}
+EXPORT_SYMBOL_GPL(unlock_markers);
+
 /*
  * Marker hash table, containing the active markers.
  * Protected by module_mutex.
@@ -44,6 +61,12 @@ static DEFINE_MUTEX(markers_mutex);
 #define MARKER_HASH_BITS 6
 #define MARKER_TABLE_SIZE (1 << MARKER_HASH_BITS)
 static struct hlist_head marker_table[MARKER_TABLE_SIZE];
+static struct hlist_head id_table[MARKER_TABLE_SIZE];
+
+struct marker_probe_array {
+	struct rcu_head rcu;
+	struct marker_probe_closure c[0];
+};
 
 /*
  * Note about RCU :
@@ -55,22 +78,24 @@ static struct hlist_head marker_table[MARKER_TABLE_SIZE];
  */
 struct marker_entry {
 	struct hlist_node hlist;
+	struct hlist_node id_list;
 	char *format;
+	char *name;
 			/* Probe wrapper */
 	void (*call)(const struct marker *mdata, void *call_private, ...);
 	struct marker_probe_closure single;
-	struct marker_probe_closure *multi;
+	struct marker_probe_array *multi;
 	int refcount;	/* Number of times armed. 0 if disarmed. */
-	struct rcu_head rcu;
-	void *oldptr;
-	int rcu_pending;
+	u16 channel_id;
+	u16 event_id;
 	unsigned char ptype:1;
 	unsigned char format_allocated:1;
-	char name[0];	/* Contains name'\0'format'\0' */
+	char channel[0];	/* Contains channel'\0'name'\0'format'\0' */
 };
 
 /**
  * __mark_empty_function - Empty probe callback
+ * @mdata: marker data
  * @probe_private: probe private data
  * @call_private: call site private data
  * @fmt: format string
@@ -81,8 +106,8 @@ struct marker_entry {
  * though the function pointer change and the marker enabling are two distinct
  * operations that modifies the execution flow of preemptible code.
  */
-notrace void __mark_empty_function(void *probe_private, void *call_private,
-	const char *fmt, va_list *args)
+notrace void __mark_empty_function(const struct marker *mdata,
+	void *probe_private, void *call_private, const char *fmt, va_list *args)
 {
 }
 EXPORT_SYMBOL_GPL(__mark_empty_function);
@@ -120,11 +145,11 @@ notrace void marker_probe_cb(const struct marker *mdata,
 		 * dependant, so we put an explicit smp_rmb() here. */
 		smp_rmb();
 		va_start(args, call_private);
-		func(mdata->single.probe_private, call_private, mdata->format,
-			&args);
+		func(mdata, mdata->single.probe_private, call_private,
+			mdata->format, &args);
 		va_end(args);
 	} else {
-		struct marker_probe_closure *multi;
+		struct marker_probe_array *multi;
 		int i;
 		/*
 		 * Read mdata->ptype before mdata->multi.
@@ -139,10 +164,10 @@ notrace void marker_probe_cb(const struct marker *mdata,
 		 * in the fast path, so put the explicit barrier here.
 		 */
 		smp_read_barrier_depends();
-		for (i = 0; multi[i].func; i++) {
+		for (i = 0; multi->c[i].func; i++) {
 			va_start(args, call_private);
-			multi[i].func(multi[i].probe_private, call_private,
-				mdata->format, &args);
+			multi->c[i].func(mdata, multi->c[i].probe_private,
+				call_private, mdata->format, &args);
 			va_end(args);
 		}
 	}
@@ -175,10 +200,10 @@ static notrace void marker_probe_cb_noarg(const struct marker *mdata,
 		/* Must read the ptr before private data. They are not data
 		 * dependant, so we put an explicit smp_rmb() here. */
 		smp_rmb();
-		func(mdata->single.probe_private, call_private, mdata->format,
-			&args);
+		func(mdata, mdata->single.probe_private, call_private,
+			mdata->format, &args);
 	} else {
-		struct marker_probe_closure *multi;
+		struct marker_probe_array *multi;
 		int i;
 		/*
 		 * Read mdata->ptype before mdata->multi.
@@ -193,21 +218,17 @@ static notrace void marker_probe_cb_noarg(const struct marker *mdata,
 		 * in the fast path, so put the explicit barrier here.
 		 */
 		smp_read_barrier_depends();
-		for (i = 0; multi[i].func; i++)
-			multi[i].func(multi[i].probe_private, call_private,
-				mdata->format, &args);
+		for (i = 0; multi->c[i].func; i++)
+			multi->c[i].func(mdata, multi->c[i].probe_private,
+				call_private, mdata->format, &args);
 	}
 	rcu_read_unlock_sched_notrace();
 }
 
 static void free_old_closure(struct rcu_head *head)
 {
-	struct marker_entry *entry = container_of(head,
-		struct marker_entry, rcu);
-	kfree(entry->oldptr);
-	/* Make sure we free the data before setting the pending flag to 0 */
-	smp_wmb();
-	entry->rcu_pending = 0;
+	struct marker_probe_array *multi = container_of(head, struct marker_probe_array, rcu);
+	kfree(multi);
 }
 
 static void debug_print_probes(struct marker_entry *entry)
@@ -222,19 +243,19 @@ static void debug_print_probes(struct marker_entry *entry)
 			entry->single.func,
 			entry->single.probe_private);
 	} else {
-		for (i = 0; entry->multi[i].func; i++)
+		for (i = 0; entry->multi->c[i].func; i++)
 			printk(KERN_DEBUG "Multi probe %d : %p %p\n", i,
-				entry->multi[i].func,
-				entry->multi[i].probe_private);
+				entry->multi->c[i].func,
+				entry->multi->c[i].probe_private);
 	}
 }
 
-static struct marker_probe_closure *
+static struct marker_probe_array *
 marker_entry_add_probe(struct marker_entry *entry,
 		marker_probe_func *probe, void *probe_private)
 {
 	int nr_probes = 0;
-	struct marker_probe_closure *old, *new;
+	struct marker_probe_array *old, *new;
 
 	WARN_ON(!probe);
 
@@ -259,24 +280,26 @@ marker_entry_add_probe(struct marker_entry *entry,
 		}
 	} else {
 		/* (N -> N+1), (N != 0, 1) probes */
-		for (nr_probes = 0; old[nr_probes].func; nr_probes++)
-			if (old[nr_probes].func == probe
-					&& old[nr_probes].probe_private
+		for (nr_probes = 0; old->c[nr_probes].func; nr_probes++)
+			if (old->c[nr_probes].func == probe
+					&& old->c[nr_probes].probe_private
 						== probe_private)
 				return ERR_PTR(-EBUSY);
 	}
 	/* + 2 : one for new probe, one for NULL func */
-	new = kzalloc((nr_probes + 2) * sizeof(struct marker_probe_closure),
+	new = kzalloc(sizeof(struct marker_probe_array)
+		      + ((nr_probes + 2) * sizeof(struct marker_probe_closure)),
 			GFP_KERNEL);
 	if (new == NULL)
 		return ERR_PTR(-ENOMEM);
+	INIT_RCU_HEAD(&new->rcu);
 	if (!old)
-		new[0] = entry->single;
+		new->c[0] = entry->single;
 	else
-		memcpy(new, old,
+		memcpy(&new->c[0], &old->c[0],
 			nr_probes * sizeof(struct marker_probe_closure));
-	new[nr_probes].func = probe;
-	new[nr_probes].probe_private = probe_private;
+	new->c[nr_probes].func = probe;
+	new->c[nr_probes].probe_private = probe_private;
 	entry->refcount = nr_probes + 1;
 	entry->multi = new;
 	entry->ptype = 1;
@@ -284,12 +307,12 @@ marker_entry_add_probe(struct marker_entry *entry,
 	return old;
 }
 
-static struct marker_probe_closure *
+static struct marker_probe_array *
 marker_entry_remove_probe(struct marker_entry *entry,
 		marker_probe_func *probe, void *probe_private)
 {
 	int nr_probes = 0, nr_del = 0, i;
-	struct marker_probe_closure *old, *new;
+	struct marker_probe_array *old, *new;
 
 	old = entry->multi;
 
@@ -307,9 +330,9 @@ marker_entry_remove_probe(struct marker_entry *entry,
 		return NULL;
 	} else {
 		/* (N -> M), (N > 1, M >= 0) probes */
-		for (nr_probes = 0; old[nr_probes].func; nr_probes++) {
-			if ((!probe || old[nr_probes].func == probe)
-					&& old[nr_probes].probe_private
+		for (nr_probes = 0; old->c[nr_probes].func; nr_probes++) {
+			if ((!probe || old->c[nr_probes].func == probe)
+					&& old->c[nr_probes].probe_private
 						== probe_private)
 				nr_del++;
 		}
@@ -322,24 +345,27 @@ marker_entry_remove_probe(struct marker_entry *entry,
 		entry->ptype = 0;
 	} else if (nr_probes - nr_del == 1) {
 		/* N -> 1, (N > 1) */
-		for (i = 0; old[i].func; i++)
-			if ((probe && old[i].func != probe) ||
-					old[i].probe_private != probe_private)
-				entry->single = old[i];
+		for (i = 0; old->c[i].func; i++)
+			if ((probe && old->c[i].func != probe) ||
+			    old->c[i].probe_private != probe_private)
+				entry->single = old->c[i];
 		entry->refcount = 1;
 		entry->ptype = 0;
 	} else {
 		int j = 0;
 		/* N -> M, (N > 1, M > 1) */
 		/* + 1 for NULL */
-		new = kzalloc((nr_probes - nr_del + 1)
-			* sizeof(struct marker_probe_closure), GFP_KERNEL);
+		new = kzalloc(sizeof(struct marker_probe_array)
+			      + ((nr_probes - nr_del + 1)
+			         * sizeof(struct marker_probe_closure)),
+			      GFP_KERNEL);
 		if (new == NULL)
 			return ERR_PTR(-ENOMEM);
-		for (i = 0; old[i].func; i++)
-			if ((probe && old[i].func != probe) ||
-					old[i].probe_private != probe_private)
-				new[j++] = old[i];
+		INIT_RCU_HEAD(&new->rcu);
+		for (i = 0; old->c[i].func; i++)
+			if ((probe && old->c[i].func != probe) ||
+			    old->c[i].probe_private != probe_private)
+				new->c[j++] = old->c[i];
 		entry->refcount = nr_probes - nr_del;
 		entry->ptype = 1;
 		entry->multi = new;
@@ -353,16 +379,19 @@ marker_entry_remove_probe(struct marker_entry *entry,
  * Must be called with markers_mutex held.
  * Returns NULL if not present.
  */
-static struct marker_entry *get_marker(const char *name)
+static struct marker_entry *get_marker(const char *channel, const char *name)
 {
 	struct hlist_head *head;
 	struct hlist_node *node;
 	struct marker_entry *e;
-	u32 hash = jhash(name, strlen(name), 0);
+	size_t channel_len = strlen(channel) + 1;
+	size_t name_len = strlen(name) + 1;
+	u32 hash;
 
+	hash = jhash(channel, channel_len-1, 0) ^ jhash(name, name_len-1, 0);
 	head = &marker_table[hash & ((1 << MARKER_HASH_BITS)-1)];
 	hlist_for_each_entry(e, node, head, hlist) {
-		if (!strcmp(name, e->name))
+		if (!strcmp(channel, e->channel) && !strcmp(name, e->name))
 			return e;
 	}
 	return NULL;
@@ -372,22 +401,25 @@ static struct marker_entry *get_marker(const char *name)
  * Add the marker to the marker hash table. Must be called with markers_mutex
  * held.
  */
-static struct marker_entry *add_marker(const char *name, const char *format)
+static struct marker_entry *add_marker(const char *channel, const char *name,
+		const char *format)
 {
 	struct hlist_head *head;
 	struct hlist_node *node;
 	struct marker_entry *e;
+	size_t channel_len = strlen(channel) + 1;
 	size_t name_len = strlen(name) + 1;
 	size_t format_len = 0;
-	u32 hash = jhash(name, name_len-1, 0);
+	u32 hash;
 
+	hash = jhash(channel, channel_len-1, 0) ^ jhash(name, name_len-1, 0);
 	if (format)
 		format_len = strlen(format) + 1;
 	head = &marker_table[hash & ((1 << MARKER_HASH_BITS)-1)];
 	hlist_for_each_entry(e, node, head, hlist) {
-		if (!strcmp(name, e->name)) {
+		if (!strcmp(channel, e->channel) && !strcmp(name, e->name)) {
 			printk(KERN_NOTICE
-				"Marker %s busy\n", name);
+				"Marker %s.%s busy\n", channel, name);
 			return ERR_PTR(-EBUSY);	/* Already there */
 		}
 	}
@@ -395,20 +427,24 @@ static struct marker_entry *add_marker(const char *name, const char *format)
 	 * Using kmalloc here to allocate a variable length element. Could
 	 * cause some memory fragmentation if overused.
 	 */
-	e = kmalloc(sizeof(struct marker_entry) + name_len + format_len,
-			GFP_KERNEL);
+	e = kmalloc(sizeof(struct marker_entry)
+		    + channel_len + name_len + format_len,
+		    GFP_KERNEL);
 	if (!e)
 		return ERR_PTR(-ENOMEM);
-	memcpy(&e->name[0], name, name_len);
+	memcpy(e->channel, channel, channel_len);
+	e->name = &e->channel[channel_len];
+	memcpy(e->name, name, name_len);
 	if (format) {
-		e->format = &e->name[name_len];
+		e->format = &e->name[channel_len + name_len];
 		memcpy(e->format, format, format_len);
 		if (strcmp(e->format, MARK_NOARGS) == 0)
 			e->call = marker_probe_cb_noarg;
 		else
 			e->call = marker_probe_cb;
-		trace_mark(core_marker_format, "name %s format %s",
-				e->name, e->format);
+		trace_mark(metadata, core_marker_format,
+			   "channel %s name %s format %s",
+			   e->channel, e->name, e->format);
 	} else {
 		e->format = NULL;
 		e->call = marker_probe_cb;
@@ -419,27 +455,31 @@ static struct marker_entry *add_marker(const char *name, const char *format)
 	e->ptype = 0;
 	e->format_allocated = 0;
 	e->refcount = 0;
-	e->rcu_pending = 0;
 	hlist_add_head(&e->hlist, head);
 	return e;
 }
 
 /*
  * Remove the marker from the marker hash table. Must be called with mutex_lock
- * held.
+ * held. Parameter "registered" indicates if the channel registration has been
+ * performed.
  */
-static int remove_marker(const char *name)
+static int remove_marker(const char *channel, const char *name, int registered,
+			 int compacting)
 {
 	struct hlist_head *head;
 	struct hlist_node *node;
 	struct marker_entry *e;
 	int found = 0;
-	size_t len = strlen(name) + 1;
-	u32 hash = jhash(name, len-1, 0);
+	size_t channel_len = strlen(channel) + 1;
+	size_t name_len = strlen(name) + 1;
+	u32 hash;
+	int ret;
 
+	hash = jhash(channel, channel_len-1, 0) ^ jhash(name, name_len-1, 0);
 	head = &marker_table[hash & ((1 << MARKER_HASH_BITS)-1)];
 	hlist_for_each_entry(e, node, head, hlist) {
-		if (!strcmp(name, e->name)) {
+		if (!strcmp(channel, e->channel) && !strcmp(name, e->name)) {
 			found = 1;
 			break;
 		}
@@ -448,12 +488,18 @@ static int remove_marker(const char *name)
 		return -ENOENT;
 	if (e->single.func != __mark_empty_function)
 		return -EBUSY;
+
+	if (registered && ltt_channels_trace_ref())
+		return 0;
+
 	hlist_del(&e->hlist);
+	hlist_del(&e->id_list);
+	if (registered) {
+		ret = ltt_channels_unregister(e->channel, compacting);
+		WARN_ON(ret);
+	}
 	if (e->format_allocated)
 		kfree(e->format);
-	/* Make sure the call_rcu has been executed */
-	if (e->rcu_pending)
-		rcu_barrier_sched();
 	kfree(e);
 	return 0;
 }
@@ -468,8 +514,9 @@ static int marker_set_format(struct marker_entry *entry, const char *format)
 		return -ENOMEM;
 	entry->format_allocated = 1;
 
-	trace_mark(core_marker_format, "name %s format %s",
-			entry->name, entry->format);
+	trace_mark(metadata, core_marker_format,
+		   "channel %s name %s format %s",
+		   entry->channel, entry->name, entry->format);
 	return 0;
 }
 
@@ -505,6 +552,8 @@ static int set_marker(struct marker_entry *entry, struct marker *elem,
 	 * callback (does not set arguments).
 	 */
 	elem->call = entry->call;
+	elem->channel_id = entry->channel_id;
+	elem->event_id = entry->event_id;
 	/*
 	 * Sanity check :
 	 * We only update the single probe private data when the ptr is
@@ -532,7 +581,7 @@ static int set_marker(struct marker_entry *entry, struct marker *elem,
 	smp_wmb();
 	elem->ptype = entry->ptype;
 
-	if (elem->tp_name && (active ^ elem->state)) {
+	if (elem->tp_name && (active ^ _imv_read(elem->state))) {
 		WARN_ON(!elem->tp_cb);
 		/*
 		 * It is ok to directly call the probe registration because type
@@ -562,7 +611,7 @@ static int set_marker(struct marker_entry *entry, struct marker *elem,
 				(unsigned long)elem->tp_cb));
 		}
 	}
-	elem->state = active;
+	elem->state__imv = active;
 
 	return ret;
 }
@@ -578,7 +627,7 @@ static void disable_marker(struct marker *elem)
 	int ret;
 
 	/* leave "call" as is. It is known statically. */
-	if (elem->tp_name && elem->state) {
+	if (elem->tp_name && _imv_read(elem->state)) {
 		WARN_ON(!elem->tp_cb);
 		/*
 		 * It is ok to directly call the probe registration because type
@@ -593,18 +642,87 @@ static void disable_marker(struct marker *elem)
 		 */
 		module_put(__module_text_address((unsigned long)elem->tp_cb));
 	}
-	elem->state = 0;
+	elem->state__imv = 0;
 	elem->single.func = __mark_empty_function;
 	/* Update the function before setting the ptype */
 	smp_wmb();
 	elem->ptype = 0;	/* single probe */
 	/*
-	 * Leave the private data and id there, because removal is racy and
-	 * should be done only after an RCU period. These are never used until
-	 * the next initialization anyway.
+	 * Leave the private data and channel_id/event_id there, because removal
+	 * is racy and should be done only after an RCU period. These are never
+	 * used until the next initialization anyway.
 	 */
 }
 
+/*
+ * is_marker_present - Check if a marker is present in kernel.
+ * @channel: channel name
+ * @name: marker name
+ *
+ * We cannot take the marker lock around calls to this function because it needs
+ * to take the module mutex within the iterator. Marker mutex nests inside
+ * module mutex.
+ * Returns 1 if the marker is present, 0 if not.
+ */
+int is_marker_present(const char *channel, const char *name)
+{
+	int ret;
+	struct marker_iter iter;
+
+	ret = 0;
+
+	marker_iter_reset(&iter);
+	marker_iter_start(&iter);
+	for (; iter.marker != NULL; marker_iter_next(&iter)) {
+		if (!strcmp(iter.marker->channel, channel) &&
+		    !strcmp(iter.marker->name, name)) {
+			ret = 1;
+			goto end;
+		}
+	}
+end:
+	marker_iter_stop(&iter);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(is_marker_present);
+
+/*
+ * _is_marker_enabled - Check if a marker is enabled, must be called with
+ *                      markers_mutex held.
+ * @channel: channel name
+ * @name: marker name
+ *
+ * Returns 1 if the marker is enabled, 0 if disabled.
+ */
+int _is_marker_enabled(const char *channel, const char *name)
+{
+	struct marker_entry *entry;
+
+	entry = get_marker(channel, name);
+
+	return entry && !!entry->refcount;
+}
+EXPORT_SYMBOL_GPL(_is_marker_enabled);
+
+/*
+ * is_marker_enabled - the wrapper of _is_marker_enabled
+ * @channel: channel name
+ * @name: marker name
+ *
+ * Returns 1 if the marker is enabled, 0 if disabled.
+ */
+int is_marker_enabled(const char *channel, const char *name)
+{
+	int ret;
+
+	lock_markers();
+	ret = _is_marker_enabled(channel, name);
+	unlock_markers();
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(is_marker_enabled);
+
 /**
  * marker_update_probe_range - Update a probe range
  * @begin: beginning of the range
@@ -620,7 +738,7 @@ void marker_update_probe_range(struct marker *begin,
 
 	mutex_lock(&markers_mutex);
 	for (iter = begin; iter < end; iter++) {
-		mark_entry = get_marker(iter->name);
+		mark_entry = get_marker(iter->channel, iter->name);
 		if (mark_entry) {
 			set_marker(mark_entry, iter, !!mark_entry->refcount);
 			/*
@@ -650,17 +768,21 @@ void marker_update_probe_range(struct marker *begin,
  * Site effect : marker_set_format may delete the marker entry (creating a
  * replacement).
  */
-static void marker_update_probes(void)
+void marker_update_probes(void)
 {
 	/* Core kernel markers */
 	marker_update_probe_range(__start___markers, __stop___markers);
 	/* Markers in modules. */
 	module_update_markers();
 	tracepoint_probe_update_all();
+	/* Update immediate values */
+	core_imv_update();
+	module_imv_update();
 }
 
 /**
  * marker_probe_register -  Connect a probe to a marker
+ * @channel: marker channel
  * @name: marker name
  * @format: format string
  * @probe: probe handler
@@ -670,60 +792,86 @@ static void marker_update_probes(void)
  * Returns 0 if ok, error value on error.
  * The probe address must at least be aligned on the architecture pointer size.
  */
-int marker_probe_register(const char *name, const char *format,
-			marker_probe_func *probe, void *probe_private)
+int marker_probe_register(const char *channel, const char *name,
+			  const char *format, marker_probe_func *probe,
+			  void *probe_private)
 {
 	struct marker_entry *entry;
-	int ret = 0;
-	struct marker_probe_closure *old;
+	int ret = 0, ret_err;
+	struct marker_probe_array *old;
+	int first_probe = 0;
 
 	mutex_lock(&markers_mutex);
-	entry = get_marker(name);
+	entry = get_marker(channel, name);
 	if (!entry) {
-		entry = add_marker(name, format);
+		first_probe = 1;
+		entry = add_marker(channel, name, format);
 		if (IS_ERR(entry))
 			ret = PTR_ERR(entry);
+		if (ret)
+			goto end;
+		ret = ltt_channels_register(channel);
+		if (ret)
+			goto error_remove_marker;
+		ret = ltt_channels_get_index_from_name(channel);
+		if (ret < 0)
+			goto error_unregister_channel;
+		entry->channel_id = ret;
+		ret = ltt_channels_get_event_id(channel, name);
+		if (ret < 0)
+			goto error_unregister_channel;
+		entry->event_id = ret;
+		hlist_add_head(&entry->id_list, id_table + hash_32(
+				(entry->channel_id << 16) | entry->event_id,
+				MARKER_HASH_BITS));
+		ret = 0;
+		trace_mark(metadata, core_marker_id,
+			   "channel %s name %s event_id %hu "
+			   "int #1u%zu long #1u%zu pointer #1u%zu "
+			   "size_t #1u%zu alignment #1u%u",
+			   channel, name, entry->event_id,
+			   sizeof(int), sizeof(long), sizeof(void *),
+			   sizeof(size_t), ltt_get_alignment());
 	} else if (format) {
 		if (!entry->format)
 			ret = marker_set_format(entry, format);
 		else if (strcmp(entry->format, format))
 			ret = -EPERM;
+		if (ret)
+			goto end;
 	}
-	if (ret)
-		goto end;
 
-	/*
-	 * If we detect that a call_rcu is pending for this marker,
-	 * make sure it's executed now.
-	 */
-	if (entry->rcu_pending)
-		rcu_barrier_sched();
 	old = marker_entry_add_probe(entry, probe, probe_private);
 	if (IS_ERR(old)) {
 		ret = PTR_ERR(old);
-		goto end;
+		if (first_probe)
+			goto error_unregister_channel;
+		else
+			goto end;
 	}
 	mutex_unlock(&markers_mutex);
+
 	marker_update_probes();
-	mutex_lock(&markers_mutex);
-	entry = get_marker(name);
-	if (!entry)
-		goto end;
-	if (entry->rcu_pending)
-		rcu_barrier_sched();
-	entry->oldptr = old;
-	entry->rcu_pending = 1;
-	/* write rcu_pending before calling the RCU callback */
-	smp_wmb();
-	call_rcu_sched(&entry->rcu, free_old_closure);
+	if (old)
+		call_rcu_sched(&old->rcu, free_old_closure);
+	return ret;
+
+error_unregister_channel:
+	ret_err = ltt_channels_unregister(channel, 1);
+	WARN_ON(ret_err);
+error_remove_marker:
+	ret_err = remove_marker(channel, name, 0, 0);
+	WARN_ON(ret_err);
 end:
 	mutex_unlock(&markers_mutex);
+	marker_update_probes();	/* for compaction on error path */
 	return ret;
 }
 EXPORT_SYMBOL_GPL(marker_probe_register);
 
 /**
  * marker_probe_unregister -  Disconnect a probe from a marker
+ * @channel: marker channel
  * @name: marker name
  * @probe: probe function pointer
  * @probe_private: probe private data
@@ -734,35 +882,28 @@ EXPORT_SYMBOL_GPL(marker_probe_register);
  * itself uses stop_machine(), which insures that every preempt disabled section
  * have finished.
  */
-int marker_probe_unregister(const char *name,
-	marker_probe_func *probe, void *probe_private)
+int marker_probe_unregister(const char *channel, const char *name,
+			    marker_probe_func *probe, void *probe_private)
 {
 	struct marker_entry *entry;
-	struct marker_probe_closure *old;
-	int ret = -ENOENT;
+	struct marker_probe_array *old;
+	int ret = 0;
 
 	mutex_lock(&markers_mutex);
-	entry = get_marker(name);
-	if (!entry)
+	entry = get_marker(channel, name);
+	if (!entry) {
+		ret = -ENOENT;
 		goto end;
-	if (entry->rcu_pending)
-		rcu_barrier_sched();
+	}
 	old = marker_entry_remove_probe(entry, probe, probe_private);
+	remove_marker(channel, name, 1, 0);	/* Ignore busy error message */
 	mutex_unlock(&markers_mutex);
+
 	marker_update_probes();
-	mutex_lock(&markers_mutex);
-	entry = get_marker(name);
-	if (!entry)
-		goto end;
-	if (entry->rcu_pending)
-		rcu_barrier_sched();
-	entry->oldptr = old;
-	entry->rcu_pending = 1;
-	/* write rcu_pending before calling the RCU callback */
-	smp_wmb();
-	call_rcu_sched(&entry->rcu, free_old_closure);
-	remove_marker(name);	/* Ignore busy error message */
-	ret = 0;
+	if (old)
+		call_rcu_sched(&old->rcu, free_old_closure);
+	return ret;
+
 end:
 	mutex_unlock(&markers_mutex);
 	return ret;
@@ -786,12 +927,12 @@ get_marker_from_private_data(marker_probe_func *probe, void *probe_private)
 						== probe_private)
 					return entry;
 			} else {
-				struct marker_probe_closure *closure;
+				struct marker_probe_array *closure;
 				closure = entry->multi;
-				for (i = 0; closure[i].func; i++) {
-					if (closure[i].func == probe &&
-							closure[i].probe_private
-							== probe_private)
+				for (i = 0; closure->c[i].func; i++) {
+					if (closure->c[i].func == probe &&
+					    closure->c[i].probe_private
+					    == probe_private)
 						return entry;
 				}
 			}
@@ -818,39 +959,38 @@ int marker_probe_unregister_private_data(marker_probe_func *probe,
 {
 	struct marker_entry *entry;
 	int ret = 0;
-	struct marker_probe_closure *old;
+	struct marker_probe_array *old;
+	const char *channel = NULL, *name = NULL;
 
 	mutex_lock(&markers_mutex);
 	entry = get_marker_from_private_data(probe, probe_private);
 	if (!entry) {
 		ret = -ENOENT;
-		goto end;
+		goto unlock;
 	}
-	if (entry->rcu_pending)
-		rcu_barrier_sched();
 	old = marker_entry_remove_probe(entry, NULL, probe_private);
+	channel = kstrdup(entry->channel, GFP_KERNEL);
+	name = kstrdup(entry->name, GFP_KERNEL);
+	remove_marker(channel, name, 1, 0);	/* Ignore busy error message */
 	mutex_unlock(&markers_mutex);
+
 	marker_update_probes();
-	mutex_lock(&markers_mutex);
-	entry = get_marker_from_private_data(probe, probe_private);
-	if (!entry)
-		goto end;
-	if (entry->rcu_pending)
-		rcu_barrier_sched();
-	entry->oldptr = old;
-	entry->rcu_pending = 1;
-	/* write rcu_pending before calling the RCU callback */
-	smp_wmb();
-	call_rcu_sched(&entry->rcu, free_old_closure);
-	remove_marker(entry->name);	/* Ignore busy error message */
-end:
+	if (old)
+		call_rcu_sched(&old->rcu, free_old_closure);
+	goto end;
+
+unlock:
 	mutex_unlock(&markers_mutex);
+end:
+	kfree(channel);
+	kfree(name);
 	return ret;
 }
 EXPORT_SYMBOL_GPL(marker_probe_unregister_private_data);
 
 /**
  * marker_get_private_data - Get a marker's probe private data
+ * @channel: marker channel
  * @name: marker name
  * @probe: probe to match
  * @num: get the nth matching probe's private data
@@ -862,31 +1002,33 @@ EXPORT_SYMBOL_GPL(marker_probe_unregister_private_data);
  * owner of the data, or its content could vanish. This is mostly used to
  * confirm that a caller is the owner of a registered probe.
  */
-void *marker_get_private_data(const char *name, marker_probe_func *probe,
-		int num)
+void *marker_get_private_data(const char *channel, const char *name,
+			      marker_probe_func *probe, int num)
 {
 	struct hlist_head *head;
 	struct hlist_node *node;
 	struct marker_entry *e;
+	size_t channel_len = strlen(channel) + 1;
 	size_t name_len = strlen(name) + 1;
-	u32 hash = jhash(name, name_len-1, 0);
 	int i;
+	u32 hash;
 
+	hash = jhash(channel, channel_len-1, 0) ^ jhash(name, name_len-1, 0);
 	head = &marker_table[hash & ((1 << MARKER_HASH_BITS)-1)];
 	hlist_for_each_entry(e, node, head, hlist) {
-		if (!strcmp(name, e->name)) {
+		if (!strcmp(channel, e->channel) && !strcmp(name, e->name)) {
 			if (!e->ptype) {
 				if (num == 0 && e->single.func == probe)
 					return e->single.probe_private;
 			} else {
-				struct marker_probe_closure *closure;
+				struct marker_probe_array *closure;
 				int match = 0;
 				closure = e->multi;
-				for (i = 0; closure[i].func; i++) {
-					if (closure[i].func != probe)
+				for (i = 0; closure->c[i].func; i++) {
+					if (closure->c[i].func != probe)
 						continue;
 					if (match++ == num)
-						return closure[i].probe_private;
+						return closure->c[i].probe_private;
 				}
 			}
 			break;
@@ -896,6 +1038,158 @@ void *marker_get_private_data(const char *name, marker_probe_func *probe,
 }
 EXPORT_SYMBOL_GPL(marker_get_private_data);
 
+static struct marker_entry *get_entry_from_id(u16 channel_id, u16 event_id)
+{
+	struct hlist_head *head;
+	struct hlist_node *node;
+	struct marker_entry *e, *found = NULL;
+	u32 hash = hash_32((channel_id << 16) | event_id, MARKER_HASH_BITS);
+
+	mutex_lock(&markers_mutex);
+	head = id_table + hash;
+	hlist_for_each_entry(e, node, head, id_list) {
+		if (e->channel_id == channel_id && e->event_id == event_id) {
+			found = e;
+			break;
+		}
+	}
+	mutex_unlock(&markers_mutex);
+	return found;
+}
+
+/* must call when ids/marker_entry are kept alive */
+const char *marker_get_name_from_id(u16 channel_id, u16 event_id)
+{
+	struct marker_entry *e = get_entry_from_id(channel_id, event_id);
+	return e ? e->name : NULL;
+}
+EXPORT_SYMBOL_GPL(marker_get_name_from_id);
+
+const char *marker_get_fmt_from_id(u16 channel_id, u16 event_id)
+{
+	struct marker_entry *e = get_entry_from_id(channel_id, event_id);
+	return e ? e->format : NULL;
+}
+EXPORT_SYMBOL_GPL(marker_get_fmt_from_id);
+
+/**
+ * markers_compact_event_ids - Compact markers event IDs and reassign channels
+ *
+ * Called when no channel users are active by the channel infrastructure.
+ * Called with trace lock, lock_markers() and channel mutex held.
+ *
+ * marker_update_probes() must be executed after compaction before releasing the
+ * trace lock.
+ */
+void markers_compact_event_ids(void)
+{
+	struct marker_entry *entry;
+	unsigned int i;
+	struct hlist_head *head;
+	struct hlist_node *node, *next;
+	int ret;
+
+	_ltt_channels_reset_event_ids();
+
+	for (i = 0; i < MARKER_TABLE_SIZE; i++) {
+		head = &marker_table[i];
+		hlist_for_each_entry_safe(entry, node, next, head, hlist) {
+			if (!entry->refcount) {
+				remove_marker(entry->channel, entry->name,
+					      1, 1);
+				continue;
+			}
+			ret = ltt_channels_get_index_from_name(entry->channel);
+			WARN_ON(ret < 0);
+			entry->channel_id = ret;
+			ret = _ltt_channels_get_event_id(entry->channel,
+							 entry->name);
+			WARN_ON(ret < 0);
+			entry->event_id = ret;
+		}
+	}
+
+	memset(id_table, 0, sizeof(id_table));
+	for (i = 0; i < MARKER_TABLE_SIZE; i++) {
+		head = &marker_table[i];
+		hlist_for_each_entry(entry, node, head, hlist) {
+			hlist_add_head(&entry->id_list, id_table + hash_32(
+					(entry->channel_id << 16)
+					| entry->event_id, MARKER_HASH_BITS));
+		}
+	}
+}
+
+/**
+ * marker_get_iter_range - Get a next marker iterator given a range.
+ * @marker: current markers (in), next marker (out)
+ * @begin: beginning of the range
+ * @end: end of the range
+ *
+ * Returns whether a next marker has been found (1) or not (0).
+ * Will return the first marker in the range if the input marker is NULL.
+ */
+int marker_get_iter_range(struct marker **marker, struct marker *begin,
+	struct marker *end)
+{
+	if (!*marker && begin != end) {
+		*marker = begin;
+		return 1;
+	}
+	if (*marker >= begin && *marker < end)
+		return 1;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(marker_get_iter_range);
+
+static void marker_get_iter(struct marker_iter *iter)
+{
+	int found = 0;
+
+	/* Core kernel markers */
+	if (!iter->module) {
+		found = marker_get_iter_range(&iter->marker,
+				__start___markers, __stop___markers);
+		if (found)
+			goto end;
+	}
+	/* Markers in modules. */
+	found = module_get_iter_markers(iter);
+end:
+	if (!found)
+		marker_iter_reset(iter);
+}
+
+void marker_iter_start(struct marker_iter *iter)
+{
+	marker_get_iter(iter);
+}
+EXPORT_SYMBOL_GPL(marker_iter_start);
+
+void marker_iter_next(struct marker_iter *iter)
+{
+	iter->marker++;
+	/*
+	 * iter->marker may be invalid because we blindly incremented it.
+	 * Make sure it is valid by marshalling on the markers, getting the
+	 * markers from following modules if necessary.
+	 */
+	marker_get_iter(iter);
+}
+EXPORT_SYMBOL_GPL(marker_iter_next);
+
+void marker_iter_stop(struct marker_iter *iter)
+{
+}
+EXPORT_SYMBOL_GPL(marker_iter_stop);
+
+void marker_iter_reset(struct marker_iter *iter)
+{
+	iter->module = NULL;
+	iter->marker = NULL;
+}
+EXPORT_SYMBOL_GPL(marker_iter_reset);
+
 #ifdef CONFIG_MODULES
 
 int marker_module_notify(struct notifier_block *self,
@@ -928,3 +1222,43 @@ static int init_markers(void)
 __initcall(init_markers);
 
 #endif /* CONFIG_MODULES */
+
+void ltt_dump_marker_state(struct ltt_trace *trace)
+{
+	struct marker_entry *entry;
+	struct ltt_probe_private_data call_data;
+	struct hlist_head *head;
+	struct hlist_node *node;
+	unsigned int i;
+
+	mutex_lock(&markers_mutex);
+	call_data.trace = trace;
+	call_data.serializer = NULL;
+
+	for (i = 0; i < MARKER_TABLE_SIZE; i++) {
+		head = &marker_table[i];
+		hlist_for_each_entry(entry, node, head, hlist) {
+			__trace_mark(0, metadata, core_marker_id,
+				&call_data,
+				"channel %s name %s event_id %hu "
+				"int #1u%zu long #1u%zu pointer #1u%zu "
+				"size_t #1u%zu alignment #1u%u",
+				entry->channel,
+				entry->name,
+				entry->event_id,
+				sizeof(int), sizeof(long),
+				sizeof(void *), sizeof(size_t),
+				ltt_get_alignment());
+			if (entry->format)
+				__trace_mark(0, metadata,
+					core_marker_format,
+					&call_data,
+					"channel %s name %s format %s",
+					entry->channel,
+					entry->name,
+					entry->format);
+		}
+	}
+	mutex_unlock(&markers_mutex);
+}
+EXPORT_SYMBOL_GPL(ltt_dump_marker_state);
diff --git a/stblinux-2.6.31/kernel/module.c b/stblinux-2.6.31/kernel/module.c
index cda4d76..1574bdf 100644
--- a/stblinux-2.6.31/kernel/module.c
+++ b/stblinux-2.6.31/kernel/module.c
@@ -36,6 +36,7 @@
 #include <linux/cpu.h>
 #include <linux/moduleparam.h>
 #include <linux/errno.h>
+#include <linux/immediate.h>
 #include <linux/err.h>
 #include <linux/vermagic.h>
 #include <linux/notifier.h>
@@ -54,6 +55,7 @@
 #include <linux/async.h>
 #include <linux/percpu.h>
 #include <linux/kmemleak.h>
+#include <trace/kernel.h>
 
 #if 0
 #define DEBUGP printk
@@ -69,7 +71,9 @@
 #define INIT_OFFSET_MASK (1UL << (BITS_PER_LONG-1))
 
 /* List of modules, protected by module_mutex or preempt_disable
- * (delete uses stop_machine/add uses RCU list operations). */
+ * (delete uses stop_machine/add uses RCU list operations).
+ * Sorted by ascending list node address.
+ */
 DEFINE_MUTEX(module_mutex);
 EXPORT_SYMBOL_GPL(module_mutex);
 static LIST_HEAD(modules);
@@ -85,6 +89,9 @@ static BLOCKING_NOTIFIER_HEAD(module_notify_list);
 /* Bounds of module allocation, for speeding __module_address */
 static unsigned long module_addr_min = -1UL, module_addr_max = 0;
 
+DEFINE_TRACE(kernel_module_load);
+DEFINE_TRACE(kernel_module_free);
+
 int register_module_notifier(struct notifier_block * nb)
 {
 	return blocking_notifier_chain_register(&module_notify_list, nb);
@@ -1500,6 +1507,8 @@ static int __unlink_module(void *_mod)
 /* Free a module, remove from lists, etc (must hold module_mutex). */
 static void free_module(struct module *mod)
 {
+	trace_kernel_module_free(mod);
+
 	/* Delete from various lists */
 	stop_machine(__unlink_module, mod, NULL);
 	remove_notes_attrs(mod);
@@ -1949,6 +1958,7 @@ static noinline struct module *load_module(void __user *umod,
 	long err = 0;
 	void *percpu = NULL, *ptr = NULL; /* Stops spurious gcc warning */
 	mm_segment_t old_fs;
+	struct module *iter;
 
 	DEBUGP("load_module: umod=%p, len=%lu, uargs=%p\n",
 	       umod, len, uargs);
@@ -1960,6 +1970,12 @@ static noinline struct module *load_module(void __user *umod,
 	if (len > 64 * 1024 * 1024 || (hdr = vmalloc(len)) == NULL)
 		return ERR_PTR(-ENOMEM);
 
+	/*
+	 * Make sure the module text or data access never generates any page
+	 * fault.
+	 */
+	vmalloc_sync_all();
+
 	if (copy_from_user(hdr, umod, len) != 0) {
 		err = -EFAULT;
 		goto free_hdr;
@@ -2230,6 +2246,11 @@ static noinline struct module *load_module(void __user *umod,
 	mod->ctors = section_objs(hdr, sechdrs, secstrings, ".ctors",
 				  sizeof(*mod->ctors), &mod->num_ctors);
 #endif
+#ifdef USE_IMMEDIATE
+	mod->immediate = section_objs(hdr, sechdrs, secstrings, "__imv",
+					sizeof(*mod->immediate),
+					&mod->num_immediate);
+#endif
 
 #ifdef CONFIG_MARKERS
 	mod->markers = section_objs(hdr, sechdrs, secstrings, "__markers",
@@ -2352,8 +2373,23 @@ static noinline struct module *load_module(void __user *umod,
 	 * function to insert in a way safe to concurrent readers.
 	 * The mutex protects against concurrent writers.
 	 */
+	/*
+	 * We sort the modules by struct module pointer address to permit
+	 * correct iteration over modules of, at least, kallsyms for preemptible
+	 * operations, such as read(). Sorting by struct module pointer address
+	 * is equivalent to sort by list node address.
+	 */
+	list_for_each_entry_reverse(iter, &modules, list) {
+		BUG_ON(iter == mod);	/* Should never be in the list twice */
+		if (iter < mod) {
+			/* We belong to the location right after iter. */
+			list_add_rcu(&mod->list, &iter->list);
+			goto module_added;
+		}
+	}
+	/* We should be added at the head of the list */
 	list_add_rcu(&mod->list, &modules);
-
+module_added:
 	err = parse_args(mod->name, mod->args, mod->kp, mod->num_kp, NULL);
 	if (err < 0)
 		goto unlink;
@@ -2367,6 +2403,8 @@ static noinline struct module *load_module(void __user *umod,
 	/* Get rid of temporary copy */
 	vfree(hdr);
 
+	trace_kernel_module_load(mod);
+
 	/* Done! */
 	return mod;
 
@@ -2481,6 +2519,10 @@ SYSCALL_DEFINE3(init_module, void __user *, umod,
 	mutex_lock(&module_mutex);
 	/* Drop initial reference. */
 	module_put(mod);
+#ifdef USE_IMMEDIATE
+	imv_unref(mod->immediate, mod->immediate + mod->num_immediate,
+		mod->module_init, mod->init_size);
+#endif
 	trim_init_extable(mod);
 	module_free(mod, mod->module_init);
 	mod->module_init = NULL;
@@ -2746,12 +2788,12 @@ static char *module_flags(struct module *mod, char *buf)
 static void *m_start(struct seq_file *m, loff_t *pos)
 {
 	mutex_lock(&module_mutex);
-	return seq_list_start(&modules, *pos);
+	return seq_sorted_list_start(&modules, pos);
 }
 
 static void *m_next(struct seq_file *m, void *p, loff_t *pos)
 {
-	return seq_list_next(p, &modules, pos);
+	return seq_sorted_list_next(p, &modules, pos);
 }
 
 static void m_stop(struct seq_file *m, void *p)
@@ -2816,6 +2858,27 @@ static int __init proc_modules_init(void)
 module_init(proc_modules_init);
 #endif
 
+void list_modules(void *call_data)
+{
+	/* Enumerate loaded modules */
+	struct list_head	*i;
+	struct module		*mod;
+	unsigned long refcount = 0;
+
+	mutex_lock(&module_mutex);
+	list_for_each(i, &modules) {
+		mod = list_entry(i, struct module, list);
+#ifdef CONFIG_MODULE_UNLOAD
+		refcount = module_refcount(mod);
+#endif
+		__trace_mark(0, module_state, list_module, call_data,
+				"name %s state %d refcount %lu",
+				mod->name, mod->state, refcount);
+	}
+	mutex_unlock(&module_mutex);
+}
+EXPORT_SYMBOL_GPL(list_modules);
+
 /* Given an address, look for it in the module exception tables. */
 const struct exception_table_entry *search_module_extables(unsigned long addr)
 {
@@ -2957,11 +3020,43 @@ void module_update_markers(void)
 
 	mutex_lock(&module_mutex);
 	list_for_each_entry(mod, &modules, list)
-		if (!mod->taints)
+		if (!(mod->taints & TAINT_FORCED_MODULE))
 			marker_update_probe_range(mod->markers,
 				mod->markers + mod->num_markers);
 	mutex_unlock(&module_mutex);
 }
+
+/*
+ * Returns 0 if current not found.
+ * Returns 1 if current found.
+ */
+int module_get_iter_markers(struct marker_iter *iter)
+{
+	struct module *iter_mod;
+	int found = 0;
+
+	mutex_lock(&module_mutex);
+	list_for_each_entry(iter_mod, &modules, list) {
+		if (!(iter_mod->taints & TAINT_FORCED_MODULE)) {
+			/*
+			 * Sorted module list
+			 */
+			if (iter_mod < iter->module)
+				continue;
+			else if (iter_mod > iter->module)
+				iter->marker = NULL;
+			found = marker_get_iter_range(&iter->marker,
+				iter_mod->markers,
+				iter_mod->markers + iter_mod->num_markers);
+			if (found) {
+				iter->module = iter_mod;
+				break;
+			}
+		}
+	}
+	mutex_unlock(&module_mutex);
+	return found;
+}
 #endif
 
 #ifdef CONFIG_TRACEPOINTS
@@ -3010,3 +3105,38 @@ int module_get_iter_tracepoints(struct tracepoint_iter *iter)
 	return found;
 }
 #endif
+
+#ifdef USE_IMMEDIATE
+/**
+ * _module_imv_update - update all immediate values in the kernel
+ *
+ * Iterate on the kernel core and modules to update the immediate values.
+ * Module_mutex must be held be the caller.
+ */
+void _module_imv_update(void)
+{
+	struct module *mod;
+
+	list_for_each_entry(mod, &modules, list) {
+		if (mod->taints)
+			continue;
+		imv_update_range(mod->immediate,
+			mod->immediate + mod->num_immediate);
+	}
+}
+EXPORT_SYMBOL_GPL(_module_imv_update);
+
+/**
+ * module_imv_update - update all immediate values in the kernel
+ *
+ * Iterate on the kernel core and modules to update the immediate values.
+ * Takes module_mutex.
+ */
+void module_imv_update(void)
+{
+	mutex_lock(&module_mutex);
+	_module_imv_update();
+	mutex_unlock(&module_mutex);
+}
+EXPORT_SYMBOL_GPL(module_imv_update);
+#endif
diff --git a/stblinux-2.6.31/kernel/notifier.c b/stblinux-2.6.31/kernel/notifier.c
index 61d5aa5..e34114e 100644
--- a/stblinux-2.6.31/kernel/notifier.c
+++ b/stblinux-2.6.31/kernel/notifier.c
@@ -5,6 +5,7 @@
 #include <linux/rcupdate.h>
 #include <linux/vmalloc.h>
 #include <linux/reboot.h>
+#include <linux/idle.h>
 
 /*
  *	Notifier list for kernel code which wants to be called
@@ -148,7 +149,7 @@ int atomic_notifier_chain_unregister(struct atomic_notifier_head *nh,
 	spin_lock_irqsave(&nh->lock, flags);
 	ret = notifier_chain_unregister(&nh->head, n);
 	spin_unlock_irqrestore(&nh->lock, flags);
-	synchronize_rcu();
+	synchronize_sched();
 	return ret;
 }
 EXPORT_SYMBOL_GPL(atomic_notifier_chain_unregister);
@@ -178,9 +179,9 @@ int __kprobes __atomic_notifier_call_chain(struct atomic_notifier_head *nh,
 {
 	int ret;
 
-	rcu_read_lock();
+	rcu_read_lock_sched_notrace();
 	ret = notifier_call_chain(&nh->head, val, v, nr_to_call, nr_calls);
-	rcu_read_unlock();
+	rcu_read_unlock_sched_notrace();
 	return ret;
 }
 EXPORT_SYMBOL_GPL(__atomic_notifier_call_chain);
@@ -584,3 +585,27 @@ int unregister_die_notifier(struct notifier_block *nb)
 	return atomic_notifier_chain_unregister(&die_chain, nb);
 }
 EXPORT_SYMBOL_GPL(unregister_die_notifier);
+
+static ATOMIC_NOTIFIER_HEAD(idle_notifier);
+
+/*
+ * Trace last event before calling notifiers. Notifiers flush data from buffers
+ * before going to idle.
+ */
+int notrace notify_idle(enum idle_val val)
+{
+	return atomic_notifier_call_chain(&idle_notifier, val, NULL);
+}
+EXPORT_SYMBOL_GPL(notify_idle);
+
+void register_idle_notifier(struct notifier_block *n)
+{
+	atomic_notifier_chain_register(&idle_notifier, n);
+}
+EXPORT_SYMBOL_GPL(register_idle_notifier);
+
+void unregister_idle_notifier(struct notifier_block *n)
+{
+	atomic_notifier_chain_unregister(&idle_notifier, n);
+}
+EXPORT_SYMBOL_GPL(unregister_idle_notifier);
diff --git a/stblinux-2.6.31/kernel/panic.c b/stblinux-2.6.31/kernel/panic.c
index 512ab73..bcc8aea 100644
--- a/stblinux-2.6.31/kernel/panic.c
+++ b/stblinux-2.6.31/kernel/panic.c
@@ -22,6 +22,9 @@
 #include <linux/init.h>
 #include <linux/nmi.h>
 #include <linux/dmi.h>
+#include <trace/kernel.h>
+
+DEFINE_TRACE(kernel_panic);
 
 int panic_on_oops;
 static unsigned long tainted_mask;
@@ -58,6 +61,10 @@ NORET_TYPE void panic(const char * fmt, ...)
 	va_list args;
 	long i;
 
+	va_start(args, fmt);
+	trace_kernel_panic(fmt, args);
+	va_end(args);
+
 	/*
 	 * It's possible to come here directly from a panic-assertion and
 	 * not have preempt disabled. Some functions called from here want
diff --git a/stblinux-2.6.31/kernel/pid.c b/stblinux-2.6.31/kernel/pid.c
index 31310b5..0195083 100644
--- a/stblinux-2.6.31/kernel/pid.c
+++ b/stblinux-2.6.31/kernel/pid.c
@@ -264,6 +264,7 @@ struct pid *alloc_pid(struct pid_namespace *ns)
 
 	get_pid_ns(ns);
 	pid->level = ns->level;
+	INIT_RCU_HEAD(&pid->rcu);
 	atomic_set(&pid->count, 1);
 	for (type = 0; type < PIDTYPE_MAX; ++type)
 		INIT_HLIST_HEAD(&pid->tasks[type]);
diff --git a/stblinux-2.6.31/kernel/printk.c b/stblinux-2.6.31/kernel/printk.c
index a64fd8c..11326b6 100644
--- a/stblinux-2.6.31/kernel/printk.c
+++ b/stblinux-2.6.31/kernel/printk.c
@@ -35,6 +35,7 @@
 #include <linux/kexec.h>
 #include <linux/cpu.h>
 #include <linux/notifier.h>
+#include <trace/kernel.h>
 
 #include <asm/uaccess.h>
 
@@ -62,6 +63,7 @@ int console_printk[4] = {
 	MINIMUM_CONSOLE_LOGLEVEL,	/* minimum_console_loglevel */
 	DEFAULT_CONSOLE_LOGLEVEL,	/* default_console_loglevel */
 };
+EXPORT_SYMBOL_GPL(console_printk);
 
 /*
  * Low level drivers may need that to know if they can schedule in
@@ -131,6 +133,9 @@ EXPORT_SYMBOL(console_set_on_cmdline);
 /* Flag: console code may call schedule() */
 static int console_may_schedule;
 
+DEFINE_TRACE(kernel_printk);
+DEFINE_TRACE(kernel_vprintk);
+
 #ifdef CONFIG_PRINTK
 
 static char __log_buf[__LOG_BUF_LEN];
@@ -581,6 +586,7 @@ asmlinkage int printk(const char *fmt, ...)
 	int r;
 
 	va_start(args, fmt);
+	_trace_kernel_printk(_RET_IP_);
 	r = vprintk(fmt, args);
 	va_end(args);
 
@@ -688,6 +694,7 @@ asmlinkage int vprintk(const char *fmt, va_list args)
 	printed_len += vscnprintf(printk_buf + printed_len,
 				  sizeof(printk_buf) - printed_len, fmt, args);
 
+	_trace_kernel_vprintk(_RET_IP_, printk_buf, printed_len);
 
 	p = printk_buf;
 
diff --git a/stblinux-2.6.31/kernel/profile.c b/stblinux-2.6.31/kernel/profile.c
index 419250e..55c8df4 100644
--- a/stblinux-2.6.31/kernel/profile.c
+++ b/stblinux-2.6.31/kernel/profile.c
@@ -42,8 +42,8 @@ static int (*timer_hook)(struct pt_regs *) __read_mostly;
 static atomic_t *prof_buffer;
 static unsigned long prof_len, prof_shift;
 
-int prof_on __read_mostly;
-EXPORT_SYMBOL_GPL(prof_on);
+DEFINE_IMV(char, prof_on) __read_mostly;
+EXPORT_IMV_SYMBOL_GPL(prof_on);
 
 static cpumask_var_t prof_cpu_mask;
 #ifdef CONFIG_SMP
@@ -61,7 +61,7 @@ int profile_setup(char *str)
 
 	if (!strncmp(str, sleepstr, strlen(sleepstr))) {
 #ifdef CONFIG_SCHEDSTATS
-		prof_on = SLEEP_PROFILING;
+		imv_set(prof_on, SLEEP_PROFILING);
 		if (str[strlen(sleepstr)] == ',')
 			str += strlen(sleepstr) + 1;
 		if (get_option(&str, &par))
@@ -74,7 +74,7 @@ int profile_setup(char *str)
 			"kernel sleep profiling requires CONFIG_SCHEDSTATS\n");
 #endif /* CONFIG_SCHEDSTATS */
 	} else if (!strncmp(str, schedstr, strlen(schedstr))) {
-		prof_on = SCHED_PROFILING;
+		imv_set(prof_on, SCHED_PROFILING);
 		if (str[strlen(schedstr)] == ',')
 			str += strlen(schedstr) + 1;
 		if (get_option(&str, &par))
@@ -83,7 +83,7 @@ int profile_setup(char *str)
 			"kernel schedule profiling enabled (shift: %ld)\n",
 			prof_shift);
 	} else if (!strncmp(str, kvmstr, strlen(kvmstr))) {
-		prof_on = KVM_PROFILING;
+		imv_set(prof_on, KVM_PROFILING);
 		if (str[strlen(kvmstr)] == ',')
 			str += strlen(kvmstr) + 1;
 		if (get_option(&str, &par))
@@ -93,7 +93,7 @@ int profile_setup(char *str)
 			prof_shift);
 	} else if (get_option(&str, &par)) {
 		prof_shift = par;
-		prof_on = CPU_PROFILING;
+		imv_set(prof_on, CPU_PROFILING);
 		printk(KERN_INFO "kernel profiling enabled (shift: %ld)\n",
 			prof_shift);
 	}
@@ -105,7 +105,7 @@ __setup("profile=", profile_setup);
 int __ref profile_init(void)
 {
 	int buffer_bytes;
-	if (!prof_on)
+	if (!_imv_read(prof_on))
 		return 0;
 
 	/* only text is profiled */
@@ -309,7 +309,7 @@ void profile_hits(int type, void *__pc, unsigned int nr_hits)
 	int i, j, cpu;
 	struct profile_hit *hits;
 
-	if (prof_on != type || !prof_buffer)
+	if (!prof_buffer)
 		return;
 	pc = min((pc - (unsigned long)_stext) >> prof_shift, prof_len - 1);
 	i = primary = (pc & (NR_PROFILE_GRP - 1)) << PROFILE_GRPSHIFT;
@@ -421,7 +421,7 @@ void profile_hits(int type, void *__pc, unsigned int nr_hits)
 {
 	unsigned long pc;
 
-	if (prof_on != type || !prof_buffer)
+	if (!prof_buffer)
 		return;
 	pc = ((unsigned long)__pc - (unsigned long)_stext) >> prof_shift;
 	atomic_add(nr_hits, &prof_buffer[min(pc, prof_len - 1)]);
@@ -582,7 +582,7 @@ static int create_hash_tables(void)
 	}
 	return 0;
 out_cleanup:
-	prof_on = 0;
+	imv_set(prof_on, 0);
 	smp_mb();
 	on_each_cpu(profile_nop, NULL, 1);
 	for_each_online_cpu(cpu) {
@@ -609,7 +609,7 @@ int __ref create_proc_profile(void) /* false positive from hotcpu_notifier */
 {
 	struct proc_dir_entry *entry;
 
-	if (!prof_on)
+	if (!_imv_read(prof_on))
 		return 0;
 	if (create_hash_tables())
 		return -ENOMEM;
diff --git a/stblinux-2.6.31/kernel/rcuclassic.c b/stblinux-2.6.31/kernel/rcuclassic.c
index 0f2b0b3..fa0a9d0 100644
--- a/stblinux-2.6.31/kernel/rcuclassic.c
+++ b/stblinux-2.6.31/kernel/rcuclassic.c
@@ -48,6 +48,7 @@
 #include <linux/cpu.h>
 #include <linux/mutex.h>
 #include <linux/time.h>
+#include <trace/rcu.h>
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 static struct lock_class_key rcu_lock_key;
@@ -99,6 +100,10 @@ static int blimit = 10;
 static int qhimark = 10000;
 static int qlowmark = 100;
 
+DEFINE_TRACE(rcu_classic_call_rcu);
+DEFINE_TRACE(rcu_classic_call_rcu_bh);
+DEFINE_TRACE(rcu_classic_callback);
+
 #ifdef CONFIG_SMP
 static void force_quiescent_state(struct rcu_data *rdp,
 			struct rcu_ctrlblk *rcp)
@@ -283,6 +288,7 @@ void call_rcu(struct rcu_head *head,
 
 	head->func = func;
 	local_irq_save(flags);
+	trace_rcu_classic_call_rcu(head, _RET_IP_);
 	__call_rcu(head, &rcu_ctrlblk, &__get_cpu_var(rcu_data));
 	local_irq_restore(flags);
 }
@@ -311,6 +317,7 @@ void call_rcu_bh(struct rcu_head *head,
 
 	head->func = func;
 	local_irq_save(flags);
+	trace_rcu_classic_call_rcu_bh(head, _RET_IP_);
 	__call_rcu(head, &rcu_bh_ctrlblk, &__get_cpu_var(rcu_bh_data));
 	local_irq_restore(flags);
 }
@@ -356,6 +363,7 @@ static void rcu_do_batch(struct rcu_data *rdp)
 	while (list) {
 		next = list->next;
 		prefetch(next);
+		trace_rcu_classic_callback(list);
 		list->func(list);
 		list = next;
 		if (++count >= rdp->blimit)
diff --git a/stblinux-2.6.31/kernel/rcupdate.c b/stblinux-2.6.31/kernel/rcupdate.c
index a967c9f..991b8fb 100644
--- a/stblinux-2.6.31/kernel/rcupdate.c
+++ b/stblinux-2.6.31/kernel/rcupdate.c
@@ -90,6 +90,7 @@ void synchronize_rcu(void)
 	if (rcu_blocking_is_gp())
 		return;
 
+	INIT_RCU_HEAD(&rcu.head);
 	init_completion(&rcu.completion);
 	/* Will wake me after RCU finished. */
 	call_rcu(&rcu.head, wakeme_after_rcu);
diff --git a/stblinux-2.6.31/kernel/rcupreempt.c b/stblinux-2.6.31/kernel/rcupreempt.c
index beb0e65..494dc98 100644
--- a/stblinux-2.6.31/kernel/rcupreempt.c
+++ b/stblinux-2.6.31/kernel/rcupreempt.c
@@ -56,6 +56,7 @@
 #include <linux/delay.h>
 #include <linux/cpumask.h>
 #include <linux/rcupreempt_trace.h>
+#include <trace/rcu.h>
 #include <asm/byteorder.h>
 
 /*
@@ -267,6 +268,10 @@ static DEFINE_PER_CPU_SHARED_ALIGNED(enum rcu_mb_flag_values, rcu_mb_flag)
 
 #define RCU_SCHED_BATCH_TIME (HZ / 50)
 
+DEFINE_TRACE(rcu_preempt_callback);
+DEFINE_TRACE(rcu_preempt_call_rcu);
+DEFINE_TRACE(rcu_preempt_call_rcu_sched);
+
 /*
  * Return the number of RCU batches processed thus far.  Useful
  * for debug and statistics.
@@ -1151,6 +1156,7 @@ static void rcu_process_callbacks(struct softirq_action *unused)
 	spin_unlock_irqrestore(&rdp->lock, flags);
 	while (list) {
 		next = list->next;
+		trace_rcu_preempt_callback(list);
 		list->func(list);
 		list = next;
 		RCU_TRACE_ME(rcupreempt_trace_invoke);
@@ -1165,6 +1171,7 @@ void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
 	head->func = func;
 	head->next = NULL;
 	local_irq_save(flags);
+	trace_rcu_preempt_call_rcu(head, _RET_IP_);
 	rdp = RCU_DATA_ME();
 	spin_lock(&rdp->lock);
 	__rcu_advance_callbacks(rdp);
@@ -1184,6 +1191,7 @@ void call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
 	head->func = func;
 	head->next = NULL;
 	local_irq_save(flags);
+	trace_rcu_preempt_call_rcu_sched(head, _RET_IP_);
 	rdp = RCU_DATA_ME();
 	spin_lock(&rdp->lock);
 	*rdp->nextschedtail = head;
diff --git a/stblinux-2.6.31/kernel/rcutree.c b/stblinux-2.6.31/kernel/rcutree.c
index 7717b95..b136e2e 100644
--- a/stblinux-2.6.31/kernel/rcutree.c
+++ b/stblinux-2.6.31/kernel/rcutree.c
@@ -38,6 +38,7 @@
 #include <asm/atomic.h>
 #include <linux/bitops.h>
 #include <linux/module.h>
+#include <linux/poison.h>
 #include <linux/completion.h>
 #include <linux/moduleparam.h>
 #include <linux/percpu.h>
@@ -45,6 +46,7 @@
 #include <linux/cpu.h>
 #include <linux/mutex.h>
 #include <linux/time.h>
+#include <trace/rcu.h>
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 static struct lock_class_key rcu_lock_key;
@@ -109,6 +111,10 @@ static int blimit = 10;		/* Maximum callbacks per softirq. */
 static int qhimark = 10000;	/* If this many pending, ignore blimit. */
 static int qlowmark = 100;	/* Once only this many pending, use blimit. */
 
+DEFINE_TRACE(rcu_tree_call_rcu);
+DEFINE_TRACE(rcu_tree_call_rcu_bh);
+DEFINE_TRACE(rcu_tree_callback);
+
 static void force_quiescent_state(struct rcu_state *rsp, int relaxed);
 
 /*
@@ -921,6 +927,11 @@ static void rcu_do_batch(struct rcu_data *rdp)
 	while (list) {
 		next = list->next;
 		prefetch(next);
+		trace_rcu_tree_callback(list);
+#ifdef DEBUG_RCU_HEAD
+		WARN_ON_ONCE(list->debug != LIST_POISON1);
+		list->debug = NULL;
+#endif
 		list->func(list);
 		list = next;
 		if (++count >= rdp->blimit)
@@ -1188,6 +1199,11 @@ __call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu),
 	unsigned long flags;
 	struct rcu_data *rdp;
 
+#ifdef DEBUG_RCU_HEAD
+	WARN_ON_ONCE(head->debug);
+	head->debug = LIST_POISON1;
+#endif
+
 	head->func = func;
 	head->next = NULL;
 
@@ -1231,6 +1247,7 @@ __call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu),
  */
 void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
 {
+	trace_rcu_tree_call_rcu(head, _RET_IP_);
 	__call_rcu(head, func, &rcu_state);
 }
 EXPORT_SYMBOL_GPL(call_rcu);
@@ -1240,6 +1257,7 @@ EXPORT_SYMBOL_GPL(call_rcu);
  */
 void call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
 {
+	trace_rcu_tree_call_rcu_bh(head, _RET_IP_);
 	__call_rcu(head, func, &rcu_bh_state);
 }
 EXPORT_SYMBOL_GPL(call_rcu_bh);
diff --git a/stblinux-2.6.31/kernel/sched.c b/stblinux-2.6.31/kernel/sched.c
index 81ede13..ebf9325 100644
--- a/stblinux-2.6.31/kernel/sched.c
+++ b/stblinux-2.6.31/kernel/sched.c
@@ -10584,3 +10584,58 @@ struct cgroup_subsys cpuacct_subsys = {
 	.subsys_id = cpuacct_subsys_id,
 };
 #endif	/* CONFIG_CGROUP_CPUACCT */
+
+static DEFINE_MUTEX(kernel_trace_mutex);
+static int kernel_trace_refcount;
+
+/**
+ * clear_kernel_trace_flag_all_tasks - clears all TIF_KERNEL_TRACE thread flags.
+ *
+ * This function iterates on all threads in the system to clear their
+ * TIF_KERNEL_TRACE flag. Setting the TIF_KERNEL_TRACE flag with the
+ * tasklist_lock held in copy_process() makes sure that once we finish clearing
+ * the thread flags, all threads have their flags cleared.
+ */
+void clear_kernel_trace_flag_all_tasks(void)
+{
+	struct task_struct *p;
+	struct task_struct *t;
+
+	mutex_lock(&kernel_trace_mutex);
+	if (--kernel_trace_refcount)
+		goto end;
+	read_lock(&tasklist_lock);
+	do_each_thread(p, t) {
+		clear_tsk_thread_flag(t, TIF_KERNEL_TRACE);
+	} while_each_thread(p, t);
+	read_unlock(&tasklist_lock);
+end:
+	mutex_unlock(&kernel_trace_mutex);
+}
+EXPORT_SYMBOL_GPL(clear_kernel_trace_flag_all_tasks);
+
+/**
+ * set_kernel_trace_flag_all_tasks - sets all TIF_KERNEL_TRACE thread flags.
+ *
+ * This function iterates on all threads in the system to set their
+ * TIF_KERNEL_TRACE flag. Setting the TIF_KERNEL_TRACE flag with the
+ * tasklist_lock held in copy_process() makes sure that once we finish setting
+ * the thread flags, all threads have their flags set.
+ */
+void set_kernel_trace_flag_all_tasks(void)
+{
+	struct task_struct *p;
+	struct task_struct *t;
+
+	mutex_lock(&kernel_trace_mutex);
+	if (kernel_trace_refcount++)
+		goto end;
+	read_lock(&tasklist_lock);
+	do_each_thread(p, t) {
+		set_tsk_thread_flag(t, TIF_KERNEL_TRACE);
+	} while_each_thread(p, t);
+	read_unlock(&tasklist_lock);
+end:
+	mutex_unlock(&kernel_trace_mutex);
+}
+EXPORT_SYMBOL_GPL(set_kernel_trace_flag_all_tasks);
diff --git a/stblinux-2.6.31/kernel/sched_fair.c b/stblinux-2.6.31/kernel/sched_fair.c
index 652e8bd..42f79b8 100644
--- a/stblinux-2.6.31/kernel/sched_fair.c
+++ b/stblinux-2.6.31/kernel/sched_fair.c
@@ -649,11 +649,8 @@ static void enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)
 			 * 20 to get a milliseconds-range estimation of the
 			 * amount of time that the task spent sleeping:
 			 */
-			if (unlikely(prof_on == SLEEP_PROFILING)) {
-				profile_hits(SLEEP_PROFILING,
-						(void *)get_wchan(tsk),
-						delta >> 20);
-			}
+			profile_hits(SLEEP_PROFILING, (void *)get_wchan(tsk),
+				     delta >> 20);
 			account_scheduler_latency(tsk, delta >> 10, 0);
 		}
 	}
diff --git a/stblinux-2.6.31/kernel/softirq.c b/stblinux-2.6.31/kernel/softirq.c
index eb5e131..3db60fa 100644
--- a/stblinux-2.6.31/kernel/softirq.c
+++ b/stblinux-2.6.31/kernel/softirq.c
@@ -23,7 +23,10 @@
 #include <linux/rcupdate.h>
 #include <linux/ftrace.h>
 #include <linux/smp.h>
+#include <linux/marker.h>
+#include <linux/kallsyms.h>
 #include <linux/tick.h>
+#include <trace/irq.h>
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/irq.h>
@@ -54,6 +57,20 @@ EXPORT_SYMBOL(irq_stat);
 
 static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;
 
+void ltt_dump_softirq_vec(void *call_data)
+{
+	int i;
+	char namebuf[KSYM_NAME_LEN];
+
+	for (i = 0; i < 32; i++) {
+		sprint_symbol(namebuf, (unsigned long)softirq_vec[i].action);
+		__trace_mark(0, softirq_state, softirq_vec, call_data,
+			"id %d address %p symbol %s",
+			i, softirq_vec[i].action, namebuf);
+	}
+}
+EXPORT_SYMBOL_GPL(ltt_dump_softirq_vec);
+
 static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
 
 char *softirq_to_name[NR_SOFTIRQS] = {
@@ -61,6 +78,11 @@ char *softirq_to_name[NR_SOFTIRQS] = {
 	"TASKLET", "SCHED", "HRTIMER",	"RCU"
 };
 
+DEFINE_TRACE(irq_tasklet_high_entry);
+DEFINE_TRACE(irq_tasklet_high_exit);
+DEFINE_TRACE(irq_tasklet_low_entry);
+DEFINE_TRACE(irq_tasklet_low_exit);
+
 /*
  * we cannot loop indefinitely here to avoid userspace starvation,
  * but we also don't want to introduce a worst case 1/HZ latency
@@ -188,6 +210,8 @@ EXPORT_SYMBOL(local_bh_enable_ip);
  */
 #define MAX_SOFTIRQ_RESTART 10
 
+DEFINE_TRACE(softirq_raise);
+
 asmlinkage void __do_softirq(void)
 {
 	struct softirq_action *h;
@@ -316,6 +340,7 @@ void irq_exit(void)
  */
 inline void raise_softirq_irqoff(unsigned int nr)
 {
+	trace_softirq_raise(nr);
 	__raise_softirq_irqoff(nr);
 
 	/*
@@ -415,7 +440,9 @@ static void tasklet_action(struct softirq_action *a)
 			if (!atomic_read(&t->count)) {
 				if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
 					BUG();
+				trace_irq_tasklet_low_entry(t);
 				t->func(t->data);
+				trace_irq_tasklet_low_exit(t);
 				tasklet_unlock(t);
 				continue;
 			}
@@ -450,7 +477,9 @@ static void tasklet_hi_action(struct softirq_action *a)
 			if (!atomic_read(&t->count)) {
 				if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
 					BUG();
+				trace_irq_tasklet_high_entry(t);
 				t->func(t->data);
+				trace_irq_tasklet_high_exit(t);
 				tasklet_unlock(t);
 				continue;
 			}
diff --git a/stblinux-2.6.31/kernel/time/Makefile b/stblinux-2.6.31/kernel/time/Makefile
index 0b0a636..7855f16 100644
--- a/stblinux-2.6.31/kernel/time/Makefile
+++ b/stblinux-2.6.31/kernel/time/Makefile
@@ -6,3 +6,4 @@ obj-$(CONFIG_GENERIC_CLOCKEVENTS_BROADCAST)	+= tick-broadcast.o
 obj-$(CONFIG_TICK_ONESHOT)			+= tick-oneshot.o
 obj-$(CONFIG_TICK_ONESHOT)			+= tick-sched.o
 obj-$(CONFIG_TIMER_STATS)			+= timer_stats.o
+obj-$(CONFIG_HAVE_UNSYNCHRONIZED_TSC)		+= tsc-sync.o
diff --git a/stblinux-2.6.31/kernel/time/tsc-sync.c b/stblinux-2.6.31/kernel/time/tsc-sync.c
new file mode 100644
index 0000000..fc181d8
--- /dev/null
+++ b/stblinux-2.6.31/kernel/time/tsc-sync.c
@@ -0,0 +1,313 @@
+/*
+ * kernel/time/tsc-sync.c
+ *
+ * Test TSC synchronization
+ *
+ * marks the tsc as unstable _and_ keep a simple "_tsc_is_sync" variable, which
+ * is fast to read when a simple test must determine which clock source to use
+ * for kernel tracing.
+ *
+ * - CPU init :
+ *
+ * We check whether all boot CPUs have their TSC's synchronized,
+ * print a warning if not and turn off the TSC clock-source.
+ *
+ * Only two CPUs may participate - they can enter in any order.
+ * ( The serial nature of the boot logic and the CPU hotplug lock
+ *   protects against more than 2 CPUs entering this code.
+ *
+ * - When CPUs are up :
+ *
+ * TSC synchronicity of all CPUs can be checked later at run-time by calling
+ * test_tsc_synchronization().
+ *
+ * Copyright 2007, 2008
+ *    Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ */
+#include <linux/module.h>
+#include <linux/timer.h>
+#include <linux/timex.h>
+#include <linux/jiffies.h>
+#include <linux/trace-clock.h>
+#include <linux/cpu.h>
+#include <linux/kthread.h>
+#include <linux/mutex.h>
+#include <linux/cpu.h>
+
+#define MAX_CYCLES_DELTA 1000ULL
+
+/*
+ * Number of loops to take care of MCE, NMIs, SMIs.
+ */
+#define NR_LOOPS	10
+
+static DEFINE_MUTEX(tscsync_mutex);
+
+struct sync_data {
+	int nr_waits;
+	int wait_sync;
+	cycles_t tsc_count;
+} ____cacheline_aligned;
+
+/* 0 is master, 1 is slave */
+static struct sync_data sync_data[2] = {
+	[0 ... 1] = {
+		.nr_waits = 3 * NR_LOOPS + 1,
+		.wait_sync = 3 * NR_LOOPS + 1,
+	},
+};
+
+int _tsc_is_sync = 1;
+EXPORT_SYMBOL(_tsc_is_sync);
+
+static int force_tsc_sync;
+static cycles_t slave_offset;
+static int slave_offset_ready;	/* for 32-bits architectures */
+
+static int __init force_tsc_sync_setup(char *str)
+{
+	force_tsc_sync = simple_strtoul(str, NULL, 0);
+	return 1;
+}
+__setup("force_tsc_sync=", force_tsc_sync_setup);
+
+/*
+ * Mark it noinline so we make sure it is not unrolled.
+ * Wait until value is reached.
+ */
+static noinline void tsc_barrier(long this_cpu)
+{
+	sync_core();
+	sync_data[this_cpu].wait_sync--;
+	smp_mb();	/* order master/slave sync_data read/write */
+	while (unlikely(sync_data[1 - this_cpu].wait_sync >=
+			sync_data[this_cpu].nr_waits))
+		barrier();	/*
+				 * barrier is used because faster and
+				 * more predictable than cpu_idle().
+				 */
+	smp_mb();	/* order master/slave sync_data read/write */
+	sync_data[this_cpu].nr_waits--;
+	get_cycles_barrier();
+	sync_data[this_cpu].tsc_count = get_cycles();
+	get_cycles_barrier();
+}
+
+/*
+ * Worker thread called on each CPU.
+ * First wait with interrupts enabled, then wait with interrupt disabled,
+ * for precision. We are already bound to one CPU.
+ * this_cpu 0 : master
+ * this_cpu 1 : slave
+ */
+static void test_sync(void *arg)
+{
+	long this_cpu = (long)arg;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	/* Make sure the instructions are in I-CACHE */
+	tsc_barrier(this_cpu);
+	tsc_barrier(this_cpu);
+	sync_data[this_cpu].wait_sync--;
+	smp_mb();	/* order master/slave sync_data read/write */
+	while (unlikely(sync_data[1 - this_cpu].wait_sync >=
+			sync_data[this_cpu].nr_waits))
+		barrier();	/*
+				 * barrier is used because faster and
+				 * more predictable than cpu_idle().
+				 */
+	smp_mb();	/* order master/slave sync_data read/write */
+	sync_data[this_cpu].nr_waits--;
+	/*
+	 * Here, only the master will wait for the slave to reach this barrier.
+	 * This makes sure that the master, which holds the mutex and will reset
+	 * the barriers, waits for the slave to stop using the barrier values
+	 * before it continues. This is only done at the complete end of all the
+	 * loops. This is why there is a + 1 in original wait_sync value.
+	 */
+	if (sync_data[this_cpu].nr_waits == 1)
+		sync_data[this_cpu].wait_sync--;
+	local_irq_restore(flags);
+}
+
+/*
+ * Each CPU (master and target) must decrement the wait_sync value twice (one
+ * for priming in cache), and also once after the get_cycles. After all the
+ * loops, one last synchronization is required to make sure the master waits
+ * for the slave before resetting the barriers.
+ */
+static void reset_barriers(void)
+{
+	int i;
+
+	/*
+	 * Wait until slave is done so that we don't overwrite
+	 * wait_end_sync prematurely.
+	 */
+	smp_mb();	/* order master/slave sync_data read/write */
+	while (unlikely(sync_data[1].wait_sync >= sync_data[0].nr_waits))
+		barrier();	/*
+				 * barrier is used because faster and
+				 * more predictable than cpu_idle().
+				 */
+	smp_mb();	/* order master/slave sync_data read/write */
+
+	for (i = 0; i < 2; i++) {
+		WARN_ON(sync_data[i].wait_sync != 0);
+		WARN_ON(sync_data[i].nr_waits != 1);
+		sync_data[i].wait_sync = 3 * NR_LOOPS + 1;
+		sync_data[i].nr_waits = 3 * NR_LOOPS + 1;
+	}
+}
+
+/*
+ * Do loops (making sure no unexpected event changes the timing), keep the best
+ * one. The result of each loop is the highest tsc delta between the master CPU
+ * and the slaves. Stop CPU hotplug when this code is executed to make sure we
+ * are concurrency-safe wrt CPU hotplug also using this code.  Test TSC
+ * synchronization even if we already "know" CPUs were not synchronized. This
+ * can be used as a test to check if, for some reason, the CPUs eventually got
+ * in sync after a CPU has been unplugged. This code is kept separate from the
+ * CPU hotplug code because the slave CPU executes in an IPI, which we want to
+ * keep as short as possible (this is happening while the system is running).
+ * Therefore, we do not send a single IPI for all the test loops, but rather
+ * send one IPI per loop.
+ */
+int test_tsc_synchronization(void)
+{
+	long cpu, master;
+	cycles_t max_diff = 0, diff, best_loop, worse_loop = 0;
+	int i;
+
+	mutex_lock(&tscsync_mutex);
+	get_online_cpus();
+
+	printk(KERN_INFO
+	       "checking TSC synchronization across all online CPUs:");
+
+	preempt_disable();
+	master = smp_processor_id();
+	for_each_online_cpu(cpu) {
+		if (master == cpu)
+			continue;
+		best_loop = (cycles_t)ULLONG_MAX;
+		for (i = 0; i < NR_LOOPS; i++) {
+			smp_call_function_single(cpu, test_sync,
+						(void *)1UL, 0);
+			test_sync((void *)0UL);
+			diff = abs(sync_data[1].tsc_count
+				- sync_data[0].tsc_count);
+			best_loop = min(best_loop, diff);
+			worse_loop = max(worse_loop, diff);
+		}
+		reset_barriers();
+		max_diff = max(best_loop, max_diff);
+	}
+	preempt_enable();
+	if (max_diff >= MAX_CYCLES_DELTA) {
+		printk(KERN_WARNING
+			"Measured %llu cycles TSC offset between CPUs,"
+			" turning off TSC clock.\n", (u64)max_diff);
+		mark_tsc_unstable("check_tsc_sync_source failed");
+		_tsc_is_sync = 0;
+	} else {
+		printk(" passed.\n");
+	}
+	put_online_cpus();
+	mutex_unlock(&tscsync_mutex);
+	return max_diff < MAX_CYCLES_DELTA;
+}
+EXPORT_SYMBOL_GPL(test_tsc_synchronization);
+
+/*
+ * Test synchronicity of a single core when it is hotplugged.
+ * Source CPU calls into this - waits for the freshly booted target CPU to
+ * arrive and then start the measurement:
+ */
+void __cpuinit check_tsc_sync_source(int cpu)
+{
+	cycles_t diff, abs_diff,
+		 best_loop = (cycles_t)ULLONG_MAX, worse_loop = 0;
+	int i;
+
+	/*
+	 * No need to check if we already know that the TSC is not synchronized:
+	 */
+	if (!force_tsc_sync && unsynchronized_tsc()) {
+		/*
+		 * Make sure we mark _tsc_is_sync to 0 if the TSC is found
+		 * to be unsynchronized for other causes than non-synchronized
+		 * TSCs across CPUs.
+		 */
+		_tsc_is_sync = 0;
+		set_trace_clock_is_sync(0);
+		return;
+	}
+
+	printk(KERN_INFO "checking TSC synchronization [CPU#%d -> CPU#%d]:",
+			  smp_processor_id(), cpu);
+
+	for (i = 0; i < NR_LOOPS; i++) {
+		test_sync((void *)0UL);
+		diff = sync_data[1].tsc_count - sync_data[0].tsc_count;
+		abs_diff = abs(diff);
+		best_loop = min(best_loop, abs_diff);
+		worse_loop = max(worse_loop, abs_diff);
+		if (force_tsc_sync && best_loop == abs_diff)
+			slave_offset = diff;
+	}
+	reset_barriers();
+
+	if (!force_tsc_sync && best_loop >= MAX_CYCLES_DELTA) {
+		printk(" failed.\n");
+		printk(KERN_WARNING
+			"Measured %llu cycles TSC offset between CPUs,"
+			" turning off TSC clock.\n", (u64)best_loop);
+		mark_tsc_unstable("check_tsc_sync_source failed");
+		_tsc_is_sync = 0;
+		set_trace_clock_is_sync(0);
+	} else {
+		printk(" %s.\n", !force_tsc_sync ? "passed" : "forced");
+	}
+	if (force_tsc_sync) {
+		/* order slave_offset and slave_offset_ready writes */
+		smp_wmb();
+		slave_offset_ready = 1;
+	}
+}
+
+/*
+ * Freshly booted CPUs call into this:
+ */
+void __cpuinit check_tsc_sync_target(void)
+{
+	int i;
+
+	if (!force_tsc_sync && unsynchronized_tsc())
+		return;
+
+	for (i = 0; i < NR_LOOPS; i++)
+		test_sync((void *)1UL);
+
+	/*
+	 * Force slave synchronization if requested.
+	 */
+	if (force_tsc_sync) {
+		unsigned long flags;
+		cycles_t new_tsc;
+
+		while (!slave_offset_ready)
+			cpu_relax();
+		/* order slave_offset and slave_offset_ready reads */
+		smp_rmb();
+		local_irq_save(flags);
+		/*
+		 * slave_offset is read when master has finished writing to it,
+		 * and is protected by cpu hotplug serialization.
+		 */
+		new_tsc = get_cycles() - slave_offset;
+		write_tsc((u32)new_tsc, (u32)((u64)new_tsc >> 32));
+		local_irq_restore(flags);
+	}
+}
diff --git a/stblinux-2.6.31/kernel/timer.c b/stblinux-2.6.31/kernel/timer.c
index a7f07d5..1a9c0ee 100644
--- a/stblinux-2.6.31/kernel/timer.c
+++ b/stblinux-2.6.31/kernel/timer.c
@@ -39,17 +39,23 @@
 #include <linux/kallsyms.h>
 #include <linux/perf_counter.h>
 #include <linux/sched.h>
+#include <trace/timer.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
 #include <asm/div64.h>
 #include <asm/timex.h>
 #include <asm/io.h>
+#include <asm/irq_regs.h>
 
 u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;
 
 EXPORT_SYMBOL(jiffies_64);
 
+DEFINE_TRACE(timer_set);
+DEFINE_TRACE(timer_update_time);
+DEFINE_TRACE(timer_timeout);
+
 /*
  * per-CPU timer vector definitions:
  */
@@ -359,6 +365,7 @@ static void internal_add_timer(struct tvec_base *base, struct timer_list *timer)
 		i = (expires >> (TVR_BITS + 3 * TVN_BITS)) & TVN_MASK;
 		vec = base->tv5.vec + i;
 	}
+	trace_timer_set(timer);
 	/*
 	 * Timers are FIFO:
 	 */
@@ -1198,6 +1205,7 @@ void do_timer(unsigned long ticks)
 {
 	jiffies_64 += ticks;
 	update_wall_time();
+	trace_timer_update_time(&xtime, &wall_to_monotonic);
 	calc_global_load();
 }
 
@@ -1280,7 +1288,9 @@ SYSCALL_DEFINE0(getegid)
 
 static void process_timeout(unsigned long __data)
 {
-	wake_up_process((struct task_struct *)__data);
+	struct task_struct *task = (struct task_struct *)__data;
+	trace_timer_timeout(task);
+	wake_up_process(task);
 }
 
 /**
diff --git a/stblinux-2.6.31/kernel/trace/Makefile b/stblinux-2.6.31/kernel/trace/Makefile
index 844164d..280de8c 100644
--- a/stblinux-2.6.31/kernel/trace/Makefile
+++ b/stblinux-2.6.31/kernel/trace/Makefile
@@ -54,5 +54,7 @@ obj-$(CONFIG_EVENT_TRACING) += trace_export.o
 obj-$(CONFIG_FTRACE_SYSCALLS) += trace_syscalls.o
 obj-$(CONFIG_EVENT_PROFILE) += trace_event_profile.o
 obj-$(CONFIG_EVENT_TRACING) += trace_events_filter.o
+obj-$(CONFIG_HAVE_TRACE_CLOCK_32_TO_64) += trace-clock-32-to-64.o
+obj-$(CONFIG_HAVE_TRACE_CLOCK_GENERIC) += trace-clock.o
 
 libftrace-y := ftrace.o
diff --git a/stblinux-2.6.31/kernel/trace/trace-clock-32-to-64.c b/stblinux-2.6.31/kernel/trace/trace-clock-32-to-64.c
new file mode 100644
index 0000000..1e0a938
--- /dev/null
+++ b/stblinux-2.6.31/kernel/trace/trace-clock-32-to-64.c
@@ -0,0 +1,307 @@
+/*
+ * kernel/trace/trace-clock-32-to-64.c
+ *
+ * (C) Copyright	2006,2007,2008 -
+ * 		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Extends a 32 bits clock source to a full 64 bits count, readable atomically
+ * from any execution context.
+ *
+ * notes :
+ * - trace clock 32->64 bits extended timer-based clock cannot be used for early
+ *   tracing in the boot process, as it depends on timer interrupts.
+ * - The timer is only on one CPU to support hotplug.
+ * - We have the choice between schedule_delayed_work_on and an IPI to get each
+ *   CPU to write the heartbeat. IPI has been chosen because it is considered
+ *   faster than passing through the timer to get the work scheduled on all the
+ *   CPUs.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/cpu.h>
+#include <linux/timex.h>
+#include <linux/bitops.h>
+#include <linux/trace-clock.h>
+#include <linux/smp.h>
+#include <linux/sched.h> /* needed due to include order problem on m68k */
+#include <linux/math64.h>
+
+#define HW_BITMASK			((1ULL << TC_HW_BITS) - 1)
+#define HW_LS32(hw)			((hw) & HW_BITMASK)
+#define SW_MS32(sw)			((sw) & ~HW_BITMASK)
+
+static DEFINE_SPINLOCK(synthetic_tsc_lock);
+static int synthetic_tsc_refcount;  /* Number of readers */
+static int synthetic_tsc_enabled;   /* synth. TSC enabled on all online CPUs */
+
+static DEFINE_PER_CPU(struct timer_list, tsc_timer);
+static unsigned int precalc_expire;
+
+struct synthetic_tsc_struct {
+	union {
+		u64 val;
+		struct {
+#ifdef __BIG_ENDIAN
+			u32 ms32;
+			u32 ls32;
+#else
+			u32 ls32;
+			u32 ms32;
+#endif
+		} sel;
+	} tsc[2];
+	unsigned int index;	/* Index of the current synth. tsc. */
+};
+
+static DEFINE_PER_CPU(struct synthetic_tsc_struct, synthetic_tsc);
+
+/* Called from IPI or timer interrupt */
+static void update_synthetic_tsc(void)
+{
+	struct synthetic_tsc_struct *cpu_synth;
+	u32 tsc;
+
+	cpu_synth = &per_cpu(synthetic_tsc, smp_processor_id());
+	tsc = trace_clock_read32();		/* Hardware clocksource read */
+
+	if (tsc < HW_LS32(cpu_synth->tsc[cpu_synth->index].sel.ls32)) {
+		unsigned int new_index = 1 - cpu_synth->index; /* 0 <-> 1 */
+		/*
+		 * Overflow
+		 * Non atomic update of the non current synthetic TSC, followed
+		 * by an atomic index change. There is no write concurrency,
+		 * so the index read/write does not need to be atomic.
+		 */
+		cpu_synth->tsc[new_index].val =
+			(SW_MS32(cpu_synth->tsc[cpu_synth->index].val)
+				| (u64)tsc) + (1ULL << TC_HW_BITS);
+		/*
+		 * Ensure the compiler does not reorder index write. It makes
+		 * sure all nested interrupts will see the new value before the
+		 * new index is written.
+		 */
+		barrier();
+		cpu_synth->index = new_index;	/* atomic change of index */
+	} else {
+		/*
+		 * No overflow : We know that the only bits changed are
+		 * contained in the 32 LS32s, which can be written to atomically.
+		 */
+		cpu_synth->tsc[cpu_synth->index].sel.ls32 =
+			SW_MS32(cpu_synth->tsc[cpu_synth->index].sel.ls32) | tsc;
+	}
+}
+
+/*
+ * Should only be called when the synthetic clock is not used.
+ */
+void _trace_clock_write_synthetic_tsc(u64 value)
+{
+	struct synthetic_tsc_struct *cpu_synth;
+	int cpu;
+
+	for_each_online_cpu(cpu) {
+		cpu_synth = &per_cpu(synthetic_tsc, cpu);
+		cpu_synth->tsc[cpu_synth->index].val = value;
+	}
+}
+
+/* Called from buffer switch : in _any_ context (even NMI) */
+u64 notrace trace_clock_read_synthetic_tsc(void)
+{
+	struct synthetic_tsc_struct *cpu_synth;
+	u64 ret;
+	unsigned int index;
+	u32 tsc;
+
+	preempt_disable_notrace();
+	cpu_synth = &per_cpu(synthetic_tsc, smp_processor_id());
+	index = ACCESS_ONCE(cpu_synth->index);	/* atomic read */
+	tsc = trace_clock_read32();		/* Hardware clocksource read */
+
+	/* Overflow detection */
+	if (unlikely(tsc < HW_LS32(cpu_synth->tsc[index].sel.ls32)))
+		ret = (SW_MS32(cpu_synth->tsc[index].val) | (u64)tsc)
+			+ (1ULL << TC_HW_BITS);
+	else
+		ret = SW_MS32(cpu_synth->tsc[index].val) | (u64)tsc;
+	preempt_enable_notrace();
+	return ret;
+}
+EXPORT_SYMBOL_GPL(trace_clock_read_synthetic_tsc);
+
+static void synthetic_tsc_ipi(void *info)
+{
+	update_synthetic_tsc();
+}
+
+/*
+ * tsc_timer_fct : - Timer function synchronizing synthetic TSC.
+ * @data: unused
+ *
+ * Guarantees at least 1 execution before low word of TSC wraps.
+ */
+static void tsc_timer_fct(unsigned long data)
+{
+	update_synthetic_tsc();
+
+	per_cpu(tsc_timer, smp_processor_id()).expires =
+		jiffies + precalc_expire;
+	add_timer_on(&per_cpu(tsc_timer, smp_processor_id()),
+		     smp_processor_id());
+}
+
+/*
+ * precalc_stsc_interval: - Precalculates the interval between the clock
+ * wraparounds.
+ */
+static int __init precalc_stsc_interval(void)
+{
+	u64 rem_freq, rem_interval;
+
+	precalc_expire =
+		__iter_div_u64_rem(HW_BITMASK, (
+		  __iter_div_u64_rem(trace_clock_frequency(),
+		  HZ * trace_clock_freq_scale(), &rem_freq) << 1
+		 )
+		 - 1
+		 - (TC_EXPECTED_INTERRUPT_LATENCY * HZ / 1000), &rem_interval)
+		>> 1;
+	WARN_ON(precalc_expire == 0);
+	printk(KERN_DEBUG "Synthetic TSC timer will fire each %u jiffies.\n",
+		precalc_expire);
+	return 0;
+}
+
+static void prepare_synthetic_tsc(int cpu)
+{
+	struct synthetic_tsc_struct *cpu_synth;
+	u64 local_count;
+
+	cpu_synth = &per_cpu(synthetic_tsc, cpu);
+	local_count = trace_clock_read_synthetic_tsc();
+	cpu_synth->tsc[0].val = local_count;
+	cpu_synth->index = 0;
+	smp_wmb();	/* Writing in data of CPU about to come up */
+	init_timer(&per_cpu(tsc_timer, cpu));
+	per_cpu(tsc_timer, cpu).function = tsc_timer_fct;
+	per_cpu(tsc_timer, cpu).expires = jiffies + precalc_expire;
+}
+
+static void enable_synthetic_tsc(int cpu)
+{
+	smp_call_function_single(cpu, synthetic_tsc_ipi, NULL, 1);
+	add_timer_on(&per_cpu(tsc_timer, cpu), cpu);
+}
+
+/*
+ * Cannot use del_timer_sync with add_timer_on, so use an IPI to locally
+ * delete the timer.
+ */
+static void disable_synthetic_tsc_ipi(void *info)
+{
+	del_timer(&per_cpu(tsc_timer, smp_processor_id()));
+}
+
+static void disable_synthetic_tsc(int cpu)
+{
+	smp_call_function_single(cpu, disable_synthetic_tsc_ipi, NULL, 1);
+}
+
+/*
+ * 	hotcpu_callback - CPU hotplug callback
+ * 	@nb: notifier block
+ * 	@action: hotplug action to take
+ * 	@hcpu: CPU number
+ *
+ *	Sets the new CPU's current synthetic TSC to the same value as the
+ *	currently running CPU.
+ *
+ * 	Returns the success/failure of the operation. (NOTIFY_OK, NOTIFY_BAD)
+ */
+static int __cpuinit hotcpu_callback(struct notifier_block *nb,
+				unsigned long action,
+				void *hcpu)
+{
+	unsigned int hotcpu = (unsigned long)hcpu;
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+	case CPU_UP_PREPARE_FROZEN:
+		spin_lock(&synthetic_tsc_lock);
+		if (synthetic_tsc_refcount)
+			prepare_synthetic_tsc(hotcpu);
+		spin_unlock(&synthetic_tsc_lock);
+		break;
+	case CPU_ONLINE:
+	case CPU_ONLINE_FROZEN:
+		spin_lock(&synthetic_tsc_lock);
+		if (synthetic_tsc_refcount)
+			enable_synthetic_tsc(hotcpu);
+		spin_unlock(&synthetic_tsc_lock);
+		break;
+#ifdef CONFIG_HOTPLUG_CPU
+	case CPU_UP_CANCELED:
+	case CPU_UP_CANCELED_FROZEN:
+	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
+		spin_lock(&synthetic_tsc_lock);
+		if (synthetic_tsc_refcount)
+			disable_synthetic_tsc(hotcpu);
+		spin_unlock(&synthetic_tsc_lock);
+		break;
+#endif /* CONFIG_HOTPLUG_CPU */
+	}
+	return NOTIFY_OK;
+}
+
+void get_synthetic_tsc(void)
+{
+	int cpu;
+
+	spin_lock(&synthetic_tsc_lock);
+	if (synthetic_tsc_refcount++)
+		goto end;
+
+	synthetic_tsc_enabled = 1;
+	for_each_online_cpu(cpu) {
+		prepare_synthetic_tsc(cpu);
+		enable_synthetic_tsc(cpu);
+	}
+end:
+	spin_unlock(&synthetic_tsc_lock);
+}
+EXPORT_SYMBOL_GPL(get_synthetic_tsc);
+
+void put_synthetic_tsc(void)
+{
+	int cpu;
+
+	spin_lock(&synthetic_tsc_lock);
+	WARN_ON(synthetic_tsc_refcount <= 0);
+	if (synthetic_tsc_refcount != 1 || !synthetic_tsc_enabled)
+		goto end;
+
+	for_each_online_cpu(cpu)
+		disable_synthetic_tsc(cpu);
+	synthetic_tsc_enabled = 0;
+end:
+	synthetic_tsc_refcount--;
+	spin_unlock(&synthetic_tsc_lock);
+}
+EXPORT_SYMBOL_GPL(put_synthetic_tsc);
+
+/* Called from CPU 0, before any tracing starts, to init each structure */
+static int __init init_synthetic_tsc(void)
+{
+	precalc_stsc_interval();
+	hotcpu_notifier(hotcpu_callback, 3);
+	return 0;
+}
+
+/* Before SMP is up */
+early_initcall(init_synthetic_tsc);
diff --git a/stblinux-2.6.31/kernel/trace/trace-clock.c b/stblinux-2.6.31/kernel/trace/trace-clock.c
new file mode 100644
index 0000000..3ed1667
--- /dev/null
+++ b/stblinux-2.6.31/kernel/trace/trace-clock.c
@@ -0,0 +1,97 @@
+/*
+ * kernel/trace/trace-clock.c
+ *
+ * (C) Copyright	2008 -
+ * 		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Generic kernel tracing clock for architectures without TSC.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/cpu.h>
+#include <linux/timex.h>
+#include <linux/bitops.h>
+#include <linux/trace-clock.h>
+#include <linux/jiffies.h>
+
+static int trace_clock_refcount;
+static DEFINE_MUTEX(trace_clock_mutex);
+static struct timer_list trace_clock_timer;
+/*
+ * bits 0..12 : counter, atomically incremented
+ * bits 13..{32,64} : time counter, incremented each jiffy.
+ */
+atomic_long_t trace_clock_var;
+EXPORT_SYMBOL(trace_clock_var);
+
+static void trace_clock_update(void)
+{
+	long old_clock, new_clock;
+	unsigned long ticks;
+
+	/*
+	 * Make sure we keep track of delayed timer.
+	 */
+	ticks = jiffies - trace_clock_timer.expires + 1;
+	/* Don't update if ticks is zero, time would go backward. */
+	if (unlikely(!ticks))
+		return;
+	do {
+		old_clock = atomic_long_read(&trace_clock_var);
+		new_clock = (old_clock + (ticks << TRACE_CLOCK_SHIFT))
+			& (~((1 << TRACE_CLOCK_SHIFT) - 1));
+	} while (atomic_long_cmpxchg(&trace_clock_var, old_clock, new_clock)
+			!= old_clock);
+}
+
+static void trace_clock_timer_fct(unsigned long data)
+{
+	trace_clock_update();
+	trace_clock_timer.expires = jiffies + 1;
+	add_timer(&trace_clock_timer);
+}
+
+static void enable_trace_clock(void)
+{
+	init_timer(&trace_clock_timer);
+	/* trace_clock_update() reads expires */
+	trace_clock_timer.function = trace_clock_timer_fct;
+	trace_clock_timer.expires = jiffies + 1;
+	trace_clock_update();
+	add_timer(&trace_clock_timer);
+}
+
+static void disable_trace_clock(void)
+{
+	del_timer_sync(&trace_clock_timer);
+}
+
+void get_trace_clock(void)
+{
+	get_synthetic_tsc();
+	mutex_lock(&trace_clock_mutex);
+	if (trace_clock_refcount++)
+		goto end;
+	enable_trace_clock();
+end:
+	mutex_unlock(&trace_clock_mutex);
+}
+EXPORT_SYMBOL_GPL(get_trace_clock);
+
+void put_trace_clock(void)
+{
+	mutex_lock(&trace_clock_mutex);
+	WARN_ON(trace_clock_refcount <= 0);
+	if (trace_clock_refcount != 1)
+		goto end;
+	disable_trace_clock();
+end:
+	trace_clock_refcount--;
+	mutex_unlock(&trace_clock_mutex);
+	put_synthetic_tsc();
+}
+EXPORT_SYMBOL_GPL(put_trace_clock);
diff --git a/stblinux-2.6.31/kernel/tracepoint.c b/stblinux-2.6.31/kernel/tracepoint.c
index 1ef5d3a..10fceca 100644
--- a/stblinux-2.6.31/kernel/tracepoint.c
+++ b/stblinux-2.6.31/kernel/tracepoint.c
@@ -24,6 +24,7 @@
 #include <linux/tracepoint.h>
 #include <linux/err.h>
 #include <linux/slab.h>
+#include <linux/immediate.h>
 
 extern struct tracepoint __start___tracepoints[];
 extern struct tracepoint __stop___tracepoints[];
@@ -250,7 +251,7 @@ static void set_tracepoint(struct tracepoint_entry **entry,
 	 * is used.
 	 */
 	rcu_assign_pointer(elem->funcs, (*entry)->funcs);
-	elem->state = active;
+	elem->state__imv = active;
 }
 
 /*
@@ -261,7 +262,7 @@ static void set_tracepoint(struct tracepoint_entry **entry,
  */
 static void disable_tracepoint(struct tracepoint *elem)
 {
-	elem->state = 0;
+	elem->state__imv = 0;
 	rcu_assign_pointer(elem->funcs, NULL);
 }
 
@@ -304,6 +305,9 @@ static void tracepoint_update_probes(void)
 		__stop___tracepoints);
 	/* tracepoints in modules. */
 	module_update_tracepoints();
+	/* Update immediate values */
+	core_imv_update();
+	module_imv_update();
 }
 
 static void *tracepoint_add_probe(const char *name, void *probe)
diff --git a/stblinux-2.6.31/lib/Kconfig.debug b/stblinux-2.6.31/lib/Kconfig.debug
index 12327b2..f1974d5 100644
--- a/stblinux-2.6.31/lib/Kconfig.debug
+++ b/stblinux-2.6.31/lib/Kconfig.debug
@@ -425,11 +425,19 @@ config DEBUG_MUTEXES
 	 This feature allows mutex semantics violations to be detected and
 	 reported.
 
+config DEBUG_PSRWLOCK
+	bool "Priority-Sifting Reader-Writer Locks: basic checks"
+	depends on DEBUG_KERNEL
+	help
+	 This feature allows psrwlock semantics violations to be detected and
+	 reported.
+
 config DEBUG_LOCK_ALLOC
 	bool "Lock debugging: detect incorrect freeing of live locks"
 	depends on DEBUG_KERNEL && TRACE_IRQFLAGS_SUPPORT && STACKTRACE_SUPPORT && LOCKDEP_SUPPORT
 	select DEBUG_SPINLOCK
 	select DEBUG_MUTEXES
+	select DEBUG_PSRWLOCK
 	select LOCKDEP
 	help
 	 This feature will check whether any held lock (spinlock, rwlock,
@@ -445,6 +453,7 @@ config PROVE_LOCKING
 	select LOCKDEP
 	select DEBUG_SPINLOCK
 	select DEBUG_MUTEXES
+	select DEBUG_PSRWLOCK
 	select DEBUG_LOCK_ALLOC
 	default n
 	help
@@ -495,6 +504,7 @@ config LOCK_STAT
 	select LOCKDEP
 	select DEBUG_SPINLOCK
 	select DEBUG_MUTEXES
+	select DEBUG_PSRWLOCK
 	select DEBUG_LOCK_ALLOC
 	default n
 	help
@@ -633,6 +643,15 @@ config DEBUG_LIST
 
 	  If unsure, say N.
 
+config DEBUG_RCU_HEAD
+	bool "Debug RCU callbacks"
+	depends on DEBUG_KERNEL
+	depends on TREE_RCU
+	help
+	  Enable this to turn on debugging of RCU list heads (call_rcu() usage).
+	  Seems to find problems more quickly with stress-tests in single-cpu
+	  mode.
+
 config DEBUG_SG
 	bool "Debug SG table operations"
 	depends on DEBUG_KERNEL
@@ -858,6 +877,14 @@ config FAULT_INJECTION_STACKTRACE_FILTER
 	help
 	  Provide stacktrace filter for fault-injection capabilities
 
+config HAVE_PSRWLOCK_ASM_CALL
+	def_bool n
+
+config PSRWLOCK_LATENCY_TEST
+	boolean "Testing API for psrwlock latency test"
+	depends on HAVE_GET_CYCLES
+	help
+
 config LATENCYTOP
 	bool "Latency measuring infrastructure"
 	select FRAME_POINTER if !MIPS && !PPC && !S390
diff --git a/stblinux-2.6.31/lib/Makefile b/stblinux-2.6.31/lib/Makefile
index 2e78277..a0db8de 100644
--- a/stblinux-2.6.31/lib/Makefile
+++ b/stblinux-2.6.31/lib/Makefile
@@ -45,6 +45,10 @@ obj-$(CONFIG_DEBUG_PREEMPT) += smp_processor_id.o
 obj-$(CONFIG_DEBUG_LIST) += list_debug.o
 obj-$(CONFIG_DEBUG_OBJECTS) += debugobjects.o
 
+obj-y += psrwlock.o
+obj-$(CONFIG_PSRWLOCK_LATENCY_TEST) += psrwlock-latency-trace.o
+obj-$(CONFIG_DEBUG_PSRWLOCK) += psrwlock-debug.o
+
 ifneq ($(CONFIG_HAVE_DEC_LOCK),y)
   lib-y += dec_and_lock.o
 endif
diff --git a/stblinux-2.6.31/lib/psrwlock-debug.c b/stblinux-2.6.31/lib/psrwlock-debug.c
new file mode 100644
index 0000000..064e62d
--- /dev/null
+++ b/stblinux-2.6.31/lib/psrwlock-debug.c
@@ -0,0 +1,131 @@
+/*
+ * Priority Sifting Reader-Writer Lock Debug
+ *
+ * Inspired from kernel/mutex-debug.c.
+ *
+ * Copyright 2008 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ */
+
+#include <linux/psrwlock.h>
+#include <linux/lockdep.h>
+#include <linux/sched.h>
+#include <linux/poison.h>
+#include <linux/module.h>
+
+#include "psrwlock-debug.h"
+
+/*
+ * Must be called with lock->wait_lock held.
+ */
+void debug_psrwlock_set_owner(struct psrwlock *lock,
+			      struct thread_info *new_owner)
+{
+	lock->owner = new_owner;
+}
+
+void debug_psrwlock_lock_common(struct psrwlock *lock,
+				struct psrwlock_waiter *waiter)
+{
+	memset(waiter, PSRWLOCK_DEBUG_INIT, sizeof(*waiter));
+	waiter->magic = waiter;
+	INIT_LIST_HEAD(&waiter->list);
+}
+
+void debug_psrwlock_wake_waiter(struct psrwlock *lock,
+				struct psrwlock_waiter *waiter)
+{
+	SMP_DEBUG_LOCKS_WARN_ON(!(atomic_read(&lock->ws) & WS_WQ_MUTEX));
+	DEBUG_LOCKS_WARN_ON(list_empty(&lock->wait_list_r) &&
+			    list_empty(&lock->wait_list_w));
+	DEBUG_LOCKS_WARN_ON(waiter->magic != waiter);
+	DEBUG_LOCKS_WARN_ON(list_empty(&waiter->list));
+}
+
+void debug_psrwlock_free_waiter(struct psrwlock_waiter *waiter)
+{
+	DEBUG_LOCKS_WARN_ON(!list_empty(&waiter->list));
+	memset(waiter, PSRWLOCK_DEBUG_FREE, sizeof(*waiter));
+}
+
+void debug_psrwlock_add_waiter(struct psrwlock *lock,
+			       struct psrwlock_waiter *waiter,
+                               struct thread_info *ti)
+{
+	SMP_DEBUG_LOCKS_WARN_ON(!(atomic_read(&lock->ws) & WS_WQ_MUTEX));
+
+	/* Mark the current thread as blocked on the lock: */
+	ti->task->psrwlock_blocked_on = waiter;
+	waiter->lock = lock;
+}
+
+void psrwlock_remove_waiter(struct psrwlock *lock,
+			    struct psrwlock_waiter *waiter,
+			    struct thread_info *ti)
+{
+	DEBUG_LOCKS_WARN_ON(list_empty(&waiter->list));
+	DEBUG_LOCKS_WARN_ON(waiter->task != ti->task);
+	DEBUG_LOCKS_WARN_ON(ti->task->psrwlock_blocked_on != waiter);
+	ti->task->psrwlock_blocked_on = NULL;
+
+	list_del_init(&waiter->list);
+	waiter->task = NULL;
+}
+
+void debug_psrwlock_unlock(struct psrwlock *lock, int rw)
+{
+	if (unlikely(!debug_locks))
+		return;
+
+	DEBUG_LOCKS_WARN_ON(lock->magic != lock);
+	if (rw)	/* read */
+		DEBUG_LOCKS_WARN_ON(lock->owner != (void *)-1UL);
+	else
+		DEBUG_LOCKS_WARN_ON(lock->owner != current_thread_info());
+	DEBUG_LOCKS_WARN_ON(!lock->wait_list_r.prev && !lock->wait_list_r.next);
+	DEBUG_LOCKS_WARN_ON(!lock->wait_list_w.prev && !lock->wait_list_w.next);
+}
+
+void debug_psrwlock_init(struct psrwlock *lock, const char *name,
+		      struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
+	lockdep_init_map(&lock->dep_map, name, key, 0);
+#endif
+	lock->owner = NULL;
+	lock->magic = lock;
+}
+
+/***
+ * psrwlock_destroy - mark a psrwlock unusable
+ * @lock: the psrwlock to be destroyed
+ *
+ * This function marks the psrwlock uninitialized, and any subsequent
+ * use of the lock is forbidden. The lock must not be locked when
+ * this function is called.
+ */
+void psrwlock_destroy(struct psrwlock *lock)
+{
+	DEBUG_LOCKS_WARN_ON(psrwlock_is_locked(lock));
+	lock->magic = NULL;
+}
+
+EXPORT_SYMBOL(psrwlock_destroy);
+
+void psread_unlock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	int nested = 1;	/* TODO support nested = 0 */
+	unsigned int uc;
+
+	psrwlock_release(&rwlock->dep_map, nested, _RET_IP_);
+	debug_psrwlock_unlock(rwlock, 1);
+	debug_psrwlock_clear_owner(rwlock);
+	uc = atomic_sub_return(UC_READER_OFFSET, &rwlock->uc);
+	if (wctx == PSRW_PRIO_P || (rctx & PSR_PTHREAD))
+		psrwlock_preempt_check(uc, rwlock);
+}
+
+EXPORT_SYMBOL(psread_unlock);
diff --git a/stblinux-2.6.31/lib/psrwlock-debug.h b/stblinux-2.6.31/lib/psrwlock-debug.h
new file mode 100644
index 0000000..e5b0173
--- /dev/null
+++ b/stblinux-2.6.31/lib/psrwlock-debug.h
@@ -0,0 +1,31 @@
+#ifndef _LIB_PSRWLOCK_H
+#define _LIB_PSRWLOCK_H
+
+/*
+ * This must be called with lock->wait_lock held.
+ */
+extern void
+debug_psrwlock_set_owner(struct psrwlock *lock,
+			 struct thread_info *new_owner);
+
+static inline void debug_psrwlock_clear_owner(struct psrwlock *lock)
+{
+	lock->owner = NULL;
+}
+
+extern void debug_psrwlock_lock_common(struct psrwlock *lock,
+				       struct psrwlock_waiter *waiter);
+extern void debug_psrwlock_wake_waiter(struct psrwlock *lock,
+				       struct psrwlock_waiter *waiter);
+extern void debug_psrwlock_free_waiter(struct psrwlock_waiter *waiter);
+extern void debug_psrwlock_add_waiter(struct psrwlock *lock,
+				      struct psrwlock_waiter *waiter,
+				      struct thread_info *ti);
+extern void psrwlock_remove_waiter(struct psrwlock *lock,
+				   struct psrwlock_waiter *waiter,
+				   struct thread_info *ti);
+extern void debug_psrwlock_unlock(struct psrwlock *lock, int rw);
+extern void debug_psrwlock_init(struct psrwlock *lock, const char *name,
+				struct lock_class_key *key);
+
+#endif /* _LIB_PSRWLOCK_H */
diff --git a/stblinux-2.6.31/lib/psrwlock-latency-trace.c b/stblinux-2.6.31/lib/psrwlock-latency-trace.c
new file mode 100644
index 0000000..43ba45e
--- /dev/null
+++ b/stblinux-2.6.31/lib/psrwlock-latency-trace.c
@@ -0,0 +1,288 @@
+/*
+ * Priority Sifting Reader-Writer Lock Latency Tracer
+ *
+ * Copyright 2008 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ */
+
+#include <linux/psrwlock.h>
+#include <linux/module.h>
+#include <linux/stop_machine.h>
+#include <linux/percpu.h>
+#include <linux/init.h>
+#include <linux/kallsyms.h>
+
+/*
+ * Use unsigned long, enough to represent cycle count diff, event on 32-bit
+ * arch.
+ */
+
+struct psrwlock_latency {
+	unsigned long last_disable_cycles, max_latency, min_latency, nr_enable;
+	cycles_t total_latency;
+	unsigned long max_latency_ip_disable,
+		max_latency_ip_enable,
+		last_ip_disable;
+};
+
+static DEFINE_PER_CPU(struct psrwlock_latency, irq_latency_info);
+static DEFINE_PER_CPU(struct psrwlock_latency, softirq_latency_info);
+static DEFINE_PER_CPU(struct psrwlock_latency, preempt_latency_info);
+
+static DEFINE_MUTEX(calibration_mutex);
+static unsigned long cycles_calibration_min,
+		cycles_calibration_avg,
+		cycles_calibration_max;
+
+/*
+ * Since we are taking the timestamps within the critical section,
+ * add the number of cycles it takes to take two consecutive
+ * cycles count reads to the total.
+ * Returns an unsigned long long for %llu print format.
+ */
+static unsigned long long calibrate_cycles(cycles_t cycles)
+{
+	return cycles + cycles_calibration_avg;
+}
+
+static void calibrate_get_cycles(void)
+{
+	int i;
+	cycles_t time1, time2;
+	unsigned long delay;
+
+	printk(KERN_INFO "** get_cycles calibration **\n");
+	cycles_calibration_min = ULLONG_MAX;
+	cycles_calibration_avg = 0;
+	cycles_calibration_max = 0;
+
+	local_irq_disable();
+	for (i = 0; i < 10; i++) {
+		get_cycles_barrier();
+		time1 = get_cycles();
+		get_cycles_barrier();
+		get_cycles_barrier();
+		time2 = get_cycles();
+		get_cycles_barrier();
+		delay = time2 - time1;
+		cycles_calibration_min = min(cycles_calibration_min, delay);
+		cycles_calibration_avg += delay;
+		cycles_calibration_max = max(cycles_calibration_max, delay);
+	}
+	cycles_calibration_avg /= 10;
+	local_irq_enable();
+
+	printk(KERN_INFO "get_cycles takes [min,avg,max] %lu,%lu,%lu "
+		"cycles, results calibrated on avg\n",
+		cycles_calibration_min,
+		cycles_calibration_avg,
+		cycles_calibration_max);
+	printk("\n");
+}
+
+static void reset_latency(struct psrwlock_latency *irql)
+{
+	irql->last_disable_cycles = 0;
+	irql->max_latency = 0;
+	irql->min_latency = ULONG_MAX;
+	irql->total_latency = 0;
+	irql->nr_enable = 0;
+	irql->max_latency_ip_disable = 0;
+	irql->max_latency_ip_enable = 0;
+	irql->last_ip_disable = 0;
+}
+
+/* can't be in irq disabled section in stop_machine */
+static int _psrwlock_profile_latency_reset(void *data)
+{
+	int cpu = smp_processor_id();
+
+	reset_latency(&per_cpu(irq_latency_info, cpu));
+	reset_latency(&per_cpu(softirq_latency_info, cpu));
+	reset_latency(&per_cpu(preempt_latency_info, cpu));
+	return 0;
+}
+
+
+void psrwlock_profile_latency_reset(void)
+{
+	mutex_lock(&calibration_mutex);
+	printk(KERN_INFO "Writer-biased rwlock latency profiling reset\n");
+	calibrate_get_cycles();
+	stop_machine(_psrwlock_profile_latency_reset,
+			NULL, &cpu_possible_map);
+	mutex_unlock(&calibration_mutex);
+}
+EXPORT_SYMBOL_GPL(psrwlock_profile_latency_reset);
+
+enum irq_latency_type {
+	IRQ_LATENCY,
+	SOFTIRQ_LATENCY,
+	PREEMPT_LATENCY,
+};
+
+/*
+ * total_irq_latency and nr_irq_enable reads are racy, but it's just an
+ * average. Off-by-one is not a big deal.
+ */
+static void print_latency(const char *typename, enum irq_latency_type type)
+{
+	struct psrwlock_latency *irql;
+	cycles_t avg;
+	unsigned long nr_enable;
+	int i;
+
+	for_each_online_cpu(i) {
+		if (type == IRQ_LATENCY)
+			irql = &per_cpu(irq_latency_info, i);
+		else if (type == SOFTIRQ_LATENCY)
+			irql = &per_cpu(softirq_latency_info, i);
+		else
+			irql = &per_cpu(preempt_latency_info, i);
+		nr_enable = irql->nr_enable;
+		if (!nr_enable)
+			continue;
+		avg = irql->total_latency / (cycles_t)nr_enable;
+		printk(KERN_INFO "%s latency for cpu %d "
+			"disabled %lu times, "
+			"[min,avg,max] %llu,%llu,%llu cycles\n",
+			typename, i, nr_enable,
+			calibrate_cycles(irql->min_latency),
+			calibrate_cycles(avg),
+			calibrate_cycles(irql->max_latency));
+		printk(KERN_INFO "Max %s latency caused by :\n", typename);
+		printk(KERN_INFO "disable : ");
+		print_ip_sym(irql->max_latency_ip_disable);
+		printk(KERN_INFO "enable : ");
+		print_ip_sym(irql->max_latency_ip_enable);
+	}
+}
+
+void psrwlock_profile_latency_print(void)
+{
+	mutex_lock(&calibration_mutex);
+	printk(KERN_INFO "Writer-biased rwlock latency profiling results\n");
+	printk(KERN_INFO "\n");
+	print_latency("IRQ", IRQ_LATENCY);
+	print_latency("SoftIRQ", SOFTIRQ_LATENCY);
+	print_latency("Preemption", PREEMPT_LATENCY);
+	printk(KERN_INFO "\n");
+	mutex_unlock(&calibration_mutex);
+}
+EXPORT_SYMBOL_GPL(psrwlock_profile_latency_print);
+
+void psrwlock_profile_irq_disable(void)
+{
+	struct psrwlock_latency *irql =
+		&per_cpu(irq_latency_info, smp_processor_id());
+
+	WARN_ON_ONCE(!irqs_disabled());
+	irql->last_ip_disable = _RET_IP_;
+	get_cycles_barrier();
+	irql->last_disable_cycles = get_cycles();
+	get_cycles_barrier();
+}
+EXPORT_SYMBOL_GPL(psrwlock_profile_irq_disable);
+
+void psrwlock_profile_irq_enable(void)
+{
+	struct psrwlock_latency *irql;
+	unsigned long cur_cycles, diff_cycles;
+
+	get_cycles_barrier();
+	cur_cycles = get_cycles();
+	get_cycles_barrier();
+	irql = &per_cpu(irq_latency_info, smp_processor_id());
+	WARN_ON_ONCE(!irqs_disabled());
+	if (!irql->last_disable_cycles)
+		return;
+	diff_cycles = cur_cycles - irql->last_disable_cycles;
+	if (diff_cycles > irql->max_latency) {
+		irql->max_latency = diff_cycles;
+		irql->max_latency_ip_enable = _RET_IP_;
+		irql->max_latency_ip_disable = irql->last_ip_disable;
+	}
+	irql->min_latency = min(irql->min_latency, diff_cycles);
+	irql->total_latency += diff_cycles;
+	irql->nr_enable++;
+}
+EXPORT_SYMBOL_GPL(psrwlock_profile_irq_enable);
+
+void psrwlock_profile_bh_disable(void)
+{
+	struct psrwlock_latency *irql =
+		&per_cpu(softirq_latency_info, smp_processor_id());
+
+	WARN_ON_ONCE(!in_softirq());
+	irql->last_ip_disable = _RET_IP_;
+	get_cycles_barrier();
+	irql->last_disable_cycles = get_cycles();
+	get_cycles_barrier();
+}
+EXPORT_SYMBOL_GPL(psrwlock_profile_bh_disable);
+
+void psrwlock_profile_bh_enable(void)
+{
+	struct psrwlock_latency *irql;
+	unsigned long cur_cycles, diff_cycles;
+
+	get_cycles_barrier();
+	cur_cycles = get_cycles();
+	get_cycles_barrier();
+	irql = &per_cpu(softirq_latency_info, smp_processor_id());
+	WARN_ON_ONCE(!in_softirq());
+	diff_cycles = cur_cycles - irql->last_disable_cycles;
+	if (diff_cycles > irql->max_latency) {
+		irql->max_latency = diff_cycles;
+		irql->max_latency_ip_enable = _RET_IP_;
+		irql->max_latency_ip_disable = irql->last_ip_disable;
+	}
+	irql->min_latency = min(irql->min_latency, diff_cycles);
+	irql->total_latency += diff_cycles;
+	irql->nr_enable++;
+}
+EXPORT_SYMBOL_GPL(psrwlock_profile_bh_enable);
+
+#ifdef CONFIG_PREEMPT
+void psrwlock_profile_preempt_disable(void)
+{
+	struct psrwlock_latency *irql =
+		&per_cpu(preempt_latency_info, smp_processor_id());
+
+	WARN_ON_ONCE(preemptible());
+	irql->last_ip_disable = _RET_IP_;
+	get_cycles_barrier();
+	irql->last_disable_cycles = get_cycles();
+	get_cycles_barrier();
+}
+EXPORT_SYMBOL_GPL(psrwlock_profile_preempt_disable);
+
+void psrwlock_profile_preempt_enable(void)
+{
+	struct psrwlock_latency *irql;
+	unsigned long cur_cycles, diff_cycles;
+
+	get_cycles_barrier();
+	cur_cycles = get_cycles();
+	get_cycles_barrier();
+	irql = &per_cpu(preempt_latency_info, smp_processor_id());
+	WARN_ON_ONCE(preemptible());
+	diff_cycles = cur_cycles - irql->last_disable_cycles;
+	if (diff_cycles > irql->max_latency) {
+		irql->max_latency = diff_cycles;
+		irql->max_latency_ip_enable = _RET_IP_;
+		irql->max_latency_ip_disable = irql->last_ip_disable;
+	}
+	irql->min_latency = min(irql->min_latency, diff_cycles);
+	irql->total_latency += diff_cycles;
+	irql->nr_enable++;
+}
+EXPORT_SYMBOL_GPL(psrwlock_profile_preempt_enable);
+#endif
+
+__init int psrwlock_init(void)
+{
+	printk(KERN_INFO "psrwlock latency profiling init\n");
+	calibrate_get_cycles();
+	return 0;
+}
+device_initcall(psrwlock_init);
diff --git a/stblinux-2.6.31/lib/psrwlock.c b/stblinux-2.6.31/lib/psrwlock.c
new file mode 100644
index 0000000..acf4bf6
--- /dev/null
+++ b/stblinux-2.6.31/lib/psrwlock.c
@@ -0,0 +1,989 @@
+/*
+ * Priority Sifting Reader-Writer Lock
+ *
+ * Priority Sifting Reader-Writer Lock (psrwlock) excludes reader execution
+ * contexts one at a time, thus increasing the writer priority in stages. It
+ * favors writers against reader threads, but lets higher priority readers in
+ * even when there are subscribed writers waiting for the lock at a given lower
+ * priority. Very frequent writers could starve reader threads.
+ *
+ * Copyright 2008 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ */
+
+#include <linux/psrwlock.h>
+#include <linux/list.h>
+#include <linux/linkage.h>
+#include <linux/freezer.h>
+#include <linux/module.h>
+#include <linux/debug_locks.h>
+
+#include <asm/processor.h>
+
+#ifdef CONFIG_DEBUG_PSRWLOCK
+# include "psrwlock-debug.h"
+#else
+# include "psrwlock.h"
+#endif
+
+#ifdef WBIAS_RWLOCK_DEBUG
+#define printk_dbg printk
+#else
+#define printk_dbg(fmt, args...)
+#endif
+
+enum preempt_type {
+	PSRW_PREEMPT,		/* preemptable */
+	PSRW_NON_PREEMPT,	/* non-preemptable */
+};
+
+enum lock_type {
+	PSRW_READ,
+	PSRW_WRITE,
+};
+
+enum v_type {
+	V_INT,
+	V_LONG,
+};
+
+static int rwlock_wait(void *vptr, psrwlock_t *rwlock,
+		unsigned long mask, unsigned long test_mask,
+		unsigned long full_mask, int check_full_mask,
+		enum v_type vtype, enum lock_type ltype, long state,
+		unsigned long ip);
+
+/***
+ * psrwlock_init - initialize the psrwlock
+ * @lock: the psrwlock to be initialized
+ * @key: the lock_class_key for the class; used by mutex lock debugging
+ *
+ * Initialize the psrwlock to unlocked state.
+ *
+ * It is not allowed to initialize an already locked psrwlock.
+ */
+void
+__psrwlock_init(struct psrwlock *lock, const char *name,
+		struct lock_class_key *key, u32 rctx, enum psrw_prio wctx)
+{
+	unsigned int i;
+
+	atomic_set(&lock->uc, 0);
+	atomic_set(&lock->ws, 0);
+	for (i = 0; i < PSRW_NR_PRIO; i++)
+		atomic_set(&lock->prio[i], 0);
+	lock->rctx_bitmap = rctx;
+	lock->wctx = wctx;
+	INIT_LIST_HEAD(&lock->wait_list_r);
+	INIT_LIST_HEAD(&lock->wait_list_w);
+
+	debug_psrwlock_init(lock, name, key);
+}
+
+EXPORT_SYMBOL(__psrwlock_init);
+
+/*
+ * Lock out a specific uncontended execution context from the read lock. Wait
+ * for the rmask (readers in previous context count) and for the writer count in
+ * the new context not to be full before proceeding to subscribe to the new
+ * write context.
+ *
+ * return values :
+ * 1 : lock taken
+ * 0 : trylock failed
+ * < 0 : interrupted
+ */
+static int _pswrite_lock_ctx_wait_sub(void *v_inout,
+		void *vptr, psrwlock_t *rwlock,
+		unsigned long wait_mask, unsigned long test_mask,
+		unsigned long full_mask, long offset,
+		enum v_type vtype, enum lock_type ltype,
+		enum preempt_type ptype, int trylock, long state,
+		unsigned long ip)
+{
+	long try = NR_PREEMPT_BUSY_LOOPS;
+	unsigned long newv;
+	unsigned long v;
+	int ret;
+
+	if (vtype == V_LONG)
+		v = *(unsigned long *)v_inout;
+	else
+		v = *(unsigned int *)v_inout;
+
+	printk_dbg("wait sub start v %lX, new %lX, wait_mask %lX, "
+		"test_mask %lX, full_mask %lX, offset %lX\n",
+		v, v + offset, wait_mask, test_mask, full_mask, offset);
+
+	for (;;) {
+		if (v & wait_mask || (v & test_mask) >= full_mask) {
+			lock_contended(&rwlock->dep_map, ip);
+			if (trylock)
+				return 0;
+			if (ptype == PSRW_PREEMPT && unlikely(!(--try))) {
+				ret = rwlock_wait(vptr, rwlock, wait_mask,
+					test_mask, full_mask, 1,
+					vtype, ltype, state, ip);
+				if (ret < 0)
+					return ret;
+				try = NR_PREEMPT_BUSY_LOOPS;
+			} else
+				cpu_relax();	/* Order v reads */
+			if (vtype == V_LONG)
+				v = atomic_long_read((atomic_long_t *)vptr);
+			else
+				v = atomic_read((atomic_t *)vptr);
+			continue;
+		}
+		if (vtype == V_LONG)
+			newv = atomic_long_cmpxchg((atomic_long_t *)vptr,
+				v, v + offset);
+		else
+			newv = atomic_cmpxchg((atomic_t *)vptr,
+				(int)v, (int)v + (int)offset);
+		if (likely(newv == v))
+			break;
+		else {
+			if (trylock)
+				return 0;
+			v = newv;
+		}
+	}
+	printk_dbg("wait sub end v %lX, new %lX, wait_mask %lX, "
+		"test_mask %lX, full_mask %lX, offset %lX\n",
+		v, v + offset, wait_mask, test_mask, full_mask, offset);
+	/* cmpxchg orders memory reads and writes */
+	v += offset;
+	if (vtype == V_LONG)
+		*(unsigned long *)v_inout = v;
+	else
+		*(unsigned int *)v_inout = v;
+	return 1;
+}
+
+/*
+ * return values :
+ * 1 : lock taken
+ * 0 : trylock failed
+ * < 0 : interrupted
+*/
+static int _pswrite_lock_ctx_wait(unsigned long v_in, void *vptr,
+		psrwlock_t *rwlock, unsigned long wait_mask,
+		enum v_type vtype, enum lock_type ltype,
+		enum preempt_type ptype, int trylock, long state,
+		unsigned long ip)
+{
+	int try = NR_PREEMPT_BUSY_LOOPS;
+	unsigned long v = v_in;
+	int ret;
+
+	printk_dbg("wait start v %lX, wait_mask %lX\n", v, wait_mask);
+	/* order all read and write memory operations. */
+	smp_mb();
+	while (v & wait_mask) {
+		if (ptype == PSRW_PREEMPT && unlikely(!(--try))) {
+			lock_contended(&rwlock->dep_map, ip);
+			if (trylock)
+				return 0;
+			ret = rwlock_wait(vptr, rwlock, wait_mask, 0, 0, 0,
+				vtype, ltype, state, ip);
+			if (ret < 0)
+				return ret;
+			try = NR_PREEMPT_BUSY_LOOPS;
+		} else
+			cpu_relax();	/* Order v reads */
+		if (vtype == V_LONG)
+			v = atomic_long_read((atomic_long_t *)vptr);
+		else
+			v = atomic_read((atomic_t *)vptr);
+	}
+	/* order all read and write memory operations. */
+	smp_mb();
+	printk_dbg("wait end v %lX, wait_mask %lX\n", v, wait_mask);
+	return 1;
+}
+
+/*
+ * Go into a wait queue.
+ *
+ * mask, v & full_mask == full_mask are the conditions for which we wait.
+ * return values :
+ * 1 : woken up
+ * < 0 : interrupted
+ */
+static int rwlock_wait(void *vptr, psrwlock_t *rwlock,
+		unsigned long mask, unsigned long test_mask,
+		unsigned long full_mask, int check_full_mask,
+		enum v_type vtype, enum lock_type ltype, long state,
+		unsigned long ip)
+{
+	struct task_struct *task = current;
+	struct psrwlock_waiter waiter;
+	unsigned long v;
+	int wq_active, ws, ret = 1;
+
+	/*
+	 * Busy-loop waiting for the waitqueue mutex.
+	 */
+	psrwlock_irq_disable();
+	ws = atomic_read(&rwlock->ws);
+	_pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
+		0, WS_WQ_MUTEX, WS_WQ_MUTEX, WS_WQ_MUTEX,
+		V_INT, ltype, PSRW_NON_PREEMPT, 0, TASK_UNINTERRUPTIBLE, ip);
+
+	debug_psrwlock_lock_common(rwlock, &waiter);
+
+	/*
+	 * Got the waitqueue mutex, get into the wait queue.
+	 */
+	wq_active = !list_empty(&rwlock->wait_list_r)
+			|| !list_empty(&rwlock->wait_list_w);
+	if (!wq_active)
+		atomic_add(UC_WQ_ACTIVE, &rwlock->uc);
+	/* Set the UC_WQ_ACTIVE flag before testing the condition. */
+	smp_mb();
+	/*
+	 * Before we go to sleep, check that the lock we were expecting
+	 * did not free between the moment we last checked for the lock and the
+	 * moment we raised the UC_WQ_ACTIVE flag.
+	 */
+	if (vtype == V_LONG)
+		v = atomic_long_read((atomic_long_t *)vptr);
+	else
+		v = atomic_read((atomic_t *)vptr);
+	if (unlikely(!(v & mask || (check_full_mask
+			&& (v & test_mask) >= full_mask))))
+		goto skip_sleep;
+	/*
+	 * got a signal ? (not done in TASK_UNINTERRUPTIBLE)
+	 */
+	if (unlikely(signal_pending_state(state, task))) {
+		ret = -EINTR;
+		goto skip_sleep;
+	}
+
+	debug_psrwlock_add_waiter(rwlock, &waiter, task_thread_info(task));
+
+	/*
+	 * Add waiting tasks to the end of the waitqueue (FIFO):
+	 * Only one thread will be woken up at a time.
+	 */
+	if (ltype == PSRW_WRITE)
+		list_add_tail(&waiter.list, &rwlock->wait_list_w);
+	else
+		list_add_tail(&waiter.list, &rwlock->wait_list_r);
+	waiter.task = task;
+	__set_task_state(task, state);
+	smp_mb();	/* Insure memory ordering when clearing the mutex. */
+
+
+	atomic_sub(WS_WQ_MUTEX, &rwlock->ws);
+	psrwlock_irq_enable();
+
+	try_to_freeze();
+	schedule();
+
+	/*
+	 * Woken up; Busy-loop waiting for the waitqueue mutex.
+	 */
+	psrwlock_irq_disable();
+	ws = atomic_read(&rwlock->ws);
+	_pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
+		0, WS_WQ_MUTEX, WS_WQ_MUTEX, WS_WQ_MUTEX,
+		V_INT, ltype, PSRW_NON_PREEMPT, 0, TASK_UNINTERRUPTIBLE, ip);
+	__set_task_state(task, TASK_RUNNING);
+	psrwlock_remove_waiter(rwlock, &waiter, task_thread_info(task));
+skip_sleep:
+	wq_active = !list_empty(&rwlock->wait_list_r)
+			|| !list_empty(&rwlock->wait_list_w);
+	if (!wq_active)
+		atomic_sub(UC_WQ_ACTIVE, &rwlock->uc);
+	smp_mb();	/* Insure memory ordering when clearing the mutex. */
+	atomic_sub(WS_WQ_MUTEX, &rwlock->ws);
+	psrwlock_irq_enable();
+	debug_psrwlock_free_waiter(&waiter);
+	return ret;
+}
+
+/*
+ * Reader lock
+ */
+
+#ifdef CONFIG_DEBUG_PSRWLOCK
+static int _psread_lock_fast_check(unsigned int uc, psrwlock_t *rwlock,
+	unsigned int uc_rmask)
+{
+	return 0;
+}
+#else
+/*
+ * _psread_lock_fast_check
+ *
+ * Second cmpxchg taken in case of many active readers.
+ * Will busy-loop if cmpxchg fails even in trylock mode.
+ *
+ * First try to get the uncontended lock. If it is non-zero (can be common,
+ * since we allow multiple readers), pass the returned cmpxchg v to the loop
+ * to try to get the reader lock.
+ *
+ * trylock will fail if a writer is subscribed or holds the lock, but will
+ * spin if there is concurency to win the cmpxchg. It could happen if, for
+ * instance, other concurrent reads need to update the roffset or if a
+ * writer updated the lock bits which does not contend us. Since many
+ * concurrent readers is a common case, it makes sense not to fail is it
+ * happens.
+ *
+ * the non-trylock case will spin for both situations.
+ *
+ * Busy-loop if the reader count is full.
+ */
+static int _psread_lock_fast_check(unsigned int uc, psrwlock_t *rwlock,
+	unsigned int uc_rmask)
+{
+	unsigned int newuc;
+
+	/*
+	 * This is the second cmpxchg taken in case of many active readers.
+	 */
+	while (likely(!(uc & (UC_SLOW_WRITER | UC_WRITER))
+			&& (uc & UC_READER_MASK) < uc_rmask)) {
+		newuc = atomic_cmpxchg(&rwlock->uc, uc, uc + UC_READER_OFFSET);
+		if (likely(newuc == uc))
+			return 1;
+		else
+			uc = newuc;
+	}
+	return 0;
+}
+#endif
+
+int __psread_lock_slow(psrwlock_t *rwlock,
+		unsigned int uc_rmask, atomic_long_t *vptr,
+		int trylock, enum preempt_type ptype, long state,
+		unsigned long ip)
+{
+	u32 rctx = rwlock->rctx_bitmap;
+	unsigned long v;
+	unsigned int uc;
+	int ret;
+	int subclass = SINGLE_DEPTH_NESTING;	/* TODO : parameter */
+
+	if (unlikely(in_irq() || irqs_disabled()))
+		WARN_ON_ONCE(!(rctx & PSR_IRQ) || ptype != PSRW_NON_PREEMPT);
+	else if (in_softirq())
+		WARN_ON_ONCE(!(rctx & PSR_BH) || ptype != PSRW_NON_PREEMPT);
+#ifdef CONFIG_PREEMPT
+	else if (in_atomic())
+		WARN_ON_ONCE(!(rctx & PSR_NPTHREAD)
+			|| ptype != PSRW_NON_PREEMPT);
+	else
+		WARN_ON_ONCE(!(rctx & PSR_PTHREAD) || ptype != PSRW_PREEMPT);
+#else
+	else
+		WARN_ON_ONCE((!(rctx & PSR_NPTHREAD)
+				|| ptype != PSRW_NON_PREEMPT)
+				&& (!(rctx & PSR_PTHREAD)
+				|| ptype != PSRW_PREEMPT));
+#endif
+
+	psrwlock_acquire_read(&rwlock->dep_map, subclass, trylock, ip);
+
+	/*
+	 * A cmpxchg read uc, which implies strict ordering.
+	 */
+	v = atomic_long_read(vptr);
+	ret = _pswrite_lock_ctx_wait_sub(&v, vptr, rwlock,
+		CTX_WMASK, CTX_RMASK, CTX_RMASK, CTX_ROFFSET,
+		V_LONG, PSRW_READ, ptype, trylock, state, ip);
+	if (unlikely(ret < 1))
+		goto fail;
+
+	/*
+	 * We are in! Well, we just have to busy-loop waiting for any
+	 * uncontended writer to release its lock.
+	 *
+	 * In this exact order :
+	 * - increment the uncontended readers count.
+	 * - decrement the current context reader count we just previously got.
+	 *
+	 * This makes sure we always count in either the slow path per context
+	 * count or the uncontended reader count starting from the moment we got
+	 * the slow path count to the moment we will release the uncontended
+	 * reader count at the unlock.
+	 *
+	 * This implies a strict read/write ordering of these two variables.
+	 * Reading first "uc" and then "v" is strictly required. The current
+	 * reader count can be summed twice in the worse case, but we are only
+	 * interested to know if there is _any_ reader left.
+	 */
+	uc = atomic_read(&rwlock->uc);
+	ret = _pswrite_lock_ctx_wait_sub(&uc, &rwlock->uc, rwlock,
+		UC_WRITER, UC_READER_MASK, uc_rmask, UC_READER_OFFSET,
+		V_INT, PSRW_READ, ptype, trylock, state, ip);
+	/*
+	 * _pswrite_lock_ctx_wait_sub has a memory barrier
+	 */
+	atomic_long_sub(CTX_ROFFSET, vptr);
+	/*
+	 * don't care about v ordering wrt memory operations inside the
+	 * read lock. It's uc which holds our read count.
+	 */
+	if (unlikely(ret < 1))
+		goto fail_preempt;
+
+	lock_acquired(&rwlock->dep_map, ip);
+	debug_psrwlock_set_owner(rwlock, (void *)-1UL);	/* -1 : all readers */
+
+	/* Success */
+	return 1;
+
+	/* Failure */
+fail_preempt:
+	/* write v before reading uc */
+	smp_mb();
+	uc = atomic_read(&rwlock->uc);
+	psrwlock_preempt_check(uc, rwlock);
+fail:
+	cpu_relax();
+	psrwlock_release(&rwlock->dep_map, 1, ip);
+	return ret;
+
+}
+
+/*
+ * _psread_lock_slow : read lock slow path.
+ *
+ * Non-preemptable :
+ * Busy-wait for the specific context lock.
+ * Preemptable :
+ * Busy-wait for the specific context lock NR_PREEMPT_BUSY_LOOPS loops, and then
+ * go to the wait queue.
+ *
+ * _psread_trylock_slow : read trylock slow path.
+ *
+ * Try to get the read lock. Returns 1 if succeeds, else returns 0.
+ */
+
+asmregparm
+void _psread_lock_slow_irq(unsigned int uc, psrwlock_t *rwlock)
+{
+	int ret;
+
+	ret = _psread_lock_fast_check(uc, rwlock, UC_HARDIRQ_READER_MASK);
+	if (ret)
+		return;
+	__psread_lock_slow(rwlock, UC_HARDIRQ_READER_MASK,
+			&rwlock->prio[PSRW_PRIO_IRQ],
+			0, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
+}
+EXPORT_SYMBOL(_psread_lock_slow_irq);
+
+asmregparm
+void _psread_lock_slow_bh(unsigned int uc, psrwlock_t *rwlock)
+{
+	int ret;
+
+	ret = _psread_lock_fast_check(uc, rwlock, UC_SOFTIRQ_READER_MASK);
+	if (ret)
+		return;
+	__psread_lock_slow(rwlock, UC_SOFTIRQ_READER_MASK,
+			&rwlock->prio[PSRW_PRIO_BH],
+			0, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
+}
+EXPORT_SYMBOL(_psread_lock_slow_bh);
+
+asmregparm
+void _psread_lock_slow_inatomic(unsigned int uc, psrwlock_t *rwlock)
+{
+	int ret;
+
+	ret = _psread_lock_fast_check(uc, rwlock, UC_NPTHREAD_READER_MASK);
+	if (ret)
+		return;
+	__psread_lock_slow(rwlock, UC_NPTHREAD_READER_MASK,
+			&rwlock->prio[PSRW_PRIO_NP],
+			0, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
+}
+EXPORT_SYMBOL(_psread_lock_slow_inatomic);
+
+asmregparm
+void _psread_lock_slow(unsigned int uc, psrwlock_t *rwlock)
+{
+	int ret;
+
+	ret = _psread_lock_fast_check(uc, rwlock, UC_PTHREAD_READER_MASK);
+	if (ret)
+		return;
+	__psread_lock_slow(rwlock, UC_PTHREAD_READER_MASK,
+			&rwlock->prio[PSRW_PRIO_P],
+			0, PSRW_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
+}
+EXPORT_SYMBOL(_psread_lock_slow);
+
+asmregparm
+int _psread_lock_interruptible_slow(unsigned int uc, psrwlock_t *rwlock)
+{
+	int ret;
+
+	ret = _psread_lock_fast_check(uc, rwlock, UC_PTHREAD_READER_MASK);
+	if (ret)
+		return 0;
+	ret = __psread_lock_slow(rwlock, UC_PTHREAD_READER_MASK,
+			&rwlock->prio[PSRW_PRIO_P],
+			0, PSRW_PREEMPT, TASK_INTERRUPTIBLE, _RET_IP_);
+	if (ret < 1)
+		return ret;
+	return 0;
+}
+EXPORT_SYMBOL(_psread_lock_interruptible_slow);
+
+asmregparm
+int _psread_trylock_slow_irq(unsigned int uc, psrwlock_t *rwlock)
+{
+	int ret;
+
+	ret = _psread_lock_fast_check(uc, rwlock, UC_HARDIRQ_READER_MASK);
+	if (ret)
+		return 1;
+	return __psread_lock_slow(rwlock, UC_HARDIRQ_READER_MASK,
+			&rwlock->prio[PSRW_PRIO_IRQ],
+			1, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
+}
+EXPORT_SYMBOL(_psread_trylock_slow_irq);
+
+asmregparm
+int _psread_trylock_slow_bh(unsigned int uc, psrwlock_t *rwlock)
+{
+	int ret;
+
+	ret = _psread_lock_fast_check(uc, rwlock, UC_SOFTIRQ_READER_MASK);
+	if (ret)
+		return 1;
+	return __psread_lock_slow(rwlock, UC_SOFTIRQ_READER_MASK,
+			&rwlock->prio[PSRW_PRIO_BH],
+			1, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
+}
+EXPORT_SYMBOL(_psread_trylock_slow_bh);
+
+asmregparm
+int _psread_trylock_slow_inatomic(unsigned int uc, psrwlock_t *rwlock)
+{
+	int ret;
+
+	ret = _psread_lock_fast_check(uc, rwlock, UC_NPTHREAD_READER_MASK);
+	if (ret)
+		return 1;
+	return __psread_lock_slow(rwlock, UC_NPTHREAD_READER_MASK,
+			&rwlock->prio[PSRW_PRIO_NP],
+			1, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
+}
+EXPORT_SYMBOL(_psread_trylock_slow_inatomic);
+
+asmregparm
+int _psread_trylock_slow(unsigned int uc, psrwlock_t *rwlock)
+{
+	int ret;
+
+	ret = _psread_lock_fast_check(uc, rwlock, UC_PTHREAD_READER_MASK);
+	if (ret)
+		return 1;
+	return __psread_lock_slow(rwlock, UC_PTHREAD_READER_MASK,
+			&rwlock->prio[PSRW_PRIO_P],
+			1, PSRW_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
+}
+EXPORT_SYMBOL(_psread_trylock_slow);
+
+
+/* Writer lock */
+
+static int _pswrite_lock_out_context(unsigned int *uc_inout,
+	atomic_long_t *vptr, psrwlock_t *rwlock,
+	enum preempt_type ptype, int trylock, long state, unsigned long ip)
+{
+	int ret;
+	unsigned long v;
+
+	/* lock out read slow paths */
+	v = atomic_long_read(vptr);
+	ret = _pswrite_lock_ctx_wait_sub(&v, vptr, rwlock,
+		0, CTX_WMASK, CTX_WMASK, CTX_WOFFSET,
+		V_LONG, PSRW_WRITE, ptype, trylock, state, ip);
+	if (unlikely(ret < 1))
+		return ret;
+	/*
+	 * continue when no reader threads left, but keep subscription, will be
+	 * removed by next subscription.
+	 */
+	ret = _pswrite_lock_ctx_wait(v, vptr, rwlock,
+		CTX_RMASK, V_LONG, PSRW_WRITE, ptype, trylock, state, ip);
+	if (unlikely(ret < 1))
+		goto fail_clean_slow;
+	/* Wait for uncontended readers and writers to unlock */
+	*uc_inout = atomic_read(&rwlock->uc);
+	ret = _pswrite_lock_ctx_wait(*uc_inout, &rwlock->uc, rwlock,
+		UC_WRITER | UC_READER_MASK,
+		V_INT, PSRW_WRITE, ptype, trylock, state, ip);
+	if (ret < 1)
+		goto fail_clean_slow;
+	return 1;
+
+fail_clean_slow:
+	atomic_long_sub(CTX_WOFFSET, vptr);
+	return ret;
+}
+
+static void writer_count_inc(unsigned int *uc, psrwlock_t *rwlock,
+		enum preempt_type ptype, unsigned long ip)
+{
+	unsigned int ws;
+
+	ws = atomic_read(&rwlock->ws);
+	/*
+	 * Take the mutex and increment the writer count at once.
+	 * Never fail.
+	 */
+	_pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
+		WS_COUNT_MUTEX, WS_MASK, WS_MASK,
+		WS_COUNT_MUTEX + WS_OFFSET,
+		V_INT, PSRW_WRITE, ptype, 0, TASK_UNINTERRUPTIBLE, ip);
+	/* First writer in slow path ? */
+	if ((ws & WS_MASK) == WS_OFFSET) {
+		atomic_add(UC_SLOW_WRITER, &rwlock->uc);
+		*uc += UC_SLOW_WRITER;
+	}
+	smp_mb();	/* serialize memory operations with mutex */
+	atomic_sub(WS_COUNT_MUTEX, &rwlock->ws);
+}
+
+static void writer_count_dec(unsigned int *uc, psrwlock_t *rwlock,
+		enum preempt_type ptype, unsigned long ip)
+{
+	unsigned int ws;
+
+	ws = atomic_read(&rwlock->ws);
+	/*
+	 * Take the mutex and decrement the writer count at once.
+	 * Never fail.
+	 */
+	_pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
+		WS_COUNT_MUTEX, WS_COUNT_MUTEX, WS_COUNT_MUTEX,
+		WS_COUNT_MUTEX - WS_OFFSET,
+		V_INT, PSRW_WRITE, ptype, 0, TASK_UNINTERRUPTIBLE, ip);
+	/* Last writer in slow path ? */
+	if (!(ws & WS_MASK)) {
+		atomic_sub(UC_SLOW_WRITER, &rwlock->uc);
+		*uc -= UC_SLOW_WRITER;
+	}
+	smp_mb();	/* serialize memory operations with mutex */
+	atomic_sub(WS_COUNT_MUTEX, &rwlock->ws);
+}
+
+static int __pswrite_lock_slow_common(unsigned int uc, psrwlock_t *rwlock,
+		int trylock, long state, unsigned long ip)
+{
+	struct task_struct *task = current;
+	enum psrw_prio wctx = rwlock->wctx;
+	u32 rctx = rwlock->rctx_bitmap;
+	enum preempt_type ptype;
+	unsigned int ws;
+	int ret;
+	int subclass = SINGLE_DEPTH_NESTING;	/* TODO : parameter */
+
+	write_context_enable(wctx, rctx);
+
+	if (wctx == PSRW_PRIO_IRQ)
+		WARN_ON_ONCE(!in_irq() && !irqs_disabled());
+	else if (wctx == PSRW_PRIO_BH)
+		WARN_ON_ONCE(!in_softirq());
+#ifdef CONFIG_PREEMPT
+	else if (wctx == PSRW_PRIO_NP)
+		WARN_ON_ONCE(!in_atomic());
+#endif
+
+	/*
+	 * We got here because the MAY_CONTEND bit is set in the uc bitmask. We
+	 * are therefore contending with fast-path or other slow-path writers.
+	 * A cmpxchg reads uc, which implies strict ordering.
+	 */
+	if (wctx == PSRW_PRIO_P)
+		ptype = PSRW_PREEMPT;
+	else
+		ptype = PSRW_NON_PREEMPT;
+
+	psrwlock_acquire(&rwlock->dep_map, subclass, trylock, ip);
+
+	/* Increment the slow path writer count */
+	writer_count_inc(&uc, rwlock, ptype, ip);
+
+	if (rctx & PSR_PTHREAD) {
+		ptype = PSRW_PREEMPT;
+		ret = _pswrite_lock_out_context(&uc,
+			&rwlock->prio[PSRW_PRIO_P], rwlock,
+			ptype, trylock, state, ip);
+		if (unlikely(ret < 1))
+			goto fail_dec_count;
+	}
+
+	/*
+	 * lock out non-preemptable threads.
+	 */
+	if (rctx & PSR_NPTHREAD) {
+		if (wctx != PSRW_PRIO_NP)
+			psrwlock_preempt_disable();
+		ptype = PSRW_NON_PREEMPT;
+		ret = _pswrite_lock_out_context(&uc,
+			&rwlock->prio[PSRW_PRIO_NP], rwlock,
+			ptype, trylock, state, ip);
+		if (unlikely(ret < 1))
+			goto fail_unsub_pthread;
+	}
+
+	/* lock out softirqs */
+	if (rctx & PSR_BH) {
+		if (wctx != PSRW_PRIO_BH)
+			psrwlock_bh_disable();
+		ptype = PSRW_NON_PREEMPT;
+		ret = _pswrite_lock_out_context(&uc,
+			&rwlock->prio[PSRW_PRIO_BH], rwlock,
+			ptype, trylock, state, ip);
+		if (unlikely(ret < 1))
+			goto fail_unsub_npthread;
+	}
+
+	/* lock out hardirqs */
+	if (rctx & PSR_IRQ) {
+		if (wctx != PSRW_PRIO_IRQ)
+			psrwlock_irq_disable();
+		ptype = PSRW_NON_PREEMPT;
+		ret = _pswrite_lock_out_context(&uc,
+			&rwlock->prio[PSRW_PRIO_IRQ], rwlock,
+			ptype, trylock, state, ip);
+		if (unlikely(ret < 1))
+			goto fail_unsub_bh;
+	}
+
+	/*
+	 * Finally, take the mutex.
+	 */
+	if (rctx & (PSR_NPTHREAD | PSR_BH | PSR_IRQ))
+		ptype = PSRW_NON_PREEMPT;
+	else
+		ptype = PSRW_PREEMPT;
+	ws = atomic_read(&rwlock->ws);
+	ret = _pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
+		0, WS_LOCK_MUTEX, WS_LOCK_MUTEX, WS_LOCK_MUTEX,
+		V_INT, PSRW_WRITE, ptype, trylock, state, ip);
+	if (unlikely(ret < 1))
+		goto fail_unsub_irq;
+	/* atomic_cmpxchg orders writes */
+
+	lock_acquired(&rwlock->dep_map, ip);
+	debug_psrwlock_set_owner(rwlock, task_thread_info(task));
+
+	return 1;	/* success */
+
+	/* Failure paths */
+fail_unsub_irq:
+	if (rctx & PSR_IRQ)
+		atomic_long_sub(CTX_WOFFSET, &rwlock->prio[PSRW_PRIO_IRQ]);
+fail_unsub_bh:
+	if ((rctx & PSR_IRQ) && wctx != PSRW_PRIO_IRQ)
+		psrwlock_irq_enable();
+	if (rctx & PSR_BH)
+		atomic_long_sub(CTX_WOFFSET, &rwlock->prio[PSRW_PRIO_BH]);
+fail_unsub_npthread:
+	if ((rctx & PSR_BH) && wctx != PSRW_PRIO_BH)
+		psrwlock_bh_enable();
+	if (rctx & PSR_NPTHREAD)
+		atomic_long_sub(CTX_WOFFSET, &rwlock->prio[PSRW_PRIO_NP]);
+fail_unsub_pthread:
+	if ((rctx & PSR_NPTHREAD) && wctx != PSRW_PRIO_NP)
+		psrwlock_preempt_enable();
+	if (rctx & PSR_PTHREAD)
+		atomic_long_sub(CTX_WOFFSET, &rwlock->prio[PSRW_PRIO_P]);
+fail_dec_count:
+	if (wctx == PSRW_PRIO_P)
+		ptype = PSRW_PREEMPT;
+	else
+		ptype = PSRW_NON_PREEMPT;
+	writer_count_dec(&uc, rwlock, ptype, ip);
+	psrwlock_preempt_check(uc, rwlock);
+	cpu_relax();
+	psrwlock_release(&rwlock->dep_map, 1, ip);
+	return ret;
+}
+
+/*
+ * _pswrite_lock_slow : Writer-biased rwlock write lock slow path.
+ *
+ * Locks out execution contexts one by one.
+ */
+asmregparm void _pswrite_lock_slow(unsigned int uc, psrwlock_t *rwlock)
+{
+	__pswrite_lock_slow_common(uc, rwlock, 0, TASK_UNINTERRUPTIBLE,
+				   _RET_IP_);
+}
+EXPORT_SYMBOL_GPL(_pswrite_lock_slow);
+
+/*
+ * pswrite lock, interruptible.
+ */
+asmregparm int _pswrite_lock_interruptible_slow(unsigned int uc,
+		psrwlock_t *rwlock)
+{
+	int ret;
+
+	ret = __pswrite_lock_slow_common(uc, rwlock, 0, TASK_INTERRUPTIBLE,
+				   _RET_IP_);
+	if (ret < 1)
+		return ret;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(_pswrite_lock_interruptible_slow);
+
+/*
+ * _pswrite_trylock_slow : Try to take a write lock.
+ */
+asmregparm
+int _pswrite_trylock_slow(unsigned int uc, psrwlock_t *rwlock)
+{
+	return __pswrite_lock_slow_common(uc, rwlock, 1, TASK_INTERRUPTIBLE,
+				   _RET_IP_);
+}
+EXPORT_SYMBOL_GPL(_pswrite_trylock_slow);
+
+asmregparm
+void _pswrite_unlock_slow(unsigned int uc, psrwlock_t *rwlock)
+{
+	enum psrw_prio wctx = rwlock->wctx;
+	u32 rctx = rwlock->rctx_bitmap;
+	enum preempt_type ptype;
+	int nested = 1;	/* FIXME : allow nested = 0 ? */
+
+	mutex_release(&rwlock->dep_map, nested, _RET_IP_);
+	debug_psrwlock_unlock(rwlock, 0);
+	debug_psrwlock_clear_owner(rwlock);
+
+	/*
+	 * We get here either :
+	 * - From the fast-path unlock, but a slow-path writer has set the
+	 *   UC_SLOW_WRITER bit.
+	 * - still having the slowpath locks.
+	 *
+	 * We have to know if we must decrement the WS_OFFSET count.
+	 *
+	 * uc, received as parameter, was read by an atomic cmpxchg, which
+	 * implies strict memory ordering. It orders memory accesses done within
+	 * the critical section with the lock.
+	 */
+	if (uc & UC_WRITER) {
+		uc = atomic_sub_return(UC_WRITER, &rwlock->uc);
+		write_context_enable(wctx, rctx);
+		psrwlock_preempt_check(uc, rwlock);
+	} else {
+		/*
+		 * Release the slow path lock.
+		 */
+		smp_mb();	/* insure memory order with lock mutex */
+		atomic_sub(WS_LOCK_MUTEX, &rwlock->ws);
+		if (rctx & PSR_IRQ) {
+			atomic_long_sub(CTX_WOFFSET,
+				&rwlock->prio[PSRW_PRIO_IRQ]);
+			if (wctx != PSRW_PRIO_IRQ)
+				psrwlock_irq_enable();
+		}
+		if (rctx & PSR_BH) {
+			atomic_long_sub(CTX_WOFFSET,
+				&rwlock->prio[PSRW_PRIO_BH]);
+			if (wctx != PSRW_PRIO_BH)
+				psrwlock_bh_enable();
+		}
+		if (rctx & PSR_NPTHREAD) {
+			atomic_long_sub(CTX_WOFFSET,
+				&rwlock->prio[PSRW_PRIO_NP]);
+			if (wctx != PSRW_PRIO_NP)
+				psrwlock_preempt_enable();
+		}
+		if (rctx & PSR_PTHREAD)
+			atomic_long_sub(CTX_WOFFSET,
+				&rwlock->prio[PSRW_PRIO_P]);
+
+		if (wctx == PSRW_PRIO_P)
+			ptype = PSRW_PREEMPT;
+		else
+			ptype = PSRW_NON_PREEMPT;
+		writer_count_dec(&uc, rwlock, ptype, _RET_IP_);
+		psrwlock_preempt_check(uc, rwlock);
+	}
+}
+EXPORT_SYMBOL_GPL(_pswrite_unlock_slow);
+
+/*
+ * _psrwlock_wakeup : Wake up tasks waiting for a write or read lock.
+ *
+ * Called from any context (irq/softirq/preempt/non-preempt). Contains a
+ * busy-loop; must therefore disable interrupts, but only for a short time.
+ */
+asmregparm void _psrwlock_wakeup(unsigned int uc, psrwlock_t *rwlock)
+{
+	unsigned long flags;
+	unsigned int ws;
+	struct psrwlock_waiter *waiter;
+
+	/*
+	 * Busy-loop waiting for the waitqueue mutex.
+	 */
+	psrwlock_irq_save(flags);
+	/*
+	 * Pass PSRW_READ since unused in PSRW_NON_PREEMPT.
+	 */
+	ws = atomic_read(&rwlock->ws);
+	_pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
+		0, WS_WQ_MUTEX, WS_WQ_MUTEX, WS_WQ_MUTEX,
+		V_INT, PSRW_READ, PSRW_NON_PREEMPT, 0, TASK_UNINTERRUPTIBLE,
+		_RET_IP_);
+	/*
+	 * If there is at least one non-preemptable writer subscribed or holding
+	 * higher priority write masks, let it handle the wakeup when it exits
+	 * its critical section which excludes any preemptable context anyway.
+	 * The same applies to preemptable readers, which are the only ones
+	 * which can cause a preemptable writer to sleep.
+	 *
+	 * The conditions here are all the states in which we are sure to reach
+	 * a preempt check without blocking on the lock.
+	 */
+	uc = atomic_read(&rwlock->uc);
+	if (!(uc & UC_WQ_ACTIVE) || uc & UC_READER_MASK
+			|| (atomic_long_read(&rwlock->prio[PSRW_PRIO_IRQ])
+				& CTX_WMASK)
+			|| (atomic_long_read(&rwlock->prio[PSRW_PRIO_BH])
+				& CTX_WMASK)
+			|| (atomic_long_read(&rwlock->prio[PSRW_PRIO_NP])
+				& CTX_WMASK)) {
+		smp_mb();	/*
+				 * Insure memory ordering when clearing the
+				 * mutex.
+				 */
+		atomic_sub(WS_WQ_MUTEX, &rwlock->ws);
+		psrwlock_irq_restore(flags);
+		return;
+	}
+
+	/*
+	 * First do an exclusive wake-up of the first writer if there is one
+	 * waiting, else wake-up the readers.
+	 */
+	if (!list_empty(&rwlock->wait_list_w))
+		waiter = list_entry(rwlock->wait_list_w.next,
+				    struct psrwlock_waiter, list);
+	else
+		waiter = list_entry(rwlock->wait_list_r.next,
+				    struct psrwlock_waiter, list);
+	debug_psrwlock_wake_waiter(rwlock, waiter);
+	wake_up_process(waiter->task);
+	smp_mb();	/*
+			 * Insure global memory order when clearing the mutex.
+			 */
+	atomic_sub(WS_WQ_MUTEX, &rwlock->ws);
+	psrwlock_irq_restore(flags);
+}
+EXPORT_SYMBOL_GPL(_psrwlock_wakeup);
diff --git a/stblinux-2.6.31/lib/psrwlock.h b/stblinux-2.6.31/lib/psrwlock.h
new file mode 100644
index 0000000..ce0f73d
--- /dev/null
+++ b/stblinux-2.6.31/lib/psrwlock.h
@@ -0,0 +1,22 @@
+/*
+ * Psrwlock
+ *
+ * Internal psrwlock prototypes for !CONFIG_DEBUG_PSRWLOCK config.
+ */
+
+#define psrwlock_remove_waiter(lock, waiter, ti) \
+		__list_del((waiter)->list.prev, (waiter)->list.next)
+
+#define debug_psrwlock_set_owner(lock, new_owner)		do { } while (0)
+#define debug_psrwlock_clear_owner(lock)			do { } while (0)
+#define debug_psrwlock_wake_waiter(lock, waiter)		do { } while (0)
+#define debug_psrwlock_free_waiter(waiter)			do { } while (0)
+#define debug_psrwlock_add_waiter(lock, waiter, ti)	do { } while (0)
+#define debug_psrwlock_unlock(lock, rw)			do { } while (0)
+#define debug_psrwlock_init(lock, name, key)		do { } while (0)
+
+static inline void
+debug_psrwlock_lock_common(struct psrwlock *lock,
+			   struct psrwlock_waiter *waiter)
+{
+}
diff --git a/stblinux-2.6.31/ltt/Kconfig b/stblinux-2.6.31/ltt/Kconfig
new file mode 100644
index 0000000..959a1b5
--- /dev/null
+++ b/stblinux-2.6.31/ltt/Kconfig
@@ -0,0 +1,223 @@
+menuconfig LTT
+	bool "Linux Trace Toolkit Next Generation (LTTng)"
+	depends on EXPERIMENTAL
+	select MARKERS
+	select TRACEPOINTS
+	default n
+	help
+	  It is possible for the kernel to log important events to a trace
+	  facility. Doing so enables the use of the generated traces in order
+	  to reconstruct the dynamic behavior of the kernel, and hence the
+	  whole system.
+
+	  The tracing process contains 4 parts :
+	      1) The logging of events by key parts of the kernel.
+	      2) The tracer that keeps the events in a data buffer (uses
+	         relay).
+	      3) A trace daemon that interacts with the tracer and is
+	         notified every time there is a certain quantity of data to
+	         read from the tracer.
+	      4) A trace event data decoder that reads the accumulated data
+	         and formats it in a human-readable format.
+
+	  If you say Y, the first component will be built into the kernel.
+
+	  For more information on kernel tracing, lttctl, lttd or lttv,
+	  please check the following address :
+	       http://ltt.polymtl.ca
+
+if LTT
+
+config LTT_FILTER
+	tristate
+
+config HAVE_LTT_DUMP_TABLES
+	def_bool n
+
+config LTT_RELAY
+	def_bool n
+
+choice
+	prompt "LTTng Buffer Concurrency Management Algorithm"
+	depends on LTT_TRACER
+	default LTT_RELAY_LOCKLESS
+	help
+	  Choose between the fast lockless and the slower, spinlock/irq disable
+	  mechanism to manage tracing concurrency within a buffer.
+
+	config LTT_RELAY_LOCKLESS
+		bool "Linux Trace Toolkit High-speed Lockless Data Relay"
+	select LTT_RELAY
+	select DEBUG_FS
+	help
+	  Support using the fast lockless algorithm to log the data obtained
+	  through LTT.
+
+	  If you don't have special hardware, you almost certainly want
+	  to say Y here.
+
+	config LTT_RELAY_IRQOFF
+		bool "Linux Trace Toolkit Irq-off Data Relay"
+	select LTT_RELAY
+	select DEBUG_FS
+	depends on BROKEN
+	help
+	  Support using interrupt disable algorithm to log the data obtained
+	  through LTT.
+
+	config LTT_RELAY_LOCKED
+		bool "Linux Trace Toolkit Lock-Protected Data Relay"
+	select LTT_RELAY
+	select DEBUG_FS
+	depends on BROKEN
+	help
+	  Support using the slow spinlock and interrupt disable algorithm to log
+	  the data obtained through LTT.
+
+endchoice
+
+config LTT_SERIALIZE
+	tristate "Linux Trace Toolkit Serializer"
+	depends on LTT_RELAY
+	depends on (LTT_RELAY_LOCKLESS || LTT_RELAY_IRQOFF || LTT_RELAY_LOCKED)
+	default y
+	help
+	  Library for serializing information from format string and argument
+	  list to the trace buffers.
+
+config LTT_FAST_SERIALIZE
+	tristate "Linux Trace Toolkit Custom Serializer"
+	depends on LTT_RELAY
+	depends on (LTT_RELAY_LOCKLESS || LTT_RELAY_IRQOFF || LTT_RELAY_LOCKED)
+	default y
+	help
+	  Library for serializing information from custom, efficient, tracepoint
+	  probes.
+
+config LTT_TRACEPROBES
+	tristate "Compile lttng tracing probes"
+	depends on LTT_FAST_SERIALIZE
+	depends on LTT_SERIALIZE
+	default m
+	select LTT_FILTER
+	help
+	  Compile lttng tracing probes, which connect to the tracepoints when
+	  loaded and format the information collected by the tracepoints with
+	  the Markers.
+
+config LTT_TRACE_CONTROL
+	tristate "Linux Trace Toolkit Trace Controller"
+	depends on LTT_TRACER
+	depends on LTT_SERIALIZE
+	default y
+	help
+	  If you enable this option, the debugfs-based Linux Trace Toolkit Trace
+	  Controller will be either built in the kernel or as module.
+
+config LTT_TRACER
+	tristate "Linux Trace Toolkit Tracer"
+	default y
+	help
+	  If you enable this option, the Linux Trace Toolkit Tracer will be
+	  either built in the kernel or as module.
+
+	  Critical parts of the kernel will call upon the kernel tracing
+	  function. The data is then recorded by the tracer if a trace daemon
+	  is running in user-space and has issued a "start" command.
+
+	  For more information on kernel tracing, the trace daemon or the event
+	  decoder, please check the following address :
+	       http://www.opersys.com/ltt
+	  See also the experimental page of the project :
+	       http://ltt.polymtl.ca
+
+config LTT_ALIGNMENT
+	bool "Align Linux Trace Toolkit Traces"
+	default n
+	help
+	  This option enables dynamic alignment of data in buffers. The
+	  alignment is made on the smallest size between architecture size
+	  and the size of the value to be written.
+
+	  Dynamically calculating the offset of the data has a performance cost,
+	  but it is more efficient on some architectures (especially 64 bits) to
+	  align data than to write it unaligned.
+
+config LTT_CHECK_ARCH_EFFICIENT_UNALIGNED_ACCESS
+	def_bool y
+	select LTT_ALIGNMENT if !HAVE_EFFICIENT_UNALIGNED_ACCESS
+
+config LTT_DEBUG_EVENT_SIZE
+	bool "Add event size field to LTT events for tracer debugging"
+	default n
+	help
+	  Tracer-internal option to help debugging event type encoding problems.
+
+config LTT_USERSPACE_EVENT
+	tristate "Support logging events from userspace"
+	depends on LTT_TRACER
+	depends on LTT_FAST_SERIALIZE
+	default m
+	help
+	  This option lets userspace write text events in
+	  /debugfs/ltt/write_event.
+
+config LTT_VMCORE
+	bool "Support trace extraction from crash dump"
+	default y
+	help
+	  If you enable this option, the Linux Trace Toolkit Tracer will
+	  support extacting ltt log from vmcore, which can be generated with
+	  kdump or LKCD tools.
+
+	  Special crash extension should be used to extract ltt buffers.
+
+config LTT_STATEDUMP
+	tristate "Linux Trace Toolkit State Dump"
+	depends on LTT_TRACER
+	default m
+	help
+	  If you enable this option, the Linux Trace Toolkit State Dump will
+	  be either built in the kernel or as module.
+
+	  This module saves the state of the running kernel at trace start
+	  into the trace buffers along with the ongoing tracing information.
+
+config LTT_KPROBES
+	tristate "Linux Trace Toolkit Kprobes Support"
+	depends on LTT_TRACE_CONTROL
+	depends on LTT_FAST_SERIALIZE
+	depends on LTT_STATEDUMP
+	depends on KPROBES
+	default y
+	help
+	  Allows connecting the LTTng tracer on kprobes using simple debugfs
+	  file operations :
+	    ltt/kprobes/enable
+	    ltt/kprobes/disable
+	    ltt/kprobes/list
+
+config LTT_FTRACE
+	bool "Linux Trace Toolkit Function Tracer Support"
+	depends on LTT_SERIALIZE
+	depends on LTT_MARKER_CONTROL
+	select FUNCTION_TRACER
+	default n
+	help
+	  Integration of function entry trace with LTTng. Connect the
+	  ftrace_cpu_start and ftrace_cpu_stop probes to markers to start/stop
+	  function tracing while a trace is being taken. Typically used when the
+	  surroundings of a problem has been identified in a prior trace.
+
+config LTT_ASCII
+	bool "Linux Trace Toolkit Ascii Output (EXPERIMENTAL)"
+	depends on EXPERIMENTAL
+	depends on LTT_TRACER
+	depends on LTT_RELAY
+	depends on LTT_SERIALIZE
+	default n
+	help
+	  Output trace data in a text-formatted ascii file, presented in
+	  /mnt/debugfs/ltt/ascii/<trace name>.
+
+endif # LTT
diff --git a/stblinux-2.6.31/ltt/Makefile b/stblinux-2.6.31/ltt/Makefile
new file mode 100644
index 0000000..88d3d46
--- /dev/null
+++ b/stblinux-2.6.31/ltt/Makefile
@@ -0,0 +1,35 @@
+#
+# Makefile for the LTT objects.
+#
+
+obj-$(CONFIG_MARKERS)			+= ltt-channels.o
+obj-$(CONFIG_LTT)			+= ltt-core.o
+obj-$(CONFIG_LTT_TRACER)		+= ltt-tracer.o
+obj-$(CONFIG_LTT_TRACE_CONTROL)		+= ltt-marker-control.o
+
+ifdef CONFIG_LTT_RELAY_LOCKLESS
+RELAY_LOCKING := ltt-relay-lockless.o
+endif
+
+ifdef CONFIG_LTT_RELAY_IRQOFF
+RELAY_LOCKING := ltt-relay-irqoff.o
+endif
+
+ifdef CONFIG_LTT_RELAY_LOCKED
+RELAY_LOCKING := ltt-relay-locked.o
+endif
+
+obj-$(CONFIG_LTT_RELAY) += ltt-relay.o
+ltt-relay-objs := $(RELAY_LOCKING) ltt-relay-alloc.o ltt-relay-splice.o \
+		  ltt-relay-vfs.o
+
+obj-$(CONFIG_LTT_SERIALIZE)		+= ltt-serialize.o
+obj-$(CONFIG_LTT_STATEDUMP)		+= ltt-statedump.o
+obj-$(CONFIG_LTT_FAST_SERIALIZE)	+= ltt-type-serializer.o
+obj-$(CONFIG_LTT_TRACE_CONTROL)		+= ltt-trace-control.o
+obj-$(CONFIG_LTT_USERSPACE_EVENT)	+= ltt-userspace-event.o
+obj-$(CONFIG_LTT_FILTER)		+= ltt-filter.o
+obj-$(CONFIG_LTT_KPROBES)		+= ltt-kprobes.o
+obj-$(CONFIG_LTT_TRACEPROBES)		+= probes/
+obj-$(CONFIG_LTT_FTRACE)		+= ltt-ftrace.o
+obj-$(CONFIG_LTT_ASCII)			+= ltt-ascii.o
diff --git a/stblinux-2.6.31/ltt/ltt-ascii.c b/stblinux-2.6.31/ltt/ltt-ascii.c
new file mode 100644
index 0000000..eeb7033
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-ascii.c
@@ -0,0 +1,571 @@
+/*
+ * LTT ascii binary buffer to ascii converter.
+ *
+ * Copyright       2008 - 2009   Lai Jiangshan (laijs@cn.fujitsu.com)
+ * Copyright       2009 -        Mathieu Desnoyers mathieu.desnoyers@polymtl.ca
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+/*
+ * TODO
+ *
+ * Move to new switch behavior: Wait for data for the duration of the
+ * timer interval + safety, if none is coming, consider that no activity occured
+ * in the buffer.
+ *
+ * Fix case when having a text file open and destroying trace.
+ *
+ * - Automate periodical switch:
+ *
+ * The debugfs file "switch_timer" receives a timer period as parameter
+ * (e.g. echo 100 > switch_timer) to activate the timer per channel. This can
+ * also be accessed through the internal API _before the trace session starts_.
+ * This timer will insure that we periodically have subbuffers to read, and
+ * therefore that the merge-sort does not wait endlessly for a subbuffer.
+ *
+ * - If a channel is switched and read without data, make sure it is still
+ * considered afterward (not removed from the queue).
+ *
+ * - Create a ascii/tracename/ALL file to merge-sort all active channels.
+ * - Create a ascii/tracename/README file to contain the text output legend.
+ * - Remove leading zeroes from timestamps.
+ * - Enhance pretty-printing to make sure all types used for addesses output in
+ * the form 0xAB00000000 (not decimal). This is true for %p and 0x%...X.
+ * - Hotplug support
+ */
+
+
+
+
+#include <linux/module.h>
+#include <linux/ltt-tracer.h>
+#include <linux/ltt-relay.h>
+#include <linux/seq_file.h>
+#include <linux/debugfs.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/cpu.h>
+#include <linux/fs.h>
+
+#include "ltt-relay-select.h"
+
+#if 0
+#define DEBUGP printk
+#else
+#define DEBUGP(fmt , a...)
+#endif
+
+struct dentry *ltt_ascii_dir_dentry;
+EXPORT_SYMBOL_GPL(ltt_ascii_dir_dentry);
+
+struct ltt_relay_iter;
+
+struct ltt_relay_cpu_iter {
+	/* cpu buffer information */
+	struct ltt_chanbuf *buf;
+	struct ltt_relay_iter *iter;
+	int sb_ref;		/* holding a reference to a subbuffer */
+	long read_sb_offset;	/* offset of the subbuffer read */
+
+	/* current event information */
+	struct ltt_subbuffer_header *header;
+	long hdr_offset;	/* event header offset */
+	long payload_offset;	/* event payload offset */
+	u64 tsc;	/* full 64-bits timestamp value */
+	u32 data_size;
+	u16 chID;	/* channel ID, const */
+	u16 eID;
+};
+
+struct ltt_relay_iter {
+	struct ltt_relay_cpu_iter iter_cpu[NR_CPUS];
+	struct ltt_chan *chan;
+	loff_t pos;
+	int cpu;
+	int nr_refs;
+};
+
+/*
+ * offset of 0 in subbuffer means "subbuf size" (filled subbuffer).
+ */
+static int is_subbuffer_offset_end(struct ltt_relay_cpu_iter *citer,
+				   long offset)
+{
+	struct ltt_chan *chan = container_of(citer->buf->a.chan,
+					     struct ltt_chan, a);
+	long sub_offset = SUBBUF_OFFSET(offset - 1, chan) + 1;
+
+	return (sub_offset <= citer->header->data_size);
+}
+
+static u64 calculate_tsc(u64 pre_tsc, u64 read_tsc, unsigned int rflags)
+{
+	u64 new_tsc = read_tsc;
+
+	if (rflags != LTT_RFLAG_ID_SIZE_TSC) {
+		BUG_ON(read_tsc >> LTT_TSC_BITS);
+
+		new_tsc = (pre_tsc & ~LTT_TSC_MASK) + read_tsc;
+		if (read_tsc < (pre_tsc & LTT_TSC_MASK))
+			new_tsc += 1UL << LTT_TSC_BITS;
+	}
+
+	return new_tsc;
+}
+
+/*
+ * calculate payload offset */
+static inline long calculate_payload_offset(long offset, u16 chID, u16 eID)
+{
+	const char *fmt;
+
+	if (!ltt_get_alignment())
+		return offset;
+
+	fmt = marker_get_fmt_from_id(chID, eID);
+	BUG_ON(!fmt);
+
+	return offset + ltt_fmt_largest_align(offset, fmt);
+}
+
+static void update_new_event(struct ltt_relay_cpu_iter *citer, long hdr_offset)
+{
+	u64 read_tsc;
+	unsigned int rflags;
+	long tmp_offset;
+
+	WARN_ON_ONCE(hdr_offset != citer->hdr_offset);
+
+	tmp_offset = ltt_read_event_header(&citer->buf->a, hdr_offset,
+					   &read_tsc, &citer->data_size,
+					   &citer->eID, &rflags);
+	citer->payload_offset = calculate_payload_offset(tmp_offset,
+							 citer->chID,
+							 citer->eID);
+
+	citer->tsc = calculate_tsc(citer->tsc, read_tsc, rflags);
+}
+
+static void update_event_size(struct ltt_relay_cpu_iter *citer, long hdr_offset)
+{
+	char output[1];
+	const char *fmt;
+	size_t data_size;
+
+	if (citer->data_size != INT_MAX)
+		return;
+
+	fmt = marker_get_fmt_from_id(citer->chID, citer->eID);
+	BUG_ON(!fmt);
+	ltt_serialize_printf(citer->buf, citer->payload_offset,
+			     &data_size, output, 0, fmt);
+	citer->data_size = data_size;
+}
+
+static void update_cpu_iter(struct ltt_relay_cpu_iter *citer, long hdr_offset)
+{
+	if (unlikely((!citer->sb_ref)
+		     || is_subbuffer_offset_end(citer, hdr_offset))) {
+		citer->header = NULL;
+		return;
+	}
+	update_new_event(citer, hdr_offset);
+	update_event_size(citer, hdr_offset);
+}
+
+/*
+ * returns 0 if we get a subbuffer reference.
+ * else, the buffer has not available data, try again later.
+ */
+static int subbuffer_start(struct ltt_relay_cpu_iter *citer, long *offset)
+{
+	int ret;
+	struct ltt_relay_iter *iter = citer->iter;
+
+	ret = ltt_chanbuf_get_subbuf(citer->buf, offset);
+	if (!ret) {
+		citer->header = ltt_relay_read_offset_address(&citer->buf->a,
+							      *offset);
+		citer->hdr_offset = (*offset) + ltt_sb_header_size();
+		citer->tsc = citer->header->cycle_count_begin;
+		iter->nr_refs++;
+		citer->sb_ref = 1;
+		return 0;
+	} else {
+		if (ltt_chanbuf_is_finalized(citer->buf))
+			return -ENODATA;
+		else
+			return -EAGAIN;
+	}
+}
+
+static void subbuffer_stop(struct ltt_relay_cpu_iter *citer,
+			   long offset)
+{
+	int ret;
+	struct ltt_relay_iter *iter = citer->iter;
+
+	WARN_ON_ONCE(!citer->sb_ref);
+	ret = ltt_chanbuf_put_subbuf(citer->buf, offset);
+	WARN_ON_ONCE(ret);
+	citer->sb_ref = 0;
+	iter->nr_refs--;
+}
+
+static void ltt_relay_advance_cpu_iter(struct ltt_relay_cpu_iter *citer)
+{
+	long old_offset = citer->payload_offset;
+	long new_offset = citer->payload_offset;
+	int ret;
+
+	/* find that whether we read all data in this subbuffer */
+	if (unlikely(is_subbuffer_offset_end(citer,
+					     old_offset + citer->data_size))) {
+		DEBUGP(KERN_DEBUG "LTT ASCII stop cpu %d offset %lX\n",
+		       citer->buf->a.cpu, citer->read_sb_offset);
+		subbuffer_stop(citer, citer->read_sb_offset);
+		for (;;) {
+			ret = subbuffer_start(citer, &citer->read_sb_offset);
+			DEBUGP(KERN_DEBUG
+			       "LTT ASCII start cpu %d ret %d offset %lX\n",
+			       citer->buf->a.cpu, ret, citer->read_sb_offset);
+			if (!ret || ret == -ENODATA) {
+				break;	/* got data, or finalized */
+			} else {	/* -EAGAIN */
+				if (signal_pending(current))
+					break;
+				schedule_timeout_interruptible(1);
+				//TODO: check for no-data delay. take ref. break
+			}
+		}
+	} else {
+		new_offset += citer->data_size;
+		citer->hdr_offset = new_offset + ltt_align(new_offset, sizeof(struct ltt_event_header));
+		DEBUGP(KERN_DEBUG
+		       "LTT ASCII old_offset %lX new_offset %lX cpu %d\n",
+		       old_offset, new_offset, citer->buf->a.cpu);
+	}
+
+	update_cpu_iter(citer, citer->hdr_offset);
+}
+
+static int cpu_iter_eof(struct ltt_relay_cpu_iter *citer)
+{
+	return !citer->sb_ref;
+}
+
+static int ltt_relay_iter_eof(struct ltt_relay_iter *iter)
+{
+	return iter->nr_refs == 0;
+}
+
+static void ltt_relay_advance_iter(struct ltt_relay_iter *iter)
+{
+	int i;
+	struct ltt_relay_cpu_iter *curr, *min = NULL;
+	iter->cpu = -1;
+
+	/*
+	 * find the event with the minimum tsc.
+	 * TODO: use min-heep for 4096CPUS
+	 */
+	for_each_possible_cpu(i) {
+		curr = &iter->iter_cpu[i];
+
+		if (!curr->buf->a.allocated || !curr->header)
+			continue;
+
+		if (cpu_iter_eof(curr))
+			continue;
+
+		if (!min || curr->tsc < min->tsc) {
+			min = curr;
+			iter->cpu = i;
+		}
+	}
+
+	/* update cpu_iter for next ltt_relay_advance_iter() */
+	if (min)
+		ltt_relay_advance_cpu_iter(min);
+}
+
+static void *ascii_next(struct seq_file *m, void *v, loff_t *ppos)
+{
+	struct ltt_relay_iter *iter = m->private;
+
+	WARN_ON_ONCE(!iter->nr_refs);
+	BUG_ON(v != iter);
+
+	ltt_relay_advance_iter(iter);
+	return (ltt_relay_iter_eof(iter) || signal_pending(current))
+		? NULL : iter;
+}
+
+static void *ascii_start(struct seq_file *m, loff_t *ppos)
+{
+	struct ltt_relay_iter *iter = m->private;
+
+	ltt_relay_advance_iter(iter);
+	return (ltt_relay_iter_eof(iter) || signal_pending(current))
+		? NULL : iter;
+}
+
+static void ascii_stop(struct seq_file *m, void *v)
+{
+}
+
+static
+int seq_serialize(struct seq_file *m, struct ltt_chanbuf *buf,
+		  size_t buf_offset, const char *fmt, size_t *data_size)
+{
+	int len;
+
+	if (m->count < m->size) {
+		len = ltt_serialize_printf(buf, buf_offset, data_size,
+					   m->buf + m->count,
+					   m->size - m->count, fmt);
+		if (m->count + len < m->size) {
+			m->count += len;
+			return 0;
+		}
+	}
+
+	m->count = m->size;
+	return -1;
+}
+
+static int ascii_show(struct seq_file *m, void *v)
+{
+	struct ltt_relay_iter *iter = v;
+	struct ltt_relay_cpu_iter *citer;
+	const char *name;
+	const char *fmt;
+	unsigned long long tsc;
+	size_t data_size;
+
+	if (iter->cpu == -1)
+		return 0;
+
+	citer = &iter->iter_cpu[iter->cpu];
+	WARN_ON_ONCE(!citer->sb_ref);
+	/*
+	 * Nothing to show, we are at the end of the last subbuffer currently
+	 * having data.
+	 */
+	if (!citer->header)
+		return 0;
+
+	tsc = citer->tsc;
+	name = marker_get_name_from_id(citer->chID, citer->eID);
+	fmt = marker_get_fmt_from_id(citer->chID, citer->eID);
+
+	if (!name || !fmt)
+		return 0;
+
+	seq_printf(m, "event:%16.16s: cpu:%2d time:%20.20llu ",
+		   name, iter->cpu, tsc);
+	seq_serialize(m, citer->buf, citer->payload_offset, fmt, &data_size);
+	seq_puts(m, "\n");
+	if (citer->data_size == INT_MAX)
+		citer->data_size = data_size;
+
+	return 0;
+}
+
+static struct seq_operations ascii_seq_ops = {
+	.start		= ascii_start,
+	.next		= ascii_next,
+	.stop		= ascii_stop,
+	.show		= ascii_show,
+};
+
+/* FIXME : cpu hotplug support */
+static int ltt_relay_iter_open_channel(struct ltt_relay_iter *iter,
+				       struct ltt_chan *chan)
+{
+	int i, ret;
+	u16 chID = ltt_channels_get_index_from_name(chan->a.filename);
+
+	/* we don't need lock relay_channels_mutex */
+	for_each_possible_cpu(i) {
+		struct ltt_relay_cpu_iter *citer = &iter->iter_cpu[i];
+
+		citer->buf = per_cpu_ptr(chan->a.buf, i);
+		if (!citer->buf->a.allocated)
+			continue;
+
+		citer->iter = iter;	/* easy lazy parent info */
+		citer->chID = chID;
+
+		ret = ltt_chanbuf_open_read(citer->buf);
+		if (ret) {
+			/* Failed to open a percpu buffer, close everything. */
+			citer->buf = NULL;
+			goto error;
+		}
+
+		for (;;) {
+			ret = subbuffer_start(citer,
+					      &citer->read_sb_offset);
+			DEBUGP(KERN_DEBUG
+				"LTT ASCII open start "
+				"cpu %d ret %d offset %lX\n",
+				citer->buf->a.cpu, ret, citer->read_sb_offset);
+			if (!ret || ret == -ENODATA) {
+				break;	/* got data, or finalized */
+			} else {	/* -EAGAIN */
+				if (signal_pending(current))
+					break;
+				schedule_timeout_interruptible(1);
+			}
+		}
+		update_cpu_iter(citer, citer->hdr_offset);
+	}
+	if (!iter->nr_refs)
+		return -ENODATA; /* no data available */
+	return 0;
+
+error:
+	for_each_possible_cpu(i) {
+		struct ltt_relay_cpu_iter *citer = &iter->iter_cpu[i];
+
+		if (!citer->buf)
+			break;
+
+		if (citer->buf->a.allocated)
+			ltt_chanbuf_release_read(citer->buf);
+	}
+	return ret;
+}
+
+/* FIXME : cpu hotplug support */
+static int ltt_relay_iter_release_channel(struct ltt_relay_iter *iter)
+{
+	int i;
+
+	for_each_possible_cpu(i) {
+		struct ltt_relay_cpu_iter *citer = &iter->iter_cpu[i];
+
+		if (citer->sb_ref) {
+			WARN_ON_ONCE(!citer->buf->a.allocated);
+			DEBUGP(KERN_DEBUG
+				"LTT ASCII release stop cpu %d offset %lX\n",
+				citer->buf->a.cpu, citer->read_sb_offset);
+			subbuffer_stop(&iter->iter_cpu[i],
+				       citer->read_sb_offset);
+		}
+		if (citer->buf->a.allocated)
+			ltt_chanbuf_release_read(citer->buf);
+	}
+	WARN_ON_ONCE(iter->nr_refs);
+	return 0;
+}
+
+static int ltt_relay_ascii_open(struct inode *inode, struct file *file)
+{
+	int ret;
+	struct ltt_chan *chan = inode->i_private;
+	struct ltt_relay_iter *iter = kzalloc(sizeof(*iter), GFP_KERNEL);
+	if (!iter)
+		return -ENOMEM;
+
+	iter->chan = chan;
+	ret = ltt_relay_iter_open_channel(iter, chan);
+	if (ret)
+		goto error_free_alloc;
+
+	ret = seq_open(file, &ascii_seq_ops);
+	if (ret)
+		goto error_release_channel;
+	((struct seq_file *)file->private_data)->private = iter;
+	return 0;
+
+error_release_channel:
+	ltt_relay_iter_release_channel(iter);
+error_free_alloc:
+	kfree(iter);
+	return ret;
+}
+
+static int ltt_relay_ascii_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *seq = file->private_data;
+	struct ltt_relay_iter *iter = seq->private;
+
+	ltt_relay_iter_release_channel(iter);
+	kfree(iter);
+	return 0;
+}
+
+static struct file_operations ltt_ascii_fops =
+{
+	.read = seq_read,
+	.open = ltt_relay_ascii_open,
+	.release = ltt_relay_ascii_release,
+	.llseek = no_llseek,
+	.owner = THIS_MODULE,
+};
+
+int ltt_ascii_create(struct ltt_chan *chan)
+{
+	struct dentry *dentry;
+
+	dentry = debugfs_create_file(chan->a.filename,
+				     S_IRUSR | S_IRGRP,
+				     chan->a.trace->dentry.ascii_root,
+				     chan, &ltt_ascii_fops);
+	if (IS_ERR(dentry))
+		return PTR_ERR(dentry);
+
+	if (!dentry)
+		return -EEXIST;
+
+	chan->a.ascii_dentry = dentry;
+	dentry->d_inode->i_private = chan;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ltt_ascii_create);
+
+void ltt_ascii_remove(struct ltt_chan *chan)
+{
+	debugfs_remove(chan->a.ascii_dentry);
+}
+EXPORT_SYMBOL_GPL(ltt_ascii_remove);
+
+int ltt_ascii_create_dir(struct ltt_trace *new_trace)
+{
+	new_trace->dentry.ascii_root = debugfs_create_dir(new_trace->trace_name,
+							  ltt_ascii_dir_dentry);
+	if (!new_trace->dentry.ascii_root)
+		return -EEXIST;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ltt_ascii_create_dir);
+
+void ltt_ascii_remove_dir(struct ltt_trace *trace)
+{
+	debugfs_remove(trace->dentry.ascii_root);
+}
+EXPORT_SYMBOL_GPL(ltt_ascii_remove_dir);
+
+static __init int ltt_ascii_init(void)
+{
+	ltt_ascii_dir_dentry = debugfs_create_dir(LTT_ASCII, get_ltt_root());
+	put_ltt_root();
+
+	return ltt_ascii_dir_dentry ? 0 : -EFAULT;
+}
+
+static __exit void ltt_ascii_exit(void)
+{
+	debugfs_remove(ltt_ascii_dir_dentry);
+}
+
+module_init(ltt_ascii_init);
+module_exit(ltt_ascii_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Lai Jiangshan@FNST and Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Ascii Converter");
diff --git a/stblinux-2.6.31/ltt/ltt-channels.c b/stblinux-2.6.31/ltt/ltt-channels.c
new file mode 100644
index 0000000..81be66a
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-channels.c
@@ -0,0 +1,387 @@
+/*
+ * ltt/ltt-channels.c
+ *
+ * (C) Copyright 2008 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng channel management.
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/ltt-channels.h>
+#include <linux/mutex.h>
+#include <linux/vmalloc.h>
+
+/*
+ * ltt_channel_mutex may be nested inside the LTT trace mutex.
+ * ltt_channel_mutex mutex may be nested inside markers mutex.
+ */
+static DEFINE_MUTEX(ltt_channel_mutex);
+static LIST_HEAD(ltt_channels);
+/*
+ * Index of next channel in array. Makes sure that as long as a trace channel is
+ * allocated, no array index will be re-used when a channel is freed and then
+ * another channel is allocated. This index is cleared and the array indexeds
+ * get reassigned when the index_kref goes back to 0, which indicates that no
+ * more trace channels are allocated.
+ */
+static unsigned int free_index;
+/* index_kref is protected by both ltt_channel_mutex and lock_markers */
+static struct kref index_kref;	/* Keeps track of allocated trace channels */
+
+static struct ltt_channel_setting *lookup_channel(const char *name)
+{
+	struct ltt_channel_setting *iter;
+
+	list_for_each_entry(iter, &ltt_channels, list)
+		if (strcmp(name, iter->name) == 0)
+			return iter;
+	return NULL;
+}
+
+/*
+ * Must be called when channel refcount falls to 0 _and_ also when the last
+ * trace is freed. This function is responsible for compacting the channel and
+ * event IDs when no users are active.
+ *
+ * Called with lock_markers() and channels mutex held.
+ */
+static void release_channel_setting(struct kref *kref)
+{
+	struct ltt_channel_setting *setting = container_of(kref,
+		struct ltt_channel_setting, kref);
+	struct ltt_channel_setting *iter;
+
+	if (atomic_read(&index_kref.refcount) == 0
+	    && atomic_read(&setting->kref.refcount) == 0) {
+		list_del(&setting->list);
+		kfree(setting);
+
+		free_index = 0;
+		list_for_each_entry(iter, &ltt_channels, list) {
+			iter->index = free_index++;
+			iter->free_event_id = 0;
+		}
+	}
+}
+
+/*
+ * Perform channel index compaction when the last trace channel is freed.
+ *
+ * Called with lock_markers() and channels mutex held.
+ */
+static void release_trace_channel(struct kref *kref)
+{
+	struct ltt_channel_setting *iter, *n;
+
+	list_for_each_entry_safe(iter, n, &ltt_channels, list)
+		release_channel_setting(&iter->kref);
+	if (atomic_read(&index_kref.refcount) == 0)
+		markers_compact_event_ids();
+}
+
+/*
+ * ltt_channel_trace_ref :  Is there an existing trace session ?
+ *
+ * Must be called with lock_markers() held.
+ */
+int ltt_channels_trace_ref(void)
+{
+	return !!atomic_read(&index_kref.refcount);
+}
+EXPORT_SYMBOL_GPL(ltt_channels_trace_ref);
+
+/**
+ * ltt_channels_register - Register a trace channel.
+ * @name: channel name
+ *
+ * Uses refcounting.
+ */
+int ltt_channels_register(const char *name)
+{
+	struct ltt_channel_setting *setting;
+	int ret = 0;
+
+	mutex_lock(&ltt_channel_mutex);
+	setting = lookup_channel(name);
+	if (setting) {
+		if (atomic_read(&setting->kref.refcount) == 0)
+			goto init_kref;
+		else {
+			kref_get(&setting->kref);
+			goto end;
+		}
+	}
+	setting = kzalloc(sizeof(*setting), GFP_KERNEL);
+	if (!setting) {
+		ret = -ENOMEM;
+		goto end;
+	}
+	list_add(&setting->list, &ltt_channels);
+	strncpy(setting->name, name, PATH_MAX-1);
+	setting->index = free_index++;
+init_kref:
+	kref_init(&setting->kref);
+end:
+	mutex_unlock(&ltt_channel_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_channels_register);
+
+/**
+ * ltt_channels_unregister - Unregister a trace channel.
+ * @name: channel name
+ * @compacting: performing compaction
+ *
+ * Must be called with markers mutex held.
+ */
+int ltt_channels_unregister(const char *name, int compacting)
+{
+	struct ltt_channel_setting *setting;
+	int ret = 0;
+
+	if (!compacting)
+		mutex_lock(&ltt_channel_mutex);
+	setting = lookup_channel(name);
+	if (!setting || atomic_read(&setting->kref.refcount) == 0) {
+		ret = -ENOENT;
+		goto end;
+	}
+	kref_put(&setting->kref, release_channel_setting);
+	if (!compacting && atomic_read(&index_kref.refcount) == 0)
+			markers_compact_event_ids();
+end:
+	if (!compacting)
+		mutex_unlock(&ltt_channel_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_channels_unregister);
+
+/**
+ * ltt_channels_set_default - Set channel default behavior.
+ * @name: default channel name
+ * @sb_size: size of the subbuffers
+ * @n_sb: number of subbuffers
+ */
+int ltt_channels_set_default(const char *name,
+			     unsigned int sb_size,
+			     unsigned int n_sb)
+{
+	struct ltt_channel_setting *setting;
+	int ret = 0;
+
+	mutex_lock(&ltt_channel_mutex);
+	setting = lookup_channel(name);
+	if (!setting || atomic_read(&setting->kref.refcount) == 0) {
+		ret = -ENOENT;
+		goto end;
+	}
+	setting->sb_size = sb_size;
+	setting->n_sb = n_sb;
+end:
+	mutex_unlock(&ltt_channel_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_channels_set_default);
+
+/**
+ * ltt_channels_get_name_from_index - get channel name from channel index
+ * @index: channel index
+ *
+ * Allows to lookup the channel name given its index. Done to keep the name
+ * information outside of each trace channel instance.
+ */
+const char *ltt_channels_get_name_from_index(unsigned int index)
+{
+	struct ltt_channel_setting *iter;
+
+	list_for_each_entry(iter, &ltt_channels, list)
+		if (iter->index == index && atomic_read(&iter->kref.refcount))
+			return iter->name;
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(ltt_channels_get_name_from_index);
+
+static struct ltt_channel_setting *
+ltt_channels_get_setting_from_name(const char *name)
+{
+	struct ltt_channel_setting *iter;
+
+	list_for_each_entry(iter, &ltt_channels, list)
+		if (!strcmp(iter->name, name)
+		    && atomic_read(&iter->kref.refcount))
+			return iter;
+	return NULL;
+}
+
+/**
+ * ltt_channels_get_index_from_name - get channel index from channel name
+ * @name: channel name
+ *
+ * Allows to lookup the channel index given its name. Done to keep the name
+ * information outside of each trace channel instance.
+ * Returns -1 if not found.
+ */
+int ltt_channels_get_index_from_name(const char *name)
+{
+	struct ltt_channel_setting *setting;
+
+	setting = ltt_channels_get_setting_from_name(name);
+	if (setting)
+		return setting->index;
+	else
+		return -1;
+}
+EXPORT_SYMBOL_GPL(ltt_channels_get_index_from_name);
+
+/**
+ * ltt_channels_trace_alloc - Allocate channel structures for a trace
+ * @sb_size: subbuffer size. 0 uses default.
+ * @n_sb: number of subbuffers per per-cpu buffers. 0 uses default.
+ * @flags: Default channel flags
+ *
+ * Use the current channel list to allocate the channels for a trace.
+ * Called with trace lock held. Does not perform the trace buffer allocation,
+ * because we must let the user overwrite specific channel sizes.
+ */
+struct ltt_chan *ltt_channels_trace_alloc(unsigned int *nr_channels,
+					  int overwrite, int active)
+{
+	struct ltt_chan *chan = NULL;
+	struct ltt_channel_setting *iter;
+
+	lock_markers();
+	mutex_lock(&ltt_channel_mutex);
+	if (!free_index)
+		goto end;
+	if (!atomic_read(&index_kref.refcount))
+		kref_init(&index_kref);
+	else
+		kref_get(&index_kref);
+	*nr_channels = free_index;
+	chan = kzalloc(sizeof(struct ltt_chan) * free_index, GFP_KERNEL);
+	if (!chan)
+		goto end;
+	list_for_each_entry(iter, &ltt_channels, list) {
+		if (!atomic_read(&iter->kref.refcount))
+			continue;
+		chan[iter->index].a.sb_size = iter->sb_size;
+		chan[iter->index].a.n_sb = iter->n_sb;
+		chan[iter->index].overwrite = overwrite;
+		chan[iter->index].active = active;
+		strncpy(chan[iter->index].a.filename, iter->name, NAME_MAX - 1);
+		chan[iter->index].switch_timer_interval = 0;
+	}
+end:
+	mutex_unlock(&ltt_channel_mutex);
+	unlock_markers();
+	return chan;
+}
+EXPORT_SYMBOL_GPL(ltt_channels_trace_alloc);
+
+/**
+ * ltt_channels_trace_free - Free one trace's channels
+ * @channels: channels to free
+ *
+ * Called with trace lock held. The actual channel buffers must be freed before
+ * this function is called.
+ */
+void ltt_channels_trace_free(struct ltt_chan *channels,
+			     unsigned int nr_channels)
+{
+	lock_markers();
+	mutex_lock(&ltt_channel_mutex);
+	kfree(channels);
+	kref_put(&index_kref, release_trace_channel);
+	mutex_unlock(&ltt_channel_mutex);
+	unlock_markers();
+	marker_update_probes();
+}
+EXPORT_SYMBOL_GPL(ltt_channels_trace_free);
+
+/**
+ * ltt_channels_trace_set_timer - set switch timer
+ * @channel: channel
+ * @interval: interval of timer interrupt, in jiffies. 0 inhibits timer.
+ */
+
+void ltt_channels_trace_set_timer(struct ltt_chan *chan,
+				  unsigned long interval)
+{
+	chan->switch_timer_interval = interval;
+}
+EXPORT_SYMBOL_GPL(ltt_channels_trace_set_timer);
+
+/**
+ * _ltt_channels_get_event_id - get next event ID for a marker
+ * @channel: channel name
+ * @name: event name
+ *
+ * Returns a unique event ID (for this channel) or < 0 on error.
+ * Must be called with channels mutex held.
+ */
+int _ltt_channels_get_event_id(const char *channel, const char *name)
+{
+	struct ltt_channel_setting *setting;
+	int ret;
+
+	setting = ltt_channels_get_setting_from_name(channel);
+	if (!setting) {
+		ret = -ENOENT;
+		goto end;
+	}
+	if (strcmp(channel, "metadata") == 0) {
+		if (strcmp(name, "core_marker_id") == 0)
+			ret = 0;
+		else if (strcmp(name, "core_marker_format") == 0)
+			ret = 1;
+		else
+			ret = -ENOENT;
+		goto end;
+	}
+	if (setting->free_event_id == EVENTS_PER_CHANNEL - 1) {
+		ret = -ENOSPC;
+		goto end;
+	}
+	ret = setting->free_event_id++;
+end:
+	return ret;
+}
+
+/**
+ * ltt_channels_get_event_id - get next event ID for a marker
+ * @channel: channel name
+ * @name: event name
+ *
+ * Returns a unique event ID (for this channel) or < 0 on error.
+ */
+int ltt_channels_get_event_id(const char *channel, const char *name)
+{
+	int ret;
+
+	mutex_lock(&ltt_channel_mutex);
+	ret = _ltt_channels_get_event_id(channel, name);
+	mutex_unlock(&ltt_channel_mutex);
+	return ret;
+}
+
+/**
+ * ltt_channels_reset_event_ids - reset event IDs at compaction
+ *
+ * Called with lock marker and channel mutex held.
+ */
+void _ltt_channels_reset_event_ids(void)
+{
+	struct ltt_channel_setting *iter;
+
+	list_for_each_entry(iter, &ltt_channels, list)
+		iter->free_event_id = 0;
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Channel Management");
diff --git a/stblinux-2.6.31/ltt/ltt-core.c b/stblinux-2.6.31/ltt/ltt-core.c
new file mode 100644
index 0000000..fcac8f7
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-core.c
@@ -0,0 +1,107 @@
+/*
+ * LTT core in-kernel infrastructure.
+ *
+ * Copyright 2006 - Mathieu Desnoyers mathieu.desnoyers@polymtl.ca
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/ltt-core.h>
+#include <linux/percpu.h>
+#include <linux/module.h>
+#include <linux/debugfs.h>
+#include <linux/kref.h>
+#include <linux/cpu.h>
+
+/* Traces structures */
+struct ltt_traces ltt_traces = {
+	.setup_head = LIST_HEAD_INIT(ltt_traces.setup_head),
+	.head = LIST_HEAD_INIT(ltt_traces.head),
+};
+EXPORT_SYMBOL(ltt_traces);
+
+/* Traces list writer locking */
+static DEFINE_MUTEX(ltt_traces_mutex);
+
+/* root dentry mutex */
+static DEFINE_MUTEX(ltt_root_mutex);
+/* dentry of ltt's root dir */
+static struct dentry *ltt_root_dentry;
+static struct kref ltt_root_kref = {
+	.refcount = ATOMIC_INIT(0),
+};
+
+static void ltt_root_release(struct kref *ref)
+{
+	debugfs_remove(ltt_root_dentry);
+	ltt_root_dentry = NULL;
+}
+
+void put_ltt_root(void)
+{
+	mutex_lock(&ltt_root_mutex);
+	if (ltt_root_dentry)
+		kref_put(&ltt_root_kref, ltt_root_release);
+	mutex_unlock(&ltt_root_mutex);
+}
+EXPORT_SYMBOL_GPL(put_ltt_root);
+
+struct dentry *get_ltt_root(void)
+{
+	mutex_lock(&ltt_root_mutex);
+	if (!ltt_root_dentry) {
+		ltt_root_dentry = debugfs_create_dir(LTT_ROOT, NULL);
+		if (!ltt_root_dentry) {
+			printk(KERN_ERR "LTT : create ltt root dir failed\n");
+			goto out;
+		}
+		kref_init(&ltt_root_kref);
+		goto out;
+	}
+	kref_get(&ltt_root_kref);
+out:
+	mutex_unlock(&ltt_root_mutex);
+	return ltt_root_dentry;
+}
+EXPORT_SYMBOL_GPL(get_ltt_root);
+
+/*
+ * ltt_lock_traces/ltt_unlock_traces also disables cpu hotplug.
+ */
+void ltt_lock_traces(void)
+{
+	mutex_lock(&ltt_traces_mutex);
+	get_online_cpus();
+}
+EXPORT_SYMBOL_GPL(ltt_lock_traces);
+
+void ltt_unlock_traces(void)
+{
+	put_online_cpus();
+	mutex_unlock(&ltt_traces_mutex);
+}
+EXPORT_SYMBOL_GPL(ltt_unlock_traces);
+
+DEFINE_PER_CPU(unsigned int, ltt_nesting);
+EXPORT_PER_CPU_SYMBOL(ltt_nesting);
+
+int ltt_run_filter_default(void *trace, uint16_t eID)
+{
+	return 1;
+}
+
+/* This function pointer is protected by a trace activation check */
+ltt_run_filter_functor ltt_run_filter = ltt_run_filter_default;
+EXPORT_SYMBOL_GPL(ltt_run_filter);
+
+void ltt_filter_register(ltt_run_filter_functor func)
+{
+	ltt_run_filter = func;
+}
+EXPORT_SYMBOL_GPL(ltt_filter_register);
+
+void ltt_filter_unregister(void)
+{
+	ltt_run_filter = ltt_run_filter_default;
+}
+EXPORT_SYMBOL_GPL(ltt_filter_unregister);
diff --git a/stblinux-2.6.31/ltt/ltt-filter.c b/stblinux-2.6.31/ltt/ltt-filter.c
new file mode 100644
index 0000000..255ef08
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-filter.c
@@ -0,0 +1,54 @@
+/*
+ * Copyright (C) 2008 Mathieu Desnoyers
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/debugfs.h>
+#include <linux/fs.h>
+#include <linux/ltt-tracer.h>
+#include <linux/mutex.h>
+
+#define LTT_FILTER_DIR	"filter"
+
+/*
+ * Protects the ltt_filter_dir allocation.
+ */
+static DEFINE_MUTEX(ltt_filter_mutex);
+
+static struct dentry *ltt_filter_dir;
+
+struct dentry *get_filter_root(void)
+{
+	struct dentry *ltt_root_dentry;
+
+	mutex_lock(&ltt_filter_mutex);
+	if (!ltt_filter_dir) {
+		ltt_root_dentry = get_ltt_root();
+		if (!ltt_root_dentry)
+			goto err_no_root;
+
+		ltt_filter_dir = debugfs_create_dir(LTT_FILTER_DIR,
+						    ltt_root_dentry);
+		if (!ltt_filter_dir)
+			printk(KERN_ERR
+			       "ltt_filter_init: failed to create dir %s\n",
+			       LTT_FILTER_DIR);
+	}
+err_no_root:
+	mutex_unlock(&ltt_filter_mutex);
+	return ltt_filter_dir;
+}
+EXPORT_SYMBOL_GPL(get_filter_root);
+
+static void __exit ltt_filter_exit(void)
+{
+	debugfs_remove(ltt_filter_dir);
+}
+
+module_exit(ltt_filter_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>");
+MODULE_DESCRIPTION("Linux Trace Toolkit Filter");
diff --git a/stblinux-2.6.31/ltt/ltt-ftrace.c b/stblinux-2.6.31/ltt/ltt-ftrace.c
new file mode 100644
index 0000000..47c1b72
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-ftrace.c
@@ -0,0 +1,178 @@
+/*
+ * (C) Copyright	2008 -
+ * 		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng Ftrace integration module.
+ *
+ * Per-cpu "tap" (ftrace_cpu_start/ftrace_cpu_stop) will enable the tap for a
+ * given CPU on which the probes has been called.
+ *
+ * System-wide "tap" (ftrace_system_start/ftrace_system_stop) will enable
+ * tracing on every CPU as soon as a single CPU hits the start probe. It is left
+ * active until the same CPU hits the "stop" probe. Uses per-cpu boolean and
+ * global reference counting to make sure we detect when _at least_ one CPU is
+ * interested in opening the tap.
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/ftrace.h>
+#include <linux/ltt-tracer.h>
+#include <linux/marker.h>
+#include <asm/atomic.h>
+
+/* per-cpu function tracing activation */
+static DEFINE_PER_CPU(int, tracing_cpu);
+
+/* system-wide function tracing activation */
+static DEFINE_PER_CPU(int, system_tracing_cpu);
+static atomic_t system_trace_refcount __read_mostly;
+
+
+static notrace
+void ltt_tracer_call(unsigned long ip, unsigned long parent_ip)
+{
+	int cpu = raw_smp_processor_id();
+	if (likely(!per_cpu(tracing_cpu, cpu)
+			&& !atomic_read(&system_trace_refcount)))
+		return;
+	trace_mark(function_trace, entry, "ip 0x%lX parent_ip 0x%lX",
+		   ip, parent_ip);
+}
+
+static notrace
+void ltt_tap_marker(const struct marker *mdata, void *probe_data,
+		    void *call_data, const char *fmt, va_list *args)
+{
+	int cpu = raw_smp_processor_id();
+	if (likely(!per_cpu(tracing_cpu, cpu)
+	    && !atomic_read(&system_trace_refcount)))
+		return;
+	ltt_vtrace(mdata, probe_data, call_data, fmt, args);
+}
+
+struct ltt_available_probe ltt_tap_marker_probe = {
+	.name = "ltt_tap_marker",
+	.format = NULL,
+	.probe_func = ltt_tap_marker,
+};
+
+static struct ftrace_ops trace_ops __read_mostly =
+{
+	.func = ltt_tracer_call,
+};
+
+static notrace
+void ftrace_cpu_start(const struct marker *mdata, void *probe_data,
+		      void *call_data, const char *fmt, va_list *args)
+{
+	int cpu = raw_smp_processor_id();
+	per_cpu(tracing_cpu, cpu) = 1;
+}
+
+struct ltt_available_probe ftrace_cpu_start_probe = {
+	.name = "ftrace_cpu_start",
+	.format = NULL,
+	.probe_func = ftrace_cpu_start,
+};
+
+static notrace
+void ftrace_cpu_stop(const struct marker *mdata, void *probe_data,
+		     void *call_data, const char *fmt, va_list *args)
+{
+	int cpu = raw_smp_processor_id();
+	per_cpu(tracing_cpu, cpu) = 0;
+}
+
+struct ltt_available_probe ftrace_cpu_stop_probe = {
+	.name = "ftrace_cpu_stop",
+	.format = NULL,
+	.probe_func = ftrace_cpu_stop,
+};
+
+static notrace
+void ftrace_system_start(const struct marker *mdata, void *probe_data,
+			 void *call_data, const char *fmt, va_list *args)
+{
+	int cpu = raw_smp_processor_id();
+	int value = xchg(&per_cpu(system_tracing_cpu, cpu), 1);
+	if (!value)
+		atomic_inc(&system_trace_refcount);
+}
+
+struct ltt_available_probe ftrace_system_start_probe = {
+	.name = "ftrace_system_start",
+	.format = NULL,
+	.probe_func = ftrace_system_start,
+};
+
+static notrace
+void ftrace_system_stop(const struct marker *mdata, void *probe_data,
+			void *call_data, const char *fmt, va_list *args)
+{
+	int cpu = raw_smp_processor_id();
+	int value = xchg(&per_cpu(system_tracing_cpu, cpu), 0);
+	if (value)
+		atomic_dec(&system_trace_refcount);
+}
+
+struct ltt_available_probe ftrace_system_stop_probe = {
+	.name = "ftrace_system_stop",
+	.format = NULL,
+	.probe_func = ftrace_system_stop,
+};
+
+static int __init ltt_ftrace_init(void)
+{
+	int ret;
+
+	printk(KERN_INFO "LTT : ltt-ftrace init\n");
+	register_ftrace_function(&trace_ops);
+	ret = ltt_probe_register(&ftrace_cpu_start_probe);
+	BUG_ON(ret);
+	ret = ltt_probe_register(&ftrace_cpu_stop_probe);
+	BUG_ON(ret);
+	ret = ltt_probe_register(&ftrace_system_start_probe);
+	BUG_ON(ret);
+	ret = ltt_probe_register(&ftrace_system_stop_probe);
+	BUG_ON(ret);
+	ret = ltt_probe_register(&ltt_tap_marker_probe);
+	BUG_ON(ret);
+
+	/*
+	 * Keep a refcount on ourselves, because ftrace forbids freeing
+	 * trace_ops.
+	 */
+	/* __module_get(THIS_MODULE); */
+
+	return 0;
+}
+module_init(ltt_ftrace_init);
+
+#if 0
+/* create file operation to activate/deactivate these probes.
+ */
+static void __exit ltt_ftrace_exit(void)
+{
+	int ret;
+
+	printk(KERN_INFO "LTT : ltt-ftrace exit\n");
+	ret = ltt_probe_unregister(&ltt_tap_marker_probe);
+	BUG_ON(ret);
+	ret = ltt_probe_unregister(&ftrace_system_stop_probe);
+	BUG_ON(ret);
+	ret = ltt_probe_unregister(&ftrace_system_start_probe);
+	BUG_ON(ret);
+	ret = ltt_probe_unregister(&ftrace_cpu_stop_probe);
+	BUG_ON(ret);
+	ret = ltt_probe_unregister(&ftrace_cpu_start_probe);
+	BUG_ON(ret);
+	unregister_ftrace_function(&trace_ops);
+}
+module_exit(ltt_ftrace_exit);
+#endif //0
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Function Tracer Support");
diff --git a/stblinux-2.6.31/ltt/ltt-kprobes.c b/stblinux-2.6.31/ltt/ltt-kprobes.c
new file mode 100644
index 0000000..586c212
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-kprobes.c
@@ -0,0 +1,492 @@
+/*
+ * (C) Copyright	2009 -
+ * 		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng kprobes integration module.
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/kprobes.h>
+#include <linux/ltt-tracer.h>
+#include <linux/marker.h>
+#include <linux/mutex.h>
+#include <linux/jhash.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/debugfs.h>
+#include <linux/kallsyms.h>
+#include <linux/ltt-type-serializer.h>
+
+#define LTT_KPROBES_DIR 	"kprobes"
+#define LTT_KPROBES_ENABLE	"enable"
+#define LTT_KPROBES_DISABLE	"disable"
+#define LTT_KPROBES_LIST	"list"
+
+/* Active LTTng kprobes hash table */
+static DEFINE_MUTEX(ltt_kprobes_mutex);
+
+#define LTT_KPROBE_HASH_BITS	6
+#define LTT_KPROBE_TABLE_SIZE	(1 << LTT_KPROBE_HASH_BITS)
+static struct hlist_head ltt_kprobe_table[LTT_KPROBE_TABLE_SIZE];
+
+struct kprobe_entry {
+	struct hlist_node hlist;
+	struct kprobe kp;
+	char key[0];
+};
+
+static struct dentry *ltt_kprobes_dir,
+		     *ltt_kprobes_enable_dentry,
+		     *ltt_kprobes_disable_dentry,
+		     *ltt_kprobes_list_dentry;
+
+static int module_exit;
+
+
+static void trace_kprobe_table_entry(void *call_data, struct kprobe_entry *e)
+{
+	unsigned long addr;
+	char *namebuf = (char *)__get_free_page(GFP_KERNEL);
+
+	if (e->kp.addr) {
+		sprint_symbol(namebuf, (unsigned long)e->kp.addr);
+		addr = (unsigned long)e->kp.addr;
+	} else {
+		strncpy(namebuf, e->kp.symbol_name, PAGE_SIZE - 1);
+		/* TODO : add offset */
+		addr = kallsyms_lookup_name(namebuf);
+	}
+	if (addr)
+		__trace_mark(0, kprobe_state, kprobe_table, call_data,
+			     "ip 0x%lX symbol %s", addr, namebuf);
+	free_page((unsigned long)namebuf);
+}
+
+DEFINE_MARKER(kernel, kprobe, "ip %lX");
+
+static int ltt_kprobe_handler_pre(struct kprobe *p, struct pt_regs *regs)
+{
+	struct marker *marker;
+	unsigned long data;
+
+	data = (unsigned long)p->addr;
+	marker = &GET_MARKER(kernel, kprobe);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+			      &data, sizeof(data), sizeof(data));
+	return 0;
+}
+
+static int ltt_register_kprobe(const char *key)
+{
+	struct hlist_head *head;
+	struct hlist_node *node;
+	struct kprobe_entry *e = NULL;
+	char *symbol_name = NULL;
+	unsigned long addr;
+	unsigned int offset = 0;
+	u32 hash;
+	size_t key_len = strlen(key) + 1;
+	int ret;
+
+	if (key_len == 1)
+		return -ENOENT;	/* only \0 */
+
+	if (sscanf(key, "%li", &addr) != 1)
+		addr = 0;
+
+	if (!addr) {
+		const char *symbol_end = NULL;
+		unsigned int symbol_len;	/* includes final \0 */
+
+		symbol_end = strchr(key, ' ');
+		if (symbol_end)
+			symbol_len = symbol_end - key + 1;
+		else
+			symbol_len = key_len;
+		symbol_name = kmalloc(symbol_len, GFP_KERNEL);
+		if (!symbol_name) {
+			ret = -ENOMEM;
+			goto error;
+		}
+		memcpy(symbol_name, key, symbol_len - 1);
+		symbol_name[symbol_len-1] = '\0';
+		if (symbol_end) {
+			symbol_end++;	/* start of offset */
+			if (sscanf(symbol_end, "%i", &offset) != 1)
+				offset = 0;
+		}
+	}
+
+	hash = jhash(key, key_len-1, 0);
+	head = &ltt_kprobe_table[hash & ((1 << LTT_KPROBE_HASH_BITS)-1)];
+	hlist_for_each_entry(e, node, head, hlist) {
+		if (!strcmp(key, e->key)) {
+			printk(KERN_NOTICE "Kprobe %s busy\n", key);
+			ret = -EBUSY;
+			goto error;
+		}
+	}
+	/*
+	 * Using kzalloc here to allocate a variable length element. Could
+	 * cause some memory fragmentation if overused.
+	 */
+	e = kzalloc(sizeof(struct kprobe_entry) + key_len, GFP_KERNEL);
+	if (!e) {
+		ret = -ENOMEM;
+		goto error;
+	}
+	memcpy(e->key, key, key_len);
+	hlist_add_head(&e->hlist, head);
+	e->kp.pre_handler = ltt_kprobe_handler_pre;
+	e->kp.symbol_name = symbol_name;
+	e->kp.offset = offset;
+	e->kp.addr = (void *)addr;
+	ret = register_kprobe(&e->kp);
+	if (ret < 0)
+		goto error_list_del;
+	trace_kprobe_table_entry(NULL, e);
+	return 0;
+
+error_list_del:
+	hlist_del(&e->hlist);
+error:
+	kfree(symbol_name);
+	kfree(e);
+	return ret;
+}
+
+static int ltt_unregister_kprobe(const char *key)
+{
+	struct hlist_head *head;
+	struct hlist_node *node;
+	struct kprobe_entry *e;
+	int found = 0;
+	size_t key_len = strlen(key) + 1;
+	u32 hash;
+
+	hash = jhash(key, key_len-1, 0);
+	head = &ltt_kprobe_table[hash & ((1 << LTT_KPROBE_HASH_BITS)-1)];
+	hlist_for_each_entry(e, node, head, hlist) {
+		if (!strcmp(key, e->key)) {
+			found = 1;
+			break;
+		}
+	}
+	if (!found)
+		return -ENOENT;
+	hlist_del(&e->hlist);
+	unregister_kprobe(&e->kp);
+	kfree(e->kp.symbol_name);
+	kfree(e);
+	return 0;
+}
+
+static void ltt_unregister_all_kprobes(void)
+{
+	struct kprobe_entry *e;
+	struct hlist_head *head;
+	struct hlist_node *node, *tmp;
+	unsigned int i;
+
+	for (i = 0; i < LTT_KPROBE_TABLE_SIZE; i++) {
+		head = &ltt_kprobe_table[i];
+		hlist_for_each_entry_safe(e, node, tmp, head, hlist) {
+			hlist_del(&e->hlist);
+			unregister_kprobe(&e->kp);
+			kfree(e->kp.symbol_name);
+			kfree(e);
+		}
+	}
+}
+
+/*
+ * Allows to specify either
+ * - symbol
+ * - symbol offset
+ * - address
+ */
+static ssize_t enable_op_write(struct file *file,
+	const char __user *user_buf, size_t count, loff_t *ppos)
+{
+	int err, buf_size;
+	char *end;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+
+	mutex_lock(&ltt_kprobes_mutex);
+	if (module_exit) {
+		err = -EPERM;
+		goto error;
+	}
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto error;
+	buf[buf_size] = '\0';
+	end = strchr(buf, '\n');
+	if (end)
+		*end = '\0';
+	err = ltt_register_kprobe(buf);
+	if (err)
+		goto error;
+
+	mutex_unlock(&ltt_kprobes_mutex);
+	free_page((unsigned long)buf);
+	return count;
+error:
+	mutex_unlock(&ltt_kprobes_mutex);
+	free_page((unsigned long)buf);
+	return err;
+}
+
+static const struct file_operations ltt_kprobes_enable = {
+	.write = enable_op_write,
+};
+
+static ssize_t disable_op_write(struct file *file,
+	const char __user *user_buf, size_t count, loff_t *ppos)
+{
+	int err, buf_size;
+	char *end;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+
+	mutex_lock(&ltt_kprobes_mutex);
+	if (module_exit)
+		goto end;
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto error;
+	buf[buf_size] = '\0';
+	end = strchr(buf, '\n');
+	if (end)
+		*end = '\0';
+	err = ltt_unregister_kprobe(buf);
+	if (err)
+		goto error;
+end:
+	mutex_unlock(&ltt_kprobes_mutex);
+	free_page((unsigned long)buf);
+	return count;
+error:
+	mutex_unlock(&ltt_kprobes_mutex);
+	free_page((unsigned long)buf);
+	return err;
+}
+
+static const struct file_operations ltt_kprobes_disable = {
+	.write = disable_op_write,
+};
+
+/*
+ * This seqfile read is not perfectly safe, as a kprobe could be removed from
+ * the hash table between two reads. This will result in an incomplete output.
+ */
+static struct kprobe_entry *ltt_find_next_kprobe(struct kprobe_entry *prev)
+{
+	struct kprobe_entry *e;
+	struct hlist_head *head;
+	struct hlist_node *node;
+	unsigned int i;
+	int found = 0;
+
+	if (prev == (void *)-1UL)
+		return NULL;
+
+	if (!prev)
+		found = 1;
+
+	for (i = 0; i < LTT_KPROBE_TABLE_SIZE; i++) {
+		head = &ltt_kprobe_table[i];
+		hlist_for_each_entry(e, node, head, hlist) {
+			if (found)
+				return e;
+			if (e == prev)
+				found = 1;
+		}
+	}
+	return NULL;
+}
+
+static void *lk_next(struct seq_file *m, void *p, loff_t *pos)
+{
+	m->private = ltt_find_next_kprobe(m->private);
+	if (!m->private) {
+		m->private = (void *)-1UL;
+		return NULL;
+	}
+	return m->private;
+}
+
+static void *lk_start(struct seq_file *m, loff_t *pos)
+{
+	mutex_lock(&ltt_kprobes_mutex);
+	if (!*pos)
+		m->private = NULL;
+	m->private = ltt_find_next_kprobe(m->private);
+	if (!m->private) {
+		m->private = (void *)-1UL;
+		return NULL;
+	}
+	return m->private;
+}
+
+static void lk_stop(struct seq_file *m, void *p)
+{
+	mutex_unlock(&ltt_kprobes_mutex);
+}
+
+static int lk_show(struct seq_file *m, void *p)
+{
+	struct kprobe_entry *e = m->private;
+	seq_printf(m, "%s\n", e->key);
+	return 0;
+}
+
+static const struct seq_operations ltt_kprobes_list_op = {
+	.start = lk_start,
+	.next = lk_next,
+	.stop = lk_stop,
+	.show = lk_show,
+};
+
+static int ltt_kprobes_list_open(struct inode *inode, struct file *file)
+{
+	int ret;
+
+	ret = seq_open(file, &ltt_kprobes_list_op);
+	if (ret == 0)
+		((struct seq_file *)file->private_data)->private = NULL;
+	return ret;
+}
+
+static int ltt_kprobes_list_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *seq = file->private_data;
+
+	seq->private = NULL;
+	return seq_release(inode, file);
+}
+
+static const struct file_operations ltt_kprobes_list = {
+	.open = ltt_kprobes_list_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = ltt_kprobes_list_release,
+};
+
+/*
+ * kprobes table dump. Callback invoked by ltt-statedump. ltt-statedump must
+ * take a reference to this module before calling this callback.
+ */
+void ltt_dump_kprobes_table(void *call_data)
+{
+	struct kprobe_entry *e;
+	struct hlist_head *head;
+	struct hlist_node *node;
+	unsigned int i;
+
+	for (i = 0; i < LTT_KPROBE_TABLE_SIZE; i++) {
+		head = &ltt_kprobe_table[i];
+		hlist_for_each_entry(e, node, head, hlist)
+			trace_kprobe_table_entry(call_data, e);
+	}
+}
+EXPORT_SYMBOL_GPL(ltt_dump_kprobes_table);
+
+static int __init ltt_kprobes_init(void)
+{
+	struct dentry *ltt_root_dentry;
+	int ret = 0;
+
+	printk(KERN_INFO "LTT : ltt-kprobes init\n");
+	mutex_lock(&ltt_kprobes_mutex);
+
+	ltt_root_dentry = get_ltt_root();
+	if (!ltt_root_dentry) {
+		ret = -ENOENT;
+		goto err_no_root;
+	}
+
+	ltt_kprobes_dir = debugfs_create_dir(LTT_KPROBES_DIR, ltt_root_dentry);
+	if (!ltt_kprobes_dir) {
+		printk(KERN_ERR
+		       "ltt_kprobes_init: failed to create dir %s\n",
+			LTT_KPROBES_DIR);
+		ret = -ENOMEM;
+		goto err_no_dir;
+	}
+
+	ltt_kprobes_enable_dentry = debugfs_create_file(LTT_KPROBES_ENABLE,
+							S_IWUSR,
+							ltt_kprobes_dir, NULL,
+							&ltt_kprobes_enable);
+	if (IS_ERR(ltt_kprobes_enable_dentry) || !ltt_kprobes_enable_dentry) {
+		printk(KERN_ERR
+		       "ltt_kprobes_init: failed to create file %s\n",
+			LTT_KPROBES_ENABLE);
+		ret = -ENOMEM;
+		goto err_no_enable;
+	}
+
+	ltt_kprobes_disable_dentry = debugfs_create_file(LTT_KPROBES_DISABLE,
+							 S_IWUSR,
+							 ltt_kprobes_dir, NULL,
+							 &ltt_kprobes_disable);
+	if (IS_ERR(ltt_kprobes_disable_dentry) || !ltt_kprobes_disable_dentry) {
+		printk(KERN_ERR
+		       "ltt_kprobes_init: failed to create file %s\n",
+			LTT_KPROBES_DISABLE);
+		ret = -ENOMEM;
+		goto err_no_disable;
+	}
+
+	ltt_kprobes_list_dentry = debugfs_create_file(LTT_KPROBES_LIST,
+						      S_IWUSR, ltt_kprobes_dir,
+						      NULL, &ltt_kprobes_list);
+	if (IS_ERR(ltt_kprobes_list_dentry) || !ltt_kprobes_list_dentry) {
+		printk(KERN_ERR
+		       "ltt_kprobes_init: failed to create file %s\n",
+			LTT_KPROBES_LIST);
+		ret = -ENOMEM;
+		goto err_no_list;
+	}
+	ltt_statedump_register_kprobes_dump(ltt_dump_kprobes_table);
+
+	mutex_unlock(&ltt_kprobes_mutex);
+	return ret;
+
+err_no_list:
+	debugfs_remove(ltt_kprobes_disable_dentry);
+err_no_disable:
+	debugfs_remove(ltt_kprobes_enable_dentry);
+err_no_enable:
+	debugfs_remove(ltt_kprobes_dir);
+err_no_dir:
+err_no_root:
+	mutex_unlock(&ltt_kprobes_mutex);
+	return ret;
+}
+module_init(ltt_kprobes_init);
+
+static void __exit ltt_kprobes_exit(void)
+{
+	printk(KERN_INFO "LTT : ltt-kprobes exit\n");
+	mutex_lock(&ltt_kprobes_mutex);
+	module_exit = 1;
+	ltt_statedump_unregister_kprobes_dump(ltt_dump_kprobes_table);
+	debugfs_remove(ltt_kprobes_list_dentry);
+	debugfs_remove(ltt_kprobes_disable_dentry);
+	debugfs_remove(ltt_kprobes_enable_dentry);
+	debugfs_remove(ltt_kprobes_dir);
+	ltt_unregister_all_kprobes();
+	mutex_unlock(&ltt_kprobes_mutex);
+}
+module_exit(ltt_kprobes_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Kprobes Support");
diff --git a/stblinux-2.6.31/ltt/ltt-marker-control.c b/stblinux-2.6.31/ltt/ltt-marker-control.c
new file mode 100644
index 0000000..e8a0a17
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-marker-control.c
@@ -0,0 +1,253 @@
+/*
+ * Copyright (C) 2007 Mathieu Desnoyers
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/vmalloc.h>
+#include <linux/marker.h>
+#include <linux/ltt-tracer.h>
+#include <linux/uaccess.h>
+#include <linux/string.h>
+#include <linux/ctype.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+
+#define DEFAULT_CHANNEL "cpu"
+#define DEFAULT_PROBE "default"
+
+LIST_HEAD(probes_list);
+
+/*
+ * Mutex protecting the probe slab cache.
+ * Nests inside the traces mutex.
+ */
+DEFINE_MUTEX(probes_mutex);
+
+struct ltt_available_probe default_probe = {
+	.name = "default",
+	.format = NULL,
+	.probe_func = ltt_vtrace,
+	.callbacks[0] = ltt_serialize_data,
+};
+
+static struct kmem_cache *markers_loaded_cachep;
+static LIST_HEAD(markers_loaded_list);
+/*
+ * List sorted by name strcmp order.
+ */
+static LIST_HEAD(probes_registered_list);
+
+static struct ltt_available_probe *get_probe_from_name(const char *pname)
+{
+	struct ltt_available_probe *iter;
+	int comparison, found = 0;
+
+	if (!pname)
+		pname = DEFAULT_PROBE;
+	list_for_each_entry(iter, &probes_registered_list, node) {
+		comparison = strcmp(pname, iter->name);
+		if (!comparison)
+			found = 1;
+		if (comparison <= 0)
+			break;
+	}
+	if (found)
+		return iter;
+	else
+		return NULL;
+}
+
+int ltt_probe_register(struct ltt_available_probe *pdata)
+{
+	int ret = 0;
+	int comparison;
+	struct ltt_available_probe *iter;
+
+	mutex_lock(&probes_mutex);
+	list_for_each_entry_reverse(iter, &probes_registered_list, node) {
+		comparison = strcmp(pdata->name, iter->name);
+		if (!comparison) {
+			ret = -EBUSY;
+			goto end;
+		} else if (comparison > 0) {
+			/* We belong to the location right after iter. */
+			list_add(&pdata->node, &iter->node);
+			goto end;
+		}
+	}
+	/* Should be added at the head of the list */
+	list_add(&pdata->node, &probes_registered_list);
+end:
+	mutex_unlock(&probes_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_probe_register);
+
+/*
+ * Called when a probe does not want to be called anymore.
+ */
+int ltt_probe_unregister(struct ltt_available_probe *pdata)
+{
+	int ret = 0;
+	struct ltt_active_marker *amark, *tmp;
+
+	mutex_lock(&probes_mutex);
+	list_for_each_entry_safe(amark, tmp, &markers_loaded_list, node) {
+		if (amark->probe == pdata) {
+			ret = marker_probe_unregister_private_data(
+				pdata->probe_func, amark);
+			if (ret)
+				goto end;
+			list_del(&amark->node);
+			kmem_cache_free(markers_loaded_cachep, amark);
+		}
+	}
+	list_del(&pdata->node);
+end:
+	mutex_unlock(&probes_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_probe_unregister);
+
+/*
+ * Connect marker "mname" to probe "pname".
+ * Only allow _only_ probe instance to be connected to a marker.
+ */
+int ltt_marker_connect(const char *channel, const char *mname,
+		       const char *pname)
+
+{
+	int ret;
+	struct ltt_active_marker *pdata;
+	struct ltt_available_probe *probe;
+
+	ltt_lock_traces();
+	mutex_lock(&probes_mutex);
+	probe = get_probe_from_name(pname);
+	if (!probe) {
+		ret = -ENOENT;
+		goto end;
+	}
+	pdata = marker_get_private_data(channel, mname, probe->probe_func, 0);
+	if (pdata && !IS_ERR(pdata)) {
+		ret = -EEXIST;
+		goto end;
+	}
+	pdata = kmem_cache_zalloc(markers_loaded_cachep, GFP_KERNEL);
+	if (!pdata) {
+		ret = -ENOMEM;
+		goto end;
+	}
+	pdata->probe = probe;
+	/*
+	 * ID has priority over channel in case of conflict.
+	 */
+	ret = marker_probe_register(channel, mname, NULL,
+		probe->probe_func, pdata);
+	if (ret)
+		kmem_cache_free(markers_loaded_cachep, pdata);
+	else
+		list_add(&pdata->node, &markers_loaded_list);
+end:
+	mutex_unlock(&probes_mutex);
+	ltt_unlock_traces();
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_marker_connect);
+
+/*
+ * Disconnect marker "mname", probe "pname".
+ */
+int ltt_marker_disconnect(const char *channel, const char *mname,
+			  const char *pname)
+{
+	struct ltt_active_marker *pdata;
+	struct ltt_available_probe *probe;
+	int ret = 0;
+
+	mutex_lock(&probes_mutex);
+	probe = get_probe_from_name(pname);
+	if (!probe) {
+		ret = -ENOENT;
+		goto end;
+	}
+	pdata = marker_get_private_data(channel, mname, probe->probe_func, 0);
+	if (IS_ERR(pdata)) {
+		ret = PTR_ERR(pdata);
+		goto end;
+	} else if (!pdata) {
+		/*
+		 * Not registered by us.
+		 */
+		ret = -EPERM;
+		goto end;
+	}
+	ret = marker_probe_unregister(channel, mname, probe->probe_func, pdata);
+	if (ret)
+		goto end;
+	else {
+		list_del(&pdata->node);
+		kmem_cache_free(markers_loaded_cachep, pdata);
+	}
+end:
+	mutex_unlock(&probes_mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_marker_disconnect);
+
+static void disconnect_all_markers(void)
+{
+	struct ltt_active_marker *pdata, *tmp;
+
+	list_for_each_entry_safe(pdata, tmp, &markers_loaded_list, node) {
+		marker_probe_unregister_private_data(pdata->probe->probe_func,
+			pdata);
+		list_del(&pdata->node);
+		kmem_cache_free(markers_loaded_cachep, pdata);
+	}
+}
+
+static int __init marker_control_init(void)
+{
+	int ret;
+
+	markers_loaded_cachep = KMEM_CACHE(ltt_active_marker, 0);
+
+	ret = ltt_probe_register(&default_probe);
+	BUG_ON(ret);
+	ret = ltt_marker_connect("metadata", "core_marker_format",
+				 DEFAULT_PROBE);
+	BUG_ON(ret);
+	ret = ltt_marker_connect("metadata", "core_marker_id", DEFAULT_PROBE);
+	BUG_ON(ret);
+
+	return 0;
+}
+module_init(marker_control_init);
+
+static void __exit marker_control_exit(void)
+{
+	int ret;
+
+	ret = ltt_marker_disconnect("metadata", "core_marker_format",
+				    DEFAULT_PROBE);
+	BUG_ON(ret);
+	ret = ltt_marker_disconnect("metadata", "core_marker_id",
+				    DEFAULT_PROBE);
+	BUG_ON(ret);
+	ret = ltt_probe_unregister(&default_probe);
+	BUG_ON(ret);
+	disconnect_all_markers();
+	kmem_cache_destroy(markers_loaded_cachep);
+	marker_synchronize_unregister();
+}
+module_exit(marker_control_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Marker Control");
diff --git a/stblinux-2.6.31/ltt/ltt-relay-alloc.c b/stblinux-2.6.31/ltt/ltt-relay-alloc.c
new file mode 100644
index 0000000..dfef18b
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-relay-alloc.c
@@ -0,0 +1,574 @@
+/*
+ * ltt-relay-alloc.c
+ *
+ * Copyright (C) 2008,2009 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/errno.h>
+#include <linux/stddef.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/ltt-relay.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+#include <linux/cpu.h>
+#include <linux/bitops.h>
+#include <linux/ltt-tracer.h>
+
+#include "ltt-relay-select.h"	/* for cpu hotplug */
+
+/**
+ * ltt_chanbuf_allocate - allocate a channel buffer
+ * @buf: the buffer struct
+ * @size: total size of the buffer
+ * @n_sb: number of subbuffers
+ * @extra_reader_sb: need extra subbuffer for reader
+ */
+static
+int ltt_chanbuf_allocate(struct ltt_chanbuf_alloc *buf, size_t size,
+			 size_t n_sb, int extra_reader_sb)
+{
+	long i, j, n_pages, n_pages_per_sb, page_idx = 0;
+	struct page **pages;
+	void **virt;
+
+	n_pages = size >> PAGE_SHIFT;
+	n_pages_per_sb = n_pages >> get_count_order(n_sb);
+	if (extra_reader_sb)
+		n_pages += n_pages_per_sb;	/* Add pages for reader */
+
+	pages = kmalloc_node(max_t(size_t, sizeof(*pages) * n_pages,
+				   1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(buf->cpu));
+	if (unlikely(!pages))
+		goto pages_error;
+
+	virt = kmalloc_node(ALIGN(sizeof(*virt) * n_pages,
+				  1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(buf->cpu));
+	if (unlikely(!virt))
+		goto virt_error;
+
+	for (i = 0; i < n_pages; i++) {
+		pages[i] = alloc_pages_node(cpu_to_node(buf->cpu),
+			GFP_KERNEL | __GFP_ZERO, 0);
+		if (unlikely(!pages[i]))
+			goto depopulate;
+		virt[i] = page_address(pages[i]);
+	}
+	buf->nr_pages = n_pages;
+	buf->_pages = pages;
+	buf->_virt = virt;
+
+	/* Allocate write-side page index */
+	buf->buf_wsb = kzalloc_node(max_t(size_t,
+				sizeof(struct chanbuf_sb) * n_sb,
+				1 << INTERNODE_CACHE_SHIFT),
+				GFP_KERNEL, cpu_to_node(buf->cpu));
+	if (unlikely(!buf->buf_wsb))
+		goto depopulate;
+
+	for (i = 0; i < n_sb; i++) {
+		buf->buf_wsb[i].pages =
+			kzalloc_node(max_t(size_t,
+				sizeof(struct chanbuf_page) * n_pages_per_sb,
+				1 << INTERNODE_CACHE_SHIFT),
+				GFP_KERNEL, cpu_to_node(buf->cpu));
+		if (!buf->buf_wsb[i].pages)
+			goto free_buf_wsb;
+	}
+
+	if (extra_reader_sb) {
+		/* Allocate read-side page index */
+		buf->buf_rsb.pages =
+			kzalloc_node(max_t(size_t,
+				sizeof(struct chanbuf_page) * n_pages_per_sb,
+				1 << INTERNODE_CACHE_SHIFT),
+				GFP_KERNEL, cpu_to_node(buf->cpu));
+		if (unlikely(!buf->buf_rsb.pages))
+			goto free_buf_wsb;
+	} else {
+		buf->buf_rsb.pages = buf->buf_wsb[0].pages;
+	}
+
+	/* Assign pages to write-side page index */
+	for (i = 0; i < n_sb; i++) {
+		for (j = 0; j < n_pages_per_sb; j++) {
+			WARN_ON(page_idx > n_pages);
+			buf->buf_wsb[i].pages[j].virt = virt[page_idx];
+			buf->buf_wsb[i].pages[j].page = pages[page_idx];
+			page_idx++;
+		}
+		RCHAN_SB_SET_NOREF(buf->buf_wsb[i].pages);
+	}
+
+	if (extra_reader_sb) {
+		for (j = 0; j < n_pages_per_sb; j++) {
+			WARN_ON(page_idx > n_pages);
+			buf->buf_rsb.pages[j].virt = virt[page_idx];
+			buf->buf_rsb.pages[j].page = pages[page_idx];
+			page_idx++;
+		}
+		RCHAN_SB_SET_NOREF(buf->buf_rsb.pages);
+	}
+
+	/*
+	 * If kmalloc ever uses vmalloc underneath, make sure the buffer pages
+	 * will not fault.
+	 */
+	vmalloc_sync_all();
+	return 0;
+
+free_buf_wsb:
+	for (i = 0; i < n_sb; i++) {
+		RCHAN_SB_CLEAR_NOREF(buf->buf_wsb[i].pages);
+		kfree(buf->buf_wsb[i].pages);
+	}
+	kfree(buf->buf_wsb);
+depopulate:
+	/*
+	 * Free all pages from [ i - 1 down to 0 ].
+	 * If i = 0, don't free anything.
+	 */
+	for (i--; i >= 0; i--)
+		__free_page(pages[i]);
+	kfree(virt);
+virt_error:
+	kfree(pages);
+pages_error:
+	return -ENOMEM;
+}
+
+int ltt_chanbuf_alloc_create(struct ltt_chanbuf_alloc *buf,
+			     struct ltt_chan_alloc *chan, int cpu)
+{
+	int ret = 0;
+
+	ret = ltt_chanbuf_allocate(buf, chan->buf_size, chan->n_sb,
+				   chan->extra_reader_sb);
+	if (ret)
+		goto end;
+
+	buf->chan = chan;
+	buf->cpu = cpu;
+end:
+	return ret;
+}
+
+void ltt_chanbuf_alloc_free(struct ltt_chanbuf_alloc *buf)
+{
+	struct ltt_chan_alloc *chan = buf->chan;
+	struct page **pages;
+	long i;
+
+	/* Destroy index */
+	if (chan->extra_reader_sb) {
+		RCHAN_SB_CLEAR_NOREF(buf->buf_rsb.pages);
+		kfree(buf->buf_rsb.pages);
+	}
+	for (i = 0; i < chan->n_sb; i++) {
+		RCHAN_SB_CLEAR_NOREF(buf->buf_wsb[i].pages);
+		kfree(buf->buf_wsb[i].pages);
+	}
+	kfree(buf->buf_wsb);
+
+	/* Destroy pages */
+	pages = buf->_pages;
+	for (i = 0; i < buf->nr_pages; i++)
+		__free_page(pages[i]);
+	kfree(buf->_pages);
+	kfree(buf->_virt);
+	buf->allocated = 0;
+}
+
+/**
+ *	ltt_relay_hotcpu_callback - CPU hotplug callback
+ *	@nb: notifier block
+ *	@action: hotplug action to take
+ *	@hcpu: CPU number
+ *
+ *	Returns the success/failure of the operation. (%NOTIFY_OK, %NOTIFY_BAD)
+ */
+static
+int __cpuinit ltt_relay_hotcpu_callback(struct notifier_block *nb,
+					unsigned long action,
+					void *hcpu)
+{
+	unsigned int cpu = (unsigned long)hcpu;
+	struct ltt_trace *trace;
+	struct ltt_chan *chan;
+	struct ltt_chanbuf *buf;
+	int ret, i;
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+	case CPU_UP_PREPARE_FROZEN:
+		/*
+		 * CPU hotplug lock protects trace lock from this callback.
+		 */
+		__list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+			for (i = 0; i < trace->nr_channels; i++) {
+				chan = &trace->channels[i];
+				buf = per_cpu_ptr(chan->a.buf, cpu);
+				ret = ltt_chanbuf_create(buf, &chan->a, cpu);
+				if (ret) {
+					printk(KERN_ERR
+					  "ltt_relay_hotcpu_callback: cpu %d "
+					  "buffer creation failed\n", cpu);
+					return NOTIFY_BAD;
+				}
+
+			}
+		}
+		break;
+	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
+		/* No need to do a buffer switch here, because it will happen
+		 * when tracing is stopped, or will be done by switch timer CPU
+		 * DEAD callback. */
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+/*
+ * Must be called with either trace lock or rcu read lock sched held.
+ */
+void ltt_chan_for_each_channel(void (*cb) (struct ltt_chanbuf *buf), int cpu)
+{
+	struct ltt_trace *trace;
+	struct ltt_chan *chan;
+	struct ltt_chanbuf *buf;
+	int i;
+
+	__list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		for (i = 0; i < trace->nr_channels; i++) {
+			chan = &trace->channels[i];
+			if (!chan->active)
+				continue;
+			buf = per_cpu_ptr(chan->a.buf, cpu);
+			cb(buf);
+		}
+	}
+}
+
+/**
+ * ltt_chan_create - create a new relay channel
+ * @chan: channel
+ * @trace: trace
+ * @base_filename: base name of files to create
+ * @parent: dentry of parent directory, %NULL for root directory
+ * @sb_size: size of sub-buffers (> PAGE_SIZE, power of 2)
+ * @n_sb: number of sub-buffers (power of 2)
+ * @extra_reader_sb: allocate an extra subbuffer for the reader
+ * @overwrite: channel is in overwrite mode
+ *
+ * Returns channel pointer if successful, %NULL otherwise.
+ *
+ * Creates per-cpu channel buffers using the sizes and attributes
+ * specified.  The created channel buffer files will be named
+ * base_filename_0...base_filename_N-1.  File permissions will
+ * be %S_IRUSR.
+ */
+int ltt_chan_alloc_init(struct ltt_chan_alloc *chan, struct ltt_trace *trace,
+			const char *base_filename,
+			struct dentry *parent, size_t sb_size,
+			size_t n_sb, int extra_reader_sb, int overwrite)
+{
+	unsigned int i;
+	int ret;
+
+	if (!base_filename)
+		return -EPERM;
+
+	if (!(sb_size && n_sb))
+		return -EPERM;
+
+	/* Check that the subbuffer size is larger than a page. */
+	WARN_ON_ONCE(sb_size < PAGE_SIZE);
+
+	/*
+	 * Make sure the number of subbuffers and subbuffer size are power of 2.
+	 */
+	WARN_ON_ONCE(hweight32(sb_size) != 1);
+	WARN_ON(hweight32(n_sb) != 1);
+
+	chan->trace = trace;
+	chan->buf_size = n_sb * sb_size;
+	chan->sb_size = sb_size;
+	chan->sb_size_order = get_count_order(sb_size);
+	chan->n_sb_order = get_count_order(n_sb);
+	chan->extra_reader_sb = extra_reader_sb;
+	chan->n_sb = n_sb;
+	chan->parent = parent;
+	strlcpy(chan->filename, base_filename, NAME_MAX);
+	kref_init(&chan->kref);
+	kref_get(&chan->trace->kref);
+
+	/* Allocating the child structure */
+	chan->buf = alloc_percpu(struct ltt_chanbuf);
+	if (!chan->buf)
+		goto free_chan;
+
+	for_each_online_cpu(i) {
+		ret = ltt_chanbuf_create(per_cpu_ptr(chan->buf, i), chan, i);
+		if (ret)
+			goto free_bufs;
+	}
+
+	return 0;
+
+free_bufs:
+	for_each_possible_cpu(i) {
+		struct ltt_chanbuf *buf = per_cpu_ptr(chan->buf, i);
+
+		if (!buf->a.allocated)
+			continue;
+		ltt_chanbuf_remove_file(buf);
+		ltt_chanbuf_free(buf);
+	}
+	free_percpu(chan->buf);
+free_chan:
+	kref_put(&chan->kref, ltt_chan_free);
+	return -ENOMEM;
+}
+
+/**
+ * ltt_chan_alloc_free - destroy the channel
+ * @chan: the channel
+ *
+ * Destroy all channel buffers and frees the channel.
+ */
+void ltt_chan_alloc_free(struct ltt_chan_alloc *chan)
+{
+	unsigned int i;
+
+	for_each_possible_cpu(i) {
+		struct ltt_chanbuf *buf = per_cpu_ptr(chan->buf, i);
+
+		if (!buf->a.allocated)
+			continue;
+		ltt_chanbuf_remove_file(buf);
+		ltt_chanbuf_free(buf);
+	}
+	free_percpu(chan->buf);
+	kref_put(&chan->trace->kref, ltt_release_trace);
+	wake_up_interruptible(&chan->trace->kref_wq);
+}
+
+/**
+ * ltt_relay_write - write data to a ltt_relay buffer.
+ * @bufa : buffer
+ * @offset : offset within the buffer
+ * @src : source address
+ * @len : length to write
+ * @page : cached buffer page
+ * @pagecpy : page size copied so far
+ */
+void _ltt_relay_write(struct ltt_chanbuf_alloc *bufa, size_t offset,
+		      const void *src, size_t len, ssize_t pagecpy)
+{
+	struct ltt_chan_alloc *chana = bufa->chan;
+	size_t sbidx, index;
+	struct chanbuf_page *rpages;
+
+	do {
+		len -= pagecpy;
+		src += pagecpy;
+		offset += pagecpy;
+		sbidx = offset >> chana->sb_size_order;
+		index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+
+		/*
+		 * Underlying layer should never ask for writes across
+		 * subbuffers.
+		 */
+		WARN_ON(offset >= chana->buf_size);
+
+		pagecpy = min_t(size_t, len, PAGE_SIZE - (offset & ~PAGE_MASK));
+		rpages = bufa->buf_wsb[sbidx].pages;
+		WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
+		ltt_relay_do_copy(rpages[index].virt + (offset & ~PAGE_MASK),
+				  src, pagecpy);
+	} while (unlikely(len != pagecpy));
+}
+EXPORT_SYMBOL_GPL(_ltt_relay_write);
+
+/**
+ * ltt_relay_read - read data from ltt_relay_buffer.
+ * @bufa : buffer
+ * @offset : offset within the buffer
+ * @dest : destination address
+ * @len : length to write
+ *
+ * Should be protected by get_subbuf/put_subbuf.
+ */
+int ltt_relay_read(struct ltt_chanbuf_alloc *bufa, size_t offset, void *dest,
+		   size_t len)
+{
+	struct ltt_chan_alloc *chana = bufa->chan;
+	size_t index;
+	ssize_t pagecpy, orig_len;
+	struct chanbuf_page *rpages;
+
+	orig_len = len;
+	offset &= chana->buf_size - 1;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+	if (unlikely(!len))
+		return 0;
+	for (;;) {
+		pagecpy = min_t(size_t, len, PAGE_SIZE - (offset & ~PAGE_MASK));
+		rpages = bufa->buf_rsb.pages;
+		WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
+		memcpy(dest, rpages[index].virt + (offset & ~PAGE_MASK),
+		       pagecpy);
+		len -= pagecpy;
+		if (likely(!len))
+			break;
+		dest += pagecpy;
+		offset += pagecpy;
+		index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+		/*
+		 * Underlying layer should never ask for reads across
+		 * subbuffers.
+		 */
+		WARN_ON(offset >= chana->buf_size);
+	}
+	return orig_len;
+}
+EXPORT_SYMBOL_GPL(ltt_relay_read);
+
+/**
+ * ltt_relay_read_cstr - read a C-style string from ltt_relay_buffer.
+ * @bufa : buffer
+ * @offset : offset within the buffer
+ * @dest : destination address
+ * @len : destination's length
+ *
+ * return string's length
+ * Should be protected by get_subbuf/put_subbuf.
+ */
+int ltt_relay_read_cstr(struct ltt_chanbuf_alloc *bufa, size_t offset,
+			void *dest, size_t len)
+{
+	struct ltt_chan_alloc *chana = bufa->chan;
+	size_t index;
+	ssize_t pagecpy, pagelen, strpagelen, orig_offset;
+	char *str;
+	struct chanbuf_page *rpages;
+
+	offset &= chana->buf_size - 1;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+	orig_offset = offset;
+	for (;;) {
+		rpages = bufa->buf_rsb.pages;
+		WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
+		str = (char *)rpages[index].virt + (offset & ~PAGE_MASK);
+		pagelen = PAGE_SIZE - (offset & ~PAGE_MASK);
+		strpagelen = strnlen(str, pagelen);
+		if (len) {
+			pagecpy = min_t(size_t, len, strpagelen);
+			if (dest) {
+				memcpy(dest, str, pagecpy);
+				dest += pagecpy;
+			}
+			len -= pagecpy;
+		}
+		offset += strpagelen;
+		index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+		if (strpagelen < pagelen)
+			break;
+		/*
+		 * Underlying layer should never ask for reads across
+		 * subbuffers.
+		 */
+		WARN_ON(offset >= chana->buf_size);
+	}
+	if (dest && len)
+		((char *)dest)[0] = 0;
+	return offset - orig_offset;
+}
+EXPORT_SYMBOL_GPL(ltt_relay_read_cstr);
+
+/**
+ * ltt_relay_read_get_page - Get a whole page to read from
+ * @bufa : buffer
+ * @offset : offset within the buffer
+ *
+ * Should be protected by get_subbuf/put_subbuf.
+ */
+struct page *ltt_relay_read_get_page(struct ltt_chanbuf_alloc *bufa,
+				     size_t offset)
+{
+	size_t index;
+	struct chanbuf_page *rpages;
+	struct ltt_chan_alloc *chana = bufa->chan;
+
+	offset &= chana->buf_size - 1;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+	rpages = bufa->buf_rsb.pages;
+	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
+	return rpages[index].page;
+}
+EXPORT_SYMBOL_GPL(ltt_relay_read_get_page);
+
+/**
+ * ltt_relay_read_offset_address - get address of a location within the buffer
+ * @bufa : buffer
+ * @offset : offset within the buffer.
+ *
+ * Return the address where a given offset is located (for read).
+ * Should be used to get the current subbuffer header pointer. Given we know
+ * it's never on a page boundary, it's safe to write directly to this address,
+ * as long as the write is never bigger than a page size.
+ */
+void *ltt_relay_read_offset_address(struct ltt_chanbuf_alloc *bufa,
+				    size_t offset)
+{
+	size_t index;
+	struct chanbuf_page *rpages;
+	struct ltt_chan_alloc *chana = bufa->chan;
+
+	offset &= chana->buf_size - 1;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+	rpages = bufa->buf_rsb.pages;
+	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
+	return rpages[index].virt + (offset & ~PAGE_MASK);
+}
+EXPORT_SYMBOL_GPL(ltt_relay_read_offset_address);
+
+/**
+ * ltt_relay_offset_address - get address of a location within the buffer
+ * @bufa : buffer
+ * @offset : offset within the buffer.
+ *
+ * Return the address where a given offset is located.
+ * Should be used to get the current subbuffer header pointer. Given we know
+ * it's never on a page boundary, it's safe to write directly to this address,
+ * as long as the write is never bigger than a page size.
+ */
+void *ltt_relay_offset_address(struct ltt_chanbuf_alloc *bufa, size_t offset)
+{
+	size_t sbidx, index;
+	struct chanbuf_page *rpages;
+	struct ltt_chan_alloc *chana = bufa->chan;
+
+	offset &= chana->buf_size - 1;
+	sbidx = offset >> chana->sb_size_order;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+	rpages = bufa->buf_wsb[sbidx].pages;
+	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
+	return rpages[index].virt + (offset & ~PAGE_MASK);
+}
+EXPORT_SYMBOL_GPL(ltt_relay_offset_address);
+
+static __init int ltt_relay_init(void)
+{
+	hotcpu_notifier(ltt_relay_hotcpu_callback, 5);
+	return 0;
+}
+
+module_init(ltt_relay_init);
diff --git a/stblinux-2.6.31/ltt/ltt-relay-irqoff.c b/stblinux-2.6.31/ltt/ltt-relay-irqoff.c
new file mode 100644
index 0000000..096072d
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-relay-irqoff.c
@@ -0,0 +1,1701 @@
+/*
+ * ltt/ltt-relay-irqoff.c
+ *
+ * (C) Copyright 2005-2008 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng irqoff buffer space management (reader/writer).
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *  Karim Yaghmour (karim@opersys.com)
+ *  Tom Zanussi (zanussi@us.ibm.com)
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  08/10/08, Cleanup.
+ *  27/05/05, Modular redesign and rewrite.
+ *
+ * Userspace reader semantic :
+ * while (poll fd != POLLHUP) {
+ *   - ioctl RELAY_GET_SUBBUF_SIZE
+ *   while (1) {
+ *     - ioctl GET_SUBBUF
+ *     - splice 1 subbuffer worth of data to a pipe
+ *     - splice the data from pipe to disk/network
+ *     - ioctl PUT_SUBBUF, check error value
+ *       if err val < 0, previous subbuffer was corrupted.
+ *   }
+ * }
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/ltt-relay.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/rcupdate.h>
+#include <linux/timer.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/debugfs.h>
+#include <linux/stat.h>
+#include <linux/cpu.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/splice.h>
+#include <asm/atomic.h>
+#include <asm/local.h>
+
+#include "ltt-relay-irqoff.h"
+
+#if 0
+#define printk_dbg(fmt, args...) printk(fmt, args)
+#else
+#define printk_dbg(fmt, args...)
+#endif
+
+struct ltt_reserve_switch_offsets {
+	long begin, end, old;
+	long begin_switch, end_switch_current, end_switch_old;
+	size_t before_hdr_pad, size;
+};
+
+static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_chan,
+		struct rchan_buf *buf,
+		unsigned int cpu,
+		unsigned int n_subbufs);
+
+static void ltt_relay_destroy_buffer(struct ltt_channel_struct *ltt_chan,
+		unsigned int cpu);
+
+static void ltt_force_switch(struct rchan_buf *buf,
+		enum force_switch_mode mode);
+
+static const struct file_operations ltt_file_operations;
+
+static void ltt_buffer_begin(struct rchan_buf *buf,
+			u64 tsc, unsigned int subbuf_idx)
+{
+	struct ltt_channel_struct *channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_offset_address(buf,
+				subbuf_idx * buf->chan->subbuf_size);
+
+	header->cycle_count_begin = tsc;
+	header->lost_size = 0xFFFFFFFF; /* for debugging */
+	header->buf_size = buf->chan->subbuf_size;
+	ltt_write_trace_header(channel->trace, header);
+}
+
+/*
+ * offset is assumed to never be 0 here : never deliver a completely empty
+ * subbuffer. The lost size is between 0 and subbuf_size-1.
+ */
+static void ltt_buffer_end(struct rchan_buf *buf,
+		u64 tsc, unsigned int offset, unsigned int subbuf_idx)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_offset_address(buf,
+				subbuf_idx * buf->chan->subbuf_size);
+
+	header->lost_size = SUBBUF_OFFSET((buf->chan->subbuf_size - offset),
+				buf->chan);
+	header->cycle_count_end = tsc;
+	header->events_lost = local_read(&ltt_buf->events_lost);
+	header->subbuf_corrupt = local_read(&ltt_buf->corrupted_subbuffers);
+}
+
+static struct dentry *ltt_create_buf_file_callback(const char *filename,
+		struct dentry *parent, int mode,
+		struct rchan_buf *buf)
+{
+	struct ltt_channel_struct *ltt_chan;
+	int err;
+	struct dentry *dentry;
+
+	ltt_chan = buf->chan->private_data;
+	err = ltt_relay_create_buffer(ltt_chan->trace, ltt_chan,
+					buf, buf->cpu,
+					buf->chan->n_subbufs);
+	if (err)
+		return ERR_PTR(err);
+
+	dentry = debugfs_create_file(filename, mode, parent, buf,
+			&ltt_file_operations);
+	if (!dentry)
+		goto error;
+	if (buf->cpu == 0)
+		buf->ascii_dentry = ltt_ascii_create(ltt_chan->trace, ltt_chan);
+	return dentry;
+error:
+	ltt_relay_destroy_buffer(ltt_chan, buf->cpu);
+	return NULL;
+}
+
+static int ltt_remove_buf_file_callback(struct dentry *dentry)
+{
+	struct rchan_buf *buf = dentry->d_inode->i_private;
+	struct ltt_channel_struct *ltt_chan = buf->chan->private_data;
+
+	ltt_ascii_remove(ltt_chan, buf->ascii_dentry);
+	debugfs_remove(dentry);
+	ltt_relay_destroy_buffer(ltt_chan, buf->cpu);
+
+	return 0;
+}
+
+/*
+ * Wake writers :
+ *
+ * This must be done after the trace is removed from the RCU list so that there
+ * are no stalled writers.
+ */
+static void ltt_relay_wake_writers(struct ltt_channel_buf_struct *ltt_buf)
+{
+
+	if (waitqueue_active(&ltt_buf->write_wait))
+		wake_up_interruptible(&ltt_buf->write_wait);
+}
+
+/*
+ * This function should not be called from NMI interrupt context
+ */
+static void ltt_buf_unfull(struct rchan_buf *buf,
+		unsigned int subbuf_idx,
+		long offset)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	ltt_relay_wake_writers(ltt_buf);
+}
+
+/*
+ * Reader API.
+ */
+static unsigned long get_offset(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	return local_read(&ltt_buf->offset);
+}
+
+static unsigned long get_consumed(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	return atomic_long_read(&ltt_buf->consumed);
+}
+
+static int _ltt_open(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	if (!atomic_long_add_unless(&ltt_buf->active_readers, 1, 1))
+		return -EBUSY;
+	ltt_relay_get_chan(buf->chan);
+	return 0;
+}
+
+static int _ltt_release(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	ltt_relay_put_chan(buf->chan);
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+	atomic_long_dec(&ltt_buf->active_readers);
+	return 0;
+}
+
+static int is_finalized(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	return ltt_buf->finalized;
+}
+
+/*
+ * Promote compiler barrier to a smp_mb().
+ * For the specific LTTng case, this IPI call should be removed if the
+ * architecture does not reorder writes.  This should eventually be provided by
+ * a separate architecture-specific infrastructure.
+ */
+static void remote_mb(void *info)
+{
+	smp_mb();
+}
+
+static int get_subbuf(struct rchan_buf *buf, unsigned long *consumed)
+{
+	struct ltt_channel_struct *ltt_channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	long consumed_old, consumed_idx, commit_count, write_offset;
+	int ret;
+
+	consumed_old = atomic_long_read(&ltt_buf->consumed);
+	consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
+	commit_count = local_read(&ltt_buf->commit_count[consumed_idx]);
+	/*
+	 * Make sure we read the commit count before reading the buffer
+	 * data and the write offset. Correct consumed offset ordering
+	 * wrt commit count is insured by the use of cmpxchg to update
+	 * the consumed offset.
+	 * smp_call_function_single can fail if the remote CPU is offline,
+	 * this is OK because then there is no wmb to execute there.
+	 * If our thread is executing on the same CPU as the on the buffers
+	 * belongs to, we don't have to synchronize it at all. If we are
+	 * migrated, the scheduler will take care of the memory barriers.
+	 * Normally, smp_call_function_single() should ensure program order when
+	 * executing the remote function, which implies that it surrounds the
+	 * function execution with :
+	 * smp_mb()
+	 * send IPI
+	 * csd_lock_wait
+	 *                recv IPI
+	 *                smp_mb()
+	 *                exec. function
+	 *                smp_mb()
+	 *                csd unlock
+	 * smp_mb()
+	 *
+	 * However, smp_call_function_single() does not seem to clearly execute
+	 * such barriers. It depends on spinlock semantic to provide the barrier
+	 * before executing the IPI and, when busy-looping, csd_lock_wait only
+	 * executes smp_mb() when it has to wait for the other CPU.
+	 *
+	 * I don't trust this code. Therefore, let's add the smp_mb() sequence
+	 * required ourself, even if duplicated. It has no performance impact
+	 * anyway.
+	 *
+	 * smp_mb() is needed because smp_rmb() and smp_wmb() only order read vs
+	 * read and write vs write. They do not ensure core synchronization. We
+	 * really have to ensure total order between the 3 barriers running on
+	 * the 2 CPUs.
+	 */
+#ifdef LTT_NO_IPI_BARRIER
+	/*
+	 * Local rmb to match the remote wmb to read the commit count before the
+	 * buffer data and the write offset.
+	 */
+	smp_rmb();
+#else
+	if (raw_smp_processor_id() != buf->cpu) {
+		smp_mb();	/* Total order with IPI handler smp_mb() */
+		smp_call_function_single(buf->cpu, remote_mb, NULL, 1);
+		smp_mb();	/* Total order with IPI handler smp_mb() */
+	}
+#endif
+	write_offset = local_read(&ltt_buf->offset);
+	/*
+	 * Check that the subbuffer we are trying to consume has been
+	 * already fully committed.
+	 */
+	if (((commit_count - buf->chan->subbuf_size)
+	     & ltt_channel->commit_count_mask)
+	    - (BUFFER_TRUNC(consumed_old, buf->chan)
+	       >> ltt_channel->n_subbufs_order)
+	    != 0) {
+		return -EAGAIN;
+	}
+	/*
+	 * Check that we are not about to read the same subbuffer in
+	 * which the writer head is.
+	 */
+	if ((SUBBUF_TRUNC(write_offset, buf->chan)
+	   - SUBBUF_TRUNC(consumed_old, buf->chan))
+	   == 0) {
+		return -EAGAIN;
+	}
+
+	ret = update_read_sb_index(buf, consumed_idx);
+	if (ret)
+		return ret;
+
+	*consumed = consumed_old;
+	return 0;
+}
+
+static int put_subbuf(struct rchan_buf *buf, unsigned long consumed)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	long consumed_new, consumed_old;
+
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+
+	consumed_old = consumed;
+	consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(buf->rchan_rsb.pages));
+	RCHAN_SB_SET_NOREF(buf->rchan_rsb.pages);
+
+	spin_lock(&ltt_buf->full_lock);
+	if (atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
+				consumed_new)
+	    != consumed_old) {
+		/* We have been pushed by the writer. */
+		spin_unlock(&ltt_buf->full_lock);
+		/*
+		 * We exchanged the subbuffer pages. No corruption possible
+		 * even if the writer did push us. No more -EIO possible.
+		 */
+		return 0;
+	} else {
+		/* tell the client that buffer is now unfull */
+		int index;
+		long data;
+		index = SUBBUF_INDEX(consumed_old, buf->chan);
+		data = BUFFER_OFFSET(consumed_old, buf->chan);
+		ltt_buf_unfull(buf, index, data);
+		spin_unlock(&ltt_buf->full_lock);
+	}
+	return 0;
+}
+
+static unsigned long get_n_subbufs(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+	return buf->chan->n_subbufs;
+}
+
+static unsigned long get_subbuf_size(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+	return buf->chan->subbuf_size;
+}
+
+static void switch_buffer(unsigned long data)
+{
+	struct ltt_channel_buf_struct *ltt_buf =
+		(struct ltt_channel_buf_struct *)data;
+	struct rchan_buf *buf = ltt_buf->rbuf;
+
+	if (buf)
+		ltt_force_switch(buf, FORCE_ACTIVE);
+
+	ltt_buf->switch_timer.expires += ltt_buf->switch_timer_interval;
+	add_timer_on(&ltt_buf->switch_timer, smp_processor_id());
+}
+
+static void start_switch_timer(struct ltt_channel_struct *ltt_channel)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	int cpu;
+
+	if (!ltt_channel->switch_timer_interval)
+		return;
+
+	// TODO : hotplug
+	for_each_online_cpu(cpu) {
+		struct ltt_channel_buf_struct *ltt_buf;
+		struct rchan_buf *buf;
+
+		buf = rchan->buf[cpu];
+		ltt_buf = buf->chan_private;
+		buf->random_access = 1;
+		ltt_buf->switch_timer_interval =
+			ltt_channel->switch_timer_interval;
+		init_timer(&ltt_buf->switch_timer);
+		ltt_buf->switch_timer.function = switch_buffer;
+		ltt_buf->switch_timer.expires = jiffies +
+					ltt_buf->switch_timer_interval;
+		ltt_buf->switch_timer.data = (unsigned long)ltt_buf;
+		add_timer_on(&ltt_buf->switch_timer, cpu);
+	}
+}
+
+/*
+ * Cannot use del_timer_sync with add_timer_on, so use an IPI to locally
+ * delete the timer.
+ */
+static void stop_switch_timer_ipi(void *info)
+{
+	struct ltt_channel_buf_struct *ltt_buf =
+		(struct ltt_channel_buf_struct *)info;
+
+	del_timer(&ltt_buf->switch_timer);
+}
+
+static void stop_switch_timer(struct ltt_channel_struct *ltt_channel)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	int cpu;
+
+	if (!ltt_channel->switch_timer_interval)
+		return;
+
+	// TODO : hotplug
+	for_each_online_cpu(cpu) {
+		struct ltt_channel_buf_struct *ltt_buf;
+		struct rchan_buf *buf;
+
+		buf = rchan->buf[cpu];
+		ltt_buf = buf->chan_private;
+		smp_call_function(stop_switch_timer_ipi, ltt_buf, 1);
+		buf->random_access = 0;
+	}
+}
+
+static struct ltt_channel_buf_access_ops ltt_channel_buf_accessor = {
+	.get_offset   = get_offset,
+	.get_consumed = get_consumed,
+	.get_subbuf = get_subbuf,
+	.put_subbuf = put_subbuf,
+	.is_finalized = is_finalized,
+	.get_n_subbufs = get_n_subbufs,
+	.get_subbuf_size = get_subbuf_size,
+	.open = _ltt_open,
+	.release = _ltt_release,
+	.start_switch_timer = start_switch_timer,
+	.stop_switch_timer = stop_switch_timer,
+};
+
+/**
+ *	ltt_open - open file op for ltt files
+ *	@inode: opened inode
+ *	@file: opened file
+ *
+ *	Open implementation. Makes sure only one open instance of a buffer is
+ *	done at a given moment.
+ */
+static int ltt_open(struct inode *inode, struct file *file)
+{
+	int ret;
+	struct rchan_buf *buf = inode->i_private;
+
+	ret = _ltt_open(buf);
+	if (!ret)
+		ret = ltt_relay_file_operations.open(inode, file);
+	return ret;
+}
+
+/**
+ *	ltt_release - release file op for ltt files
+ *	@inode: opened inode
+ *	@file: opened file
+ *
+ *	Release implementation.
+ */
+static int ltt_release(struct inode *inode, struct file *file)
+{
+	struct rchan_buf *buf = inode->i_private;
+	int ret;
+
+	_ltt_release(buf);
+	ret = ltt_relay_file_operations.release(inode, file);
+	WARN_ON(ret);
+	return ret;
+}
+
+/**
+ *	ltt_poll - file op for ltt files
+ *	@filp: the file
+ *	@wait: poll table
+ *
+ *	Poll implementation.
+ */
+static unsigned int ltt_poll(struct file *filp, poll_table *wait)
+{
+	unsigned int mask = 0;
+	struct inode *inode = filp->f_dentry->d_inode;
+	struct rchan_buf *buf = inode->i_private;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	if (filp->f_mode & FMODE_READ) {
+		poll_wait_set_exclusive(wait);
+		poll_wait(filp, &ltt_buf->read_wait, wait);
+
+		WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+		if (SUBBUF_TRUNC(local_read(&ltt_buf->offset),
+							buf->chan)
+		  - SUBBUF_TRUNC(atomic_long_read(&ltt_buf->consumed),
+							buf->chan)
+		  == 0) {
+			if (ltt_buf->finalized)
+				return POLLHUP;
+			else
+				return 0;
+		} else {
+			struct rchan *rchan = buf->chan;
+			if (SUBBUF_TRUNC(local_read(&ltt_buf->offset),
+					buf->chan)
+			  - SUBBUF_TRUNC(atomic_long_read(
+						&ltt_buf->consumed),
+					buf->chan)
+			  >= rchan->alloc_size)
+				return POLLPRI | POLLRDBAND;
+			else
+				return POLLIN | POLLRDNORM;
+		}
+	}
+	return mask;
+}
+
+/**
+ *	ltt_ioctl - control on the debugfs file
+ *
+ *	@inode: the inode
+ *	@filp: the file
+ *	@cmd: the command
+ *	@arg: command arg
+ *
+ *	This ioctl implements three commands necessary for a minimal
+ *	producer/consumer implementation :
+ *	RELAY_GET_SUBBUF
+ *		Get the next sub buffer that can be read. It never blocks.
+ *	RELAY_PUT_SUBBUF
+ *		Release the currently read sub-buffer. Parameter is the last
+ *		put subbuffer (returned by GET_SUBBUF).
+ *	RELAY_GET_N_BUBBUFS
+ *		returns the number of sub buffers in the per cpu channel.
+ *	RELAY_GET_SUBBUF_SIZE
+ *		returns the size of the sub buffers.
+ */
+static int ltt_ioctl(struct inode *inode, struct file *filp,
+		unsigned int cmd, unsigned long arg)
+{
+	struct rchan_buf *buf = inode->i_private;
+	u32 __user *argp = (u32 __user *)arg;
+
+	switch (cmd) {
+	case RELAY_GET_SUBBUF:
+	{
+		unsigned long consumed;
+		int ret;
+
+		ret = get_subbuf(buf, &consumed);
+		if (ret)
+			return ret;
+		else
+			return put_user((u32)consumed, argp);
+		break;
+	}
+	case RELAY_PUT_SUBBUF:
+	{
+		struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+		u32 uconsumed_old;
+		int ret;
+		long consumed_old;
+
+		ret = get_user(uconsumed_old, argp);
+		if (ret)
+			return ret; /* will return -EFAULT */
+
+		consumed_old = atomic_long_read(&ltt_buf->consumed);
+		consumed_old = consumed_old & (~0xFFFFFFFFL);
+		consumed_old = consumed_old | uconsumed_old;
+		ret = put_subbuf(buf, consumed_old);
+		if (ret)
+			return ret;
+		break;
+	}
+	case RELAY_GET_N_SUBBUFS:
+		return put_user((u32)get_n_subbufs(buf), argp);
+		break;
+	case RELAY_GET_SUBBUF_SIZE:
+		return put_user((u32)get_subbuf_size(buf), argp);
+		break;
+	default:
+		return -ENOIOCTLCMD;
+	}
+	return 0;
+}
+
+#ifdef CONFIG_COMPAT
+static long ltt_compat_ioctl(struct file *file, unsigned int cmd,
+		unsigned long arg)
+{
+	long ret = -ENOIOCTLCMD;
+
+	lock_kernel();
+	ret = ltt_ioctl(file->f_dentry->d_inode, file, cmd, arg);
+	unlock_kernel();
+
+	return ret;
+}
+#endif
+
+static void ltt_relay_pipe_buf_release(struct pipe_inode_info *pipe,
+				   struct pipe_buffer *pbuf)
+{
+}
+
+static struct pipe_buf_operations ltt_relay_pipe_buf_ops = {
+	.can_merge = 0,
+	.map = generic_pipe_buf_map,
+	.unmap = generic_pipe_buf_unmap,
+	.confirm = generic_pipe_buf_confirm,
+	.release = ltt_relay_pipe_buf_release,
+	.steal = generic_pipe_buf_steal,
+	.get = generic_pipe_buf_get,
+};
+
+static void ltt_relay_page_release(struct splice_pipe_desc *spd, unsigned int i)
+{
+}
+
+/*
+ *	subbuf_splice_actor - splice up to one subbuf's worth of data
+ */
+static int subbuf_splice_actor(struct file *in,
+			       loff_t *ppos,
+			       struct pipe_inode_info *pipe,
+			       size_t len,
+			       unsigned int flags)
+{
+	struct rchan_buf *buf = in->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	unsigned int poff, subbuf_pages, nr_pages;
+	struct page *pages[PIPE_BUFFERS];
+	struct partial_page partial[PIPE_BUFFERS];
+	struct splice_pipe_desc spd = {
+		.pages = pages,
+		.nr_pages = 0,
+		.partial = partial,
+		.flags = flags,
+		.ops = &ltt_relay_pipe_buf_ops,
+		.spd_release = ltt_relay_page_release,
+	};
+	long consumed_old, consumed_idx, roffset;
+	unsigned long bytes_avail;
+
+	/*
+	 * Check that a GET_SUBBUF ioctl has been done before.
+	 */
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+	consumed_old = atomic_long_read(&ltt_buf->consumed);
+	consumed_old += *ppos;
+	consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
+
+	/*
+	 * Adjust read len, if longer than what is available.
+	 * Max read size is 1 subbuffer due to get_subbuf/put_subbuf for
+	 * protection.
+	 */
+	bytes_avail = buf->chan->subbuf_size;
+	WARN_ON(bytes_avail > buf->chan->alloc_size);
+	len = min_t(size_t, len, bytes_avail);
+	subbuf_pages = bytes_avail >> PAGE_SHIFT;
+	nr_pages = min_t(unsigned int, subbuf_pages, PIPE_BUFFERS);
+	roffset = consumed_old & PAGE_MASK;
+	poff = consumed_old & ~PAGE_MASK;
+	printk_dbg(KERN_DEBUG "SPLICE actor len %zu pos %zd write_pos %ld\n",
+		len, (ssize_t)*ppos, local_read(&ltt_buf->offset));
+
+	for (; spd.nr_pages < nr_pages; spd.nr_pages++) {
+		unsigned int this_len;
+		struct page *page;
+
+		if (!len)
+			break;
+		printk_dbg(KERN_DEBUG "SPLICE actor loop len %zu roffset %ld\n",
+			len, roffset);
+
+		this_len = PAGE_SIZE - poff;
+		page = ltt_relay_read_get_page(buf, roffset);
+		spd.pages[spd.nr_pages] = page;
+		spd.partial[spd.nr_pages].offset = poff;
+		spd.partial[spd.nr_pages].len = this_len;
+
+		poff = 0;
+		roffset += PAGE_SIZE;
+		len -= this_len;
+	}
+
+	if (!spd.nr_pages)
+		return 0;
+
+	return splice_to_pipe(pipe, &spd);
+}
+
+static ssize_t ltt_relay_file_splice_read(struct file *in,
+				      loff_t *ppos,
+				      struct pipe_inode_info *pipe,
+				      size_t len,
+				      unsigned int flags)
+{
+	ssize_t spliced;
+	int ret;
+
+	ret = 0;
+	spliced = 0;
+
+	printk_dbg(KERN_DEBUG "SPLICE read len %zu pos %zd\n",
+		len, (ssize_t)*ppos);
+	while (len && !spliced) {
+		ret = subbuf_splice_actor(in, ppos, pipe, len, flags);
+		printk_dbg(KERN_DEBUG "SPLICE read loop ret %d\n", ret);
+		if (ret < 0)
+			break;
+		else if (!ret) {
+			if (flags & SPLICE_F_NONBLOCK)
+				ret = -EAGAIN;
+			break;
+		}
+
+		*ppos += ret;
+		if (ret > len)
+			len = 0;
+		else
+			len -= ret;
+		spliced += ret;
+	}
+
+	if (spliced)
+		return spliced;
+
+	return ret;
+}
+
+static void ltt_relay_print_subbuffer_errors(
+		struct ltt_channel_struct *ltt_chan,
+		long cons_off, unsigned int cpu)
+{
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
+	long cons_idx, commit_count, write_offset;
+
+	cons_idx = SUBBUF_INDEX(cons_off, rchan);
+	commit_count = local_read(&ltt_buf->commit_count[cons_idx]);
+	/*
+	 * No need to order commit_count and write_offset reads because we
+	 * execute after trace is stopped when there are no readers left.
+	 */
+	write_offset = local_read(&ltt_buf->offset);
+	printk(KERN_WARNING
+		"LTT : unread channel %s offset is %ld "
+		"and cons_off : %ld (cpu %u)\n",
+		ltt_chan->channel_name, write_offset, cons_off, cpu);
+	/* Check each sub-buffer for non filled commit count */
+	if (((commit_count - rchan->subbuf_size) & ltt_chan->commit_count_mask)
+	    - (BUFFER_TRUNC(cons_off, rchan) >> ltt_chan->n_subbufs_order)
+	    != 0)
+		printk(KERN_ALERT
+			"LTT : %s : subbuffer %lu has non filled "
+			"commit count %lu.\n",
+			ltt_chan->channel_name, cons_idx, commit_count);
+	printk(KERN_ALERT "LTT : %s : commit count : %lu, subbuf size %zd\n",
+			ltt_chan->channel_name, commit_count,
+			rchan->subbuf_size);
+}
+
+static void ltt_relay_print_errors(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_chan, int cpu)
+{
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
+	long cons_off;
+
+	/*
+	 * Can be called in the error path of allocation when
+	 * trans_channel_data is not yet set.
+	 */
+	if (!rchan)
+		return;
+	for (cons_off = atomic_long_read(&ltt_buf->consumed);
+			(SUBBUF_TRUNC(local_read(&ltt_buf->offset),
+				      rchan)
+			 - cons_off) > 0;
+			cons_off = SUBBUF_ALIGN(cons_off, rchan))
+		ltt_relay_print_subbuffer_errors(ltt_chan, cons_off, cpu);
+}
+
+static void ltt_relay_print_buffer_errors(struct ltt_channel_struct *ltt_chan,
+		unsigned int cpu)
+{
+	struct ltt_trace_struct *trace = ltt_chan->trace;
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
+
+	if (local_read(&ltt_buf->events_lost))
+		printk(KERN_ALERT
+			"LTT : %s : %ld events lost "
+			"in %s channel (cpu %u).\n",
+			ltt_chan->channel_name,
+			local_read(&ltt_buf->events_lost),
+			ltt_chan->channel_name, cpu);
+	if (local_read(&ltt_buf->corrupted_subbuffers))
+		printk(KERN_ALERT
+			"LTT : %s : %ld corrupted subbuffers "
+			"in %s channel (cpu %u).\n",
+			ltt_chan->channel_name,
+			local_read(&ltt_buf->corrupted_subbuffers),
+			ltt_chan->channel_name, cpu);
+
+	ltt_relay_print_errors(trace, ltt_chan, cpu);
+}
+
+static void ltt_relay_remove_dirs(struct ltt_trace_struct *trace)
+{
+	ltt_ascii_remove_dir(trace);
+	debugfs_remove(trace->dentry.trace_root);
+}
+
+/*
+ * Create ltt buffer.
+ */
+static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_chan, struct rchan_buf *buf,
+		unsigned int cpu, unsigned int n_subbufs)
+{
+	struct ltt_channel_buf_struct *ltt_buf;
+	unsigned int j;
+
+	ltt_buf = kzalloc_node(sizeof(*ltt_buf), GFP_KERNEL, cpu_to_node(cpu));
+	if (!ltt_buf)
+		return -ENOMEM;
+
+	ltt_buf->commit_count =
+		kzalloc_node(ALIGN(sizeof(*ltt_buf->commit_count) * n_subbufs,
+				   1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(cpu));
+	if (!ltt_buf->commit_count) {
+		kfree(ltt_buf);
+		return -ENOMEM;
+	}
+
+#ifdef CONFIG_LTT_VMCORE
+	ltt_buf->commit_seq =
+		kzalloc_node(ALIGN(sizeof(*ltt_buf->commit_seq) * n_subbufs,
+				   1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(cpu));
+	if (!ltt_buf->commit_seq) {
+		kfree(ltt_buf->commit_count);
+		kfree(ltt_buf);
+		return -ENOMEM;
+	}
+#endif
+
+	buf->chan_private = ltt_buf;
+
+	kref_get(&trace->kref);
+	kref_get(&trace->ltt_transport_kref);
+	local_set(&ltt_buf->offset, ltt_subbuffer_header_size());
+	atomic_long_set(&ltt_buf->consumed, 0);
+	atomic_long_set(&ltt_buf->active_readers, 0);
+	for (j = 0; j < n_subbufs; j++)
+		local_set(&ltt_buf->commit_count[j], 0);
+	init_waitqueue_head(&ltt_buf->write_wait);
+	init_waitqueue_head(&ltt_buf->read_wait);
+	spin_lock_init(&ltt_buf->full_lock);
+
+	RCHAN_SB_CLEAR_NOREF(buf->rchan_wsb[0].pages);
+	ltt_buffer_begin(buf, trace->start_tsc, 0);
+	/* atomic_add made on local variable on data that belongs to
+	 * various CPUs : ok because tracing not started (for this cpu). */
+	local_add(ltt_subbuffer_header_size(), &ltt_buf->commit_count[0]);
+
+	local_set(&ltt_buf->events_lost, 0);
+	local_set(&ltt_buf->corrupted_subbuffers, 0);
+	ltt_buf->finalized = 0;
+	ltt_buf->rbuf = buf;
+
+	return 0;
+}
+
+static void ltt_relay_destroy_buffer(struct ltt_channel_struct *ltt_chan,
+		unsigned int cpu)
+{
+	struct ltt_trace_struct *trace = ltt_chan->trace;
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
+
+	kref_put(&ltt_chan->trace->ltt_transport_kref,
+		ltt_release_transport);
+	ltt_relay_print_buffer_errors(ltt_chan, cpu);
+#ifdef CONFIG_LTT_VMCORE
+	kfree(ltt_buf->commit_seq);
+#endif
+	kfree(ltt_buf->commit_count);
+	kfree(ltt_buf);
+	kref_put(&trace->kref, ltt_release_trace);
+	wake_up_interruptible(&trace->kref_wq);
+}
+
+/*
+ * Create channel.
+ */
+static int ltt_relay_create_channel(const char *trace_name,
+		struct ltt_trace_struct *trace, struct dentry *dir,
+		const char *channel_name, struct ltt_channel_struct *ltt_chan,
+		unsigned int subbuf_size, unsigned int n_subbufs,
+		int overwrite)
+{
+	char *tmpname;
+	unsigned int tmpname_len;
+	int err = 0;
+
+	tmpname = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (!tmpname)
+		return EPERM;
+	if (overwrite) {
+		strncpy(tmpname, LTT_FLIGHT_PREFIX, PATH_MAX-1);
+		strncat(tmpname, channel_name,
+			PATH_MAX-1-sizeof(LTT_FLIGHT_PREFIX));
+	} else {
+		strncpy(tmpname, channel_name, PATH_MAX-1);
+	}
+	strncat(tmpname, "_", PATH_MAX-1-strlen(tmpname));
+
+	ltt_chan->trace = trace;
+	ltt_chan->overwrite = overwrite;
+	ltt_chan->n_subbufs_order = get_count_order(n_subbufs);
+	ltt_chan->commit_count_mask = (~0UL >> ltt_chan->n_subbufs_order);
+	ltt_chan->trans_channel_data = ltt_relay_open(tmpname,
+			dir,
+			subbuf_size,
+			n_subbufs,
+			&trace->callbacks,
+			ltt_chan,
+			overwrite);
+	tmpname_len = strlen(tmpname);
+	if (tmpname_len > 0) {
+		/* Remove final _ for pretty printing */
+		tmpname[tmpname_len-1] = '\0';
+	}
+	if (ltt_chan->trans_channel_data == NULL) {
+		printk(KERN_ERR "LTT : Can't open %s channel for trace %s\n",
+				tmpname, trace_name);
+		goto relay_open_error;
+	}
+
+	ltt_chan->buf_access_ops = &ltt_channel_buf_accessor;
+
+	err = 0;
+	goto end;
+
+relay_open_error:
+	err = EPERM;
+end:
+	kfree(tmpname);
+	return err;
+}
+
+static int ltt_relay_create_dirs(struct ltt_trace_struct *new_trace)
+{
+	struct dentry *ltt_root_dentry;
+	int ret;
+
+	ltt_root_dentry = get_ltt_root();
+	if (!ltt_root_dentry)
+		return ENOENT;
+
+	new_trace->dentry.trace_root = debugfs_create_dir(new_trace->trace_name,
+			ltt_root_dentry);
+	put_ltt_root();
+	if (new_trace->dentry.trace_root == NULL) {
+		printk(KERN_ERR "LTT : Trace directory name %s already taken\n",
+				new_trace->trace_name);
+		return EEXIST;
+	}
+	ret = ltt_ascii_create_dir(new_trace);
+	if (ret)
+		printk(KERN_WARNING "LTT : Unable to create ascii output file "
+				    "for trace %s\n", new_trace->trace_name);
+
+	new_trace->callbacks.create_buf_file = ltt_create_buf_file_callback;
+	new_trace->callbacks.remove_buf_file = ltt_remove_buf_file_callback;
+
+	return 0;
+}
+
+/*
+ * LTTng channel flush function.
+ *
+ * Must be called when no tracing is active in the channel, because of
+ * accesses across CPUs.
+ */
+static notrace void ltt_relay_buffer_flush(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	ltt_buf->finalized = 1;
+	ltt_force_switch(buf, FORCE_FLUSH);
+}
+
+static void ltt_relay_async_wakeup_chan(struct ltt_channel_struct *ltt_channel)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	unsigned int i;
+
+	for_each_possible_cpu(i) {
+		struct ltt_channel_buf_struct *ltt_buf;
+
+		if (!rchan->buf[i])
+			continue;
+
+		ltt_buf = rchan->buf[i]->chan_private;
+		if (ltt_poll_deliver(ltt_channel, ltt_buf,
+				     rchan, rchan->buf[i]))
+			wake_up_interruptible(&ltt_buf->read_wait);
+	}
+}
+
+static void ltt_relay_finish_buffer(struct ltt_channel_struct *ltt_channel,
+		unsigned int cpu)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+
+	if (rchan->buf[cpu]) {
+		struct ltt_channel_buf_struct *ltt_buf =
+				rchan->buf[cpu]->chan_private;
+		ltt_relay_buffer_flush(rchan->buf[cpu]);
+		ltt_relay_wake_writers(ltt_buf);
+	}
+}
+
+
+static void ltt_relay_finish_channel(struct ltt_channel_struct *ltt_channel)
+{
+	unsigned int i;
+
+	for_each_possible_cpu(i)
+		ltt_relay_finish_buffer(ltt_channel, i);
+}
+
+static void ltt_relay_remove_channel(struct ltt_channel_struct *channel)
+{
+	struct rchan *rchan = channel->trans_channel_data;
+
+	ltt_relay_close(rchan);
+}
+
+/*
+ * This is called with preemption disabled when user space has requested
+ * blocking mode.  If one of the active traces has free space below a
+ * specific threshold value, we reenable preemption and block.
+ */
+static int ltt_relay_user_blocking(struct ltt_trace_struct *trace,
+		unsigned int chan_index, size_t data_size,
+		struct user_dbg_data *dbg)
+{
+	struct rchan *rchan;
+	struct ltt_channel_buf_struct *ltt_buf;
+	struct ltt_channel_struct *channel;
+	struct rchan_buf *relay_buf;
+	int cpu;
+	DECLARE_WAITQUEUE(wait, current);
+
+	channel = &trace->channels[chan_index];
+	rchan = channel->trans_channel_data;
+	cpu = smp_processor_id();
+	relay_buf = rchan->buf[cpu];
+	ltt_buf = relay_buf->chan_private;
+
+	/*
+	 * Check if data is too big for the channel : do not
+	 * block for it.
+	 */
+	if (LTT_RESERVE_CRITICAL + data_size > relay_buf->chan->subbuf_size)
+		return 0;
+
+	/*
+	 * If free space too low, we block. We restart from the
+	 * beginning after we resume (cpu id may have changed
+	 * while preemption is active).
+	 */
+	spin_lock(&ltt_buf->full_lock);
+	if (!channel->overwrite) {
+		dbg->write = local_read(&ltt_buf->offset);
+		dbg->read = atomic_long_read(&ltt_buf->consumed);
+		dbg->avail_size = dbg->write + LTT_RESERVE_CRITICAL + data_size
+				  - SUBBUF_TRUNC(dbg->read,
+						 relay_buf->chan);
+		if (dbg->avail_size > rchan->alloc_size) {
+			__set_current_state(TASK_INTERRUPTIBLE);
+			add_wait_queue(&ltt_buf->write_wait, &wait);
+			spin_unlock(&ltt_buf->full_lock);
+			preempt_enable();
+			schedule();
+			__set_current_state(TASK_RUNNING);
+			remove_wait_queue(&ltt_buf->write_wait, &wait);
+			if (signal_pending(current))
+				return -ERESTARTSYS;
+			preempt_disable();
+			return 1;
+		}
+	}
+	spin_unlock(&ltt_buf->full_lock);
+	return 0;
+}
+
+static void ltt_relay_print_user_errors(struct ltt_trace_struct *trace,
+		unsigned int chan_index, size_t data_size,
+		struct user_dbg_data *dbg, int cpu)
+{
+	struct rchan *rchan;
+	struct ltt_channel_buf_struct *ltt_buf;
+	struct ltt_channel_struct *channel;
+	struct rchan_buf *relay_buf;
+
+	channel = &trace->channels[chan_index];
+	rchan = channel->trans_channel_data;
+	relay_buf = rchan->buf[cpu];
+	ltt_buf = relay_buf->chan_private;
+
+	printk(KERN_ERR "Error in LTT usertrace : "
+	"buffer full : event lost in blocking "
+	"mode. Increase LTT_RESERVE_CRITICAL.\n");
+	printk(KERN_ERR "LTT nesting level is %u.\n",
+		per_cpu(ltt_nesting, cpu));
+	printk(KERN_ERR "LTT avail size %lu.\n",
+		dbg->avail_size);
+	printk(KERN_ERR "avai write : %lu, read : %lu\n",
+			dbg->write, dbg->read);
+
+	dbg->write = local_read(&ltt_buf->offset);
+	dbg->read = atomic_long_read(&ltt_buf->consumed);
+
+	printk(KERN_ERR "LTT cur size %lu.\n",
+		dbg->write + LTT_RESERVE_CRITICAL + data_size
+		- SUBBUF_TRUNC(dbg->read, relay_buf->chan));
+	printk(KERN_ERR "cur write : %lu, read : %lu\n",
+			dbg->write, dbg->read);
+}
+
+static void ltt_reserve_push_reader(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets)
+{
+	long consumed_old, consumed_new;
+
+	do {
+		consumed_old = atomic_long_read(&ltt_buf->consumed);
+		/*
+		 * If buffer is in overwrite mode, push the reader consumed
+		 * count if the write position has reached it and we are not
+		 * at the first iteration (don't push the reader farther than
+		 * the writer). This operation can be done concurrently by many
+		 * writers in the same buffer, the writer being at the farthest
+		 * write position sub-buffer index in the buffer being the one
+		 * which will win this loop.
+		 * If the buffer is not in overwrite mode, pushing the reader
+		 * only happens if a sub-buffer is corrupted.
+		 */
+		if (unlikely((SUBBUF_TRUNC(offsets->end-1, buf->chan)
+		   - SUBBUF_TRUNC(consumed_old, buf->chan))
+		   >= rchan->alloc_size))
+			consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+		else
+			return;
+	} while (unlikely(atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
+			consumed_new) != consumed_old));
+}
+
+
+/*
+ * ltt_reserve_switch_old_subbuf: switch old subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ *
+ * Note : offset_old should never be 0 here.
+ */
+static void ltt_reserve_switch_old_subbuf(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long oldidx = SUBBUF_INDEX(offsets->old - 1, rchan);
+	long commit_count, padding_size;
+
+	padding_size = rchan->subbuf_size
+			- (SUBBUF_OFFSET(offsets->old - 1, rchan) + 1);
+	ltt_buffer_end(buf, *tsc, offsets->old, oldidx);
+	/*
+	 * Must write slot data before incrementing commit count.
+	 * This compiler barrier is upgraded into a smp_wmb() by the IPI
+	 * sent by get_subbuf() when it does its smp_rmb().
+	 */
+	barrier();
+	commit_count = local_read(&ltt_buf->commit_count[oldidx])
+				  + padding_size;
+	local_set(&ltt_buf->commit_count[oldidx], commit_count);
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offsets->old - 1, commit_count, oldidx);
+	ltt_write_commit_counter(buf, ltt_buf, oldidx,
+		offsets->old, commit_count, padding_size);
+}
+
+/*
+ * ltt_reserve_switch_new_subbuf: Populate new subbuffer.
+ *
+ * This code can be executed unordered : writers may already have written to the
+ * sub-buffer before this code gets executed, caution.  The commit makes sure
+ * that this code is executed before the deliver of this sub-buffer.
+ */
+static void ltt_reserve_switch_new_subbuf(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long beginidx = SUBBUF_INDEX(offsets->begin, rchan);
+	long commit_count;
+
+	ltt_buffer_begin(buf, *tsc, beginidx);
+	/*
+	 * Must write slot data before incrementing commit count.
+	 * This compiler barrier is upgraded into a smp_wmb() by the IPI
+	 * sent by get_subbuf() when it does its smp_rmb().
+	 */
+	barrier();
+	commit_count = local_read(&ltt_buf->commit_count[beginidx])
+				  + ltt_subbuffer_header_size();
+	local_set(&ltt_buf->commit_count[beginidx], commit_count);
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offsets->begin, commit_count, beginidx);
+	ltt_write_commit_counter(buf, ltt_buf, beginidx,
+		offsets->begin, commit_count, ltt_subbuffer_header_size());
+}
+
+
+/*
+ * ltt_reserve_end_switch_current: finish switching current subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ */
+static void ltt_reserve_end_switch_current(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long endidx = SUBBUF_INDEX(offsets->end - 1, rchan);
+	long commit_count, padding_size;
+
+	padding_size = rchan->subbuf_size
+			- (SUBBUF_OFFSET(offsets->end - 1, rchan) + 1);
+	ltt_buffer_end(buf, *tsc, offsets->end, endidx);
+	/*
+	 * Must write slot data before incrementing commit count.
+	 * This compiler barrier is upgraded into a smp_wmb() by the IPI
+	 * sent by get_subbuf() when it does its smp_rmb().
+	 */
+	barrier();
+	commit_count = local_read(&ltt_buf->commit_count[endidx])
+				  + padding_size;
+	local_set(&ltt_buf->commit_count[endidx], commit_count);
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offsets->end - 1, commit_count, endidx);
+	ltt_write_commit_counter(buf, ltt_buf, endidx,
+		offsets->end, commit_count, padding_size);
+}
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static int ltt_relay_try_switch_slow(
+		enum force_switch_mode mode,
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets,
+		u64 *tsc)
+{
+	long subbuf_index;
+	long reserve_commit_diff;
+
+	offsets->begin = local_read(&ltt_buf->offset);
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_old = 0;
+
+	*tsc = trace_clock_read64();
+
+	if (SUBBUF_OFFSET(offsets->begin, buf->chan) != 0) {
+		offsets->begin = SUBBUF_ALIGN(offsets->begin, buf->chan);
+		offsets->end_switch_old = 1;
+	} else {
+		/* we do not have to switch : buffer is empty */
+		return -1;
+	}
+	if (mode == FORCE_ACTIVE)
+		offsets->begin += ltt_subbuffer_header_size();
+	/*
+	 * Always begin_switch in FORCE_ACTIVE mode.
+	 * Test new buffer integrity
+	 */
+	subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
+	reserve_commit_diff =
+		(BUFFER_TRUNC(offsets->begin, buf->chan)
+		 >> ltt_channel->n_subbufs_order)
+		- (local_read(&ltt_buf->commit_count[subbuf_index])
+			& ltt_channel->commit_count_mask);
+	if (reserve_commit_diff == 0) {
+		/* Next buffer not corrupted. */
+		if (mode == FORCE_ACTIVE
+		    && !ltt_channel->overwrite
+		    && offsets->begin - atomic_long_read(&ltt_buf->consumed)
+		       >= rchan->alloc_size) {
+			/*
+			 * We do not overwrite non consumed buffers and we are
+			 * full : ignore switch while tracing is active.
+			 */
+			return -1;
+		}
+	} else {
+		/*
+		 * Next subbuffer corrupted. Force pushing reader even in normal
+		 * mode
+		 */
+	}
+	offsets->end = offsets->begin;
+	return 0;
+}
+
+/*
+ * Force a sub-buffer switch for a per-cpu buffer. This operation is
+ * completely reentrant : can be called while tracing is active with
+ * absolutely no lock held.
+ *
+ * Note, however, that as we are disabling interrupts to make local operations
+ * atomic, this function must be called from the CPU which owns the buffer for
+ * an ACTIVE flush.
+ */
+void ltt_force_switch_irqoff_slow(struct rchan_buf *buf,
+		enum force_switch_mode mode)
+{
+	struct ltt_channel_struct *ltt_channel =
+			(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct ltt_reserve_switch_offsets offsets;
+	unsigned long flags;
+	u64 tsc;
+
+	raw_local_irq_save(flags);
+
+	offsets.size = 0;
+
+	if (ltt_relay_try_switch_slow(mode, ltt_channel, ltt_buf,
+			rchan, buf, &offsets, &tsc)) {
+		raw_local_irq_restore(flags);
+		return;
+	}
+
+	local_set(&ltt_buf->offset, offsets.end);
+
+	save_last_tsc(ltt_buf, tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	if (mode == FORCE_ACTIVE) {
+		ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan,
+					buf, &offsets);
+		ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.end - 1,
+							      rchan));
+	}
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (offsets.end_switch_old) {
+		ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.old - 1,
+							      rchan));
+		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
+			&offsets, &tsc);
+	}
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (mode == FORCE_ACTIVE)
+		ltt_reserve_switch_new_subbuf(ltt_channel,
+			ltt_buf, rchan, buf, &offsets, &tsc);
+
+	raw_local_irq_restore(flags);
+}
+EXPORT_SYMBOL_GPL(ltt_force_switch_irqoff_slow);
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static int ltt_relay_try_reserve_slow(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, size_t data_size,
+		u64 *tsc, unsigned int *rflags, int largest_align)
+{
+	long reserve_commit_diff;
+
+	offsets->begin = local_read(&ltt_buf->offset);
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_current = 0;
+	offsets->end_switch_old = 0;
+
+	*tsc = trace_clock_read64();
+	if (last_tsc_overflow(ltt_buf, *tsc))
+		*rflags = LTT_RFLAG_ID_SIZE_TSC;
+
+	if (unlikely(SUBBUF_OFFSET(offsets->begin, buf->chan) == 0)) {
+		offsets->begin_switch = 1;		/* For offsets->begin */
+	} else {
+		offsets->size = ltt_get_header_size(ltt_channel,
+					offsets->begin, data_size,
+					&offsets->before_hdr_pad, *rflags);
+		offsets->size += ltt_align(offsets->begin + offsets->size,
+					   largest_align)
+				 + data_size;
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan) +
+			     offsets->size) > buf->chan->subbuf_size)) {
+			offsets->end_switch_old = 1;	/* For offsets->old */
+			offsets->begin_switch = 1;	/* For offsets->begin */
+		}
+	}
+	if (unlikely(offsets->begin_switch)) {
+		long subbuf_index;
+
+		/*
+		 * We are typically not filling the previous buffer completely.
+		 */
+		if (likely(offsets->end_switch_old))
+			offsets->begin = SUBBUF_ALIGN(offsets->begin,
+						      buf->chan);
+		offsets->begin = offsets->begin + ltt_subbuffer_header_size();
+		/* Test new buffer integrity */
+		subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
+		reserve_commit_diff =
+			(BUFFER_TRUNC(offsets->begin, buf->chan)
+			 >> ltt_channel->n_subbufs_order)
+			- (local_read(&ltt_buf->commit_count[subbuf_index])
+				& ltt_channel->commit_count_mask);
+		if (likely(reserve_commit_diff == 0)) {
+			/* Next buffer not corrupted. */
+			if (unlikely(!ltt_channel->overwrite &&
+				(SUBBUF_TRUNC(offsets->begin, buf->chan)
+				 - SUBBUF_TRUNC(atomic_long_read(
+							&ltt_buf->consumed),
+						buf->chan))
+				>= rchan->alloc_size)) {
+				/*
+				 * We do not overwrite non consumed buffers
+				 * and we are full : event is lost.
+				 */
+				local_inc(&ltt_buf->events_lost);
+				return -1;
+			} else {
+				/*
+				 * next buffer not corrupted, we are either in
+				 * overwrite mode or the buffer is not full.
+				 * It's safe to write in this new subbuffer.
+				 */
+			}
+		} else {
+			/*
+			 * Next subbuffer corrupted. Drop event in normal and
+			 * overwrite mode. Caused by either a writer OOPS or
+			 * too many nested writes over a reserve/commit pair.
+			 */
+			local_inc(&ltt_buf->events_lost);
+			return -1;
+		}
+		offsets->size = ltt_get_header_size(ltt_channel,
+					offsets->begin, data_size,
+					&offsets->before_hdr_pad, *rflags);
+		offsets->size += ltt_align(offsets->begin + offsets->size,
+					   largest_align)
+				 + data_size;
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan)
+			     + offsets->size) > buf->chan->subbuf_size)) {
+			/*
+			 * Event too big for subbuffers, report error, don't
+			 * complete the sub-buffer switch.
+			 */
+			local_inc(&ltt_buf->events_lost);
+			return -1;
+		} else {
+			/*
+			 * We just made a successful buffer switch and the event
+			 * fits in the new subbuffer. Let's write.
+			 */
+		}
+	} else {
+		/*
+		 * Event fits in the current buffer and we are not on a switch
+		 * boundary. It's safe to write.
+		 */
+	}
+	offsets->end = offsets->begin + offsets->size;
+
+	if (unlikely((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0)) {
+		/*
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
+		 */
+		offsets->end_switch_current = 1;	/* For offsets->begin */
+	}
+	return 0;
+}
+
+/**
+ * ltt_relay_reserve_slot_irqoff_slow - Atomic slot reservation in a buffer.
+ * @trace: the trace structure to log to.
+ * @ltt_channel: channel structure
+ * @transport_data: data structure specific to ltt relay
+ * @data_size: size of the variable length data to log.
+ * @slot_size: pointer to total size of the slot (out)
+ * @buf_offset : pointer to reserved buffer offset (out)
+ * @tsc: pointer to the tsc at the slot reservation (out)
+ * @cpu: cpuid
+ *
+ * Return : -ENOSPC if not enough space, else returns 0.
+ * It will take care of sub-buffer switching.
+ */
+int ltt_reserve_slot_irqoff_slow(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
+		unsigned int *rflags, int largest_align, int cpu)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct rchan_buf *buf = *transport_data = rchan->buf[cpu];
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_reserve_switch_offsets offsets;
+
+	offsets.size = 0;
+
+	if (unlikely(ltt_relay_try_reserve_slow(ltt_channel, ltt_buf,
+			rchan, buf, &offsets, data_size, tsc, rflags,
+			largest_align))) {
+		raw_local_irq_restore(ltt_buf->irqflags);
+		return -ENOSPC;
+	}
+
+	local_set(&ltt_buf->offset, offsets.end);
+
+	save_last_tsc(ltt_buf, *tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan, buf, &offsets);
+
+	/*
+	 * Clear noref flag for this subbuffer.
+	 */
+	ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.end - 1, rchan));
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (unlikely(offsets.end_switch_old)) {
+		ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.old - 1,
+							      rchan));
+		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
+			&offsets, tsc);
+	}
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (unlikely(offsets.begin_switch))
+		ltt_reserve_switch_new_subbuf(ltt_channel, ltt_buf, rchan,
+			buf, &offsets, tsc);
+
+	if (unlikely(offsets.end_switch_current))
+		ltt_reserve_end_switch_current(ltt_channel, ltt_buf, rchan,
+			buf, &offsets, tsc);
+
+	*slot_size = offsets.size;
+	*buf_offset = offsets.begin + offsets.before_hdr_pad;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ltt_reserve_slot_irqoff_slow);
+
+static struct ltt_transport ltt_relay_transport = {
+	.name = "relay",
+	.owner = THIS_MODULE,
+	.ops = {
+		.create_dirs = ltt_relay_create_dirs,
+		.remove_dirs = ltt_relay_remove_dirs,
+		.create_channel = ltt_relay_create_channel,
+		.finish_channel = ltt_relay_finish_channel,
+		.remove_channel = ltt_relay_remove_channel,
+		.wakeup_channel = ltt_relay_async_wakeup_chan,
+		.user_blocking = ltt_relay_user_blocking,
+		.user_errors = ltt_relay_print_user_errors,
+	},
+};
+
+static const struct file_operations ltt_file_operations = {
+	.open = ltt_open,
+	.release = ltt_release,
+	.poll = ltt_poll,
+	.splice_read = ltt_relay_file_splice_read,
+	.ioctl = ltt_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = ltt_compat_ioctl,
+#endif
+};
+
+static int __init ltt_relay_init(void)
+{
+	printk(KERN_INFO "LTT : ltt-relay init\n");
+
+	ltt_transport_register(&ltt_relay_transport);
+
+	return 0;
+}
+
+static void __exit ltt_relay_exit(void)
+{
+	printk(KERN_INFO "LTT : ltt-relay exit\n");
+
+	ltt_transport_unregister(&ltt_relay_transport);
+}
+
+module_init(ltt_relay_init);
+module_exit(ltt_relay_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Irqoff Relay");
diff --git a/stblinux-2.6.31/ltt/ltt-relay-irqoff.h b/stblinux-2.6.31/ltt/ltt-relay-irqoff.h
new file mode 100644
index 0000000..97fbb32
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-relay-irqoff.h
@@ -0,0 +1,444 @@
+#ifndef _LTT_LTT_RELAY_IRQOFF_H
+#define _LTT_LTT_RELAY_IRQOFF_H
+
+/*
+ * ltt/ltt-relay-irqoff.h
+ *
+ * (C) Copyright 2005-2008 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng irqoff buffer space management (reader/writer).
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *  Karim Yaghmour (karim@opersys.com)
+ *  Tom Zanussi (zanussi@us.ibm.com)
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  08/10/08, Cleanup.
+ *  19/10/05, Complete irqoff mechanism.
+ *  27/05/05, Modular redesign and rewrite.
+ *
+ * Userspace reader semantic :
+ * while (poll fd != POLLHUP) {
+ *   - ioctl RELAY_GET_SUBBUF_SIZE
+ *   while (1) {
+ *     - ioctl GET_SUBBUF
+ *     - splice 1 subbuffer worth of data to a pipe
+ *     - splice the data from pipe to disk/network
+ *     - ioctl PUT_SUBBUF, check error value
+ *       if err val < 0, previous subbuffer was corrupted.
+ *   }
+ * }
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/cache.h>
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/ltt-relay.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/rcupdate.h>
+#include <linux/timer.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/debugfs.h>
+#include <linux/stat.h>
+#include <linux/cpu.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/splice.h>
+#include <linux/hardirq.h>
+#include <asm/atomic.h>
+#include <asm/local.h>
+
+#if 0
+#define printk_dbg(fmt, args...) printk(fmt, args)
+#else
+#define printk_dbg(fmt, args...)
+#endif
+
+/* LTTng irqoff logging buffer info */
+struct ltt_channel_buf_struct {
+	/* First 32 bytes cache-hot cacheline */
+	local_t offset;			/* Current offset in the buffer */
+	local_t *commit_count;		/* Commit count per sub-buffer */
+	unsigned long irqflags;		/* IRQ flags saved by reserve */
+	unsigned long last_tsc;		/*
+					 * Last timestamp written in the buffer.
+					 */
+	/* End of first 32 bytes cacheline */
+#ifdef CONFIG_LTT_VMCORE
+	local_t *commit_seq;		/* Consecutive commits */
+#endif
+	atomic_long_t consumed;		/*
+					 * Current offset in the buffer
+					 * standard atomic access (shared)
+					 */
+	atomic_long_t active_readers;	/*
+					 * Active readers count
+					 * standard atomic access (shared)
+					 */
+	local_t events_lost;
+	local_t corrupted_subbuffers;
+	spinlock_t full_lock;		/*
+					 * buffer full condition spinlock, only
+					 * for userspace tracing blocking mode
+					 * synchronization with reader.
+					 */
+	wait_queue_head_t write_wait;	/*
+					 * Wait queue for blocking user space
+					 * writers
+					 */
+	wait_queue_head_t read_wait;	/* reader wait queue */
+	unsigned int finalized;		/* buffer has been finalized */
+	struct timer_list switch_timer;	/* timer for periodical switch */
+	unsigned long switch_timer_interval;	/* in jiffies. 0 unset */
+	struct rchan_buf *rbuf;		/* Pointer to rchan_buf */
+} ____cacheline_internodealigned_in_smp;
+
+/*
+ * A switch is done during tracing or as a final flush after tracing (so it
+ * won't write in the new sub-buffer).
+ */
+enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
+
+extern int ltt_reserve_slot_irqoff_slow(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
+		unsigned int *rflags, int largest_align, int cpu);
+
+extern void ltt_force_switch_irqoff_slow(struct rchan_buf *buf,
+		enum force_switch_mode mode);
+
+/*
+ * Last TSC comparison functions. Check if the current TSC overflows
+ * LTT_TSC_BITS bits from the last TSC read. Reads and writes last_tsc
+ * atomically.
+ */
+
+#if (BITS_PER_LONG == 32)
+static __inline__ void save_last_tsc(struct ltt_channel_buf_struct *ltt_buf,
+					u64 tsc)
+{
+	ltt_buf->last_tsc = (unsigned long)(tsc >> LTT_TSC_BITS);
+}
+
+static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
+					u64 tsc)
+{
+	unsigned long tsc_shifted = (unsigned long)(tsc >> LTT_TSC_BITS);
+
+	if (unlikely((tsc_shifted - ltt_buf->last_tsc)))
+		return 1;
+	else
+		return 0;
+}
+#else
+static __inline__ void save_last_tsc(struct ltt_channel_buf_struct *ltt_buf,
+					u64 tsc)
+{
+	ltt_buf->last_tsc = (unsigned long)tsc;
+}
+
+static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
+					u64 tsc)
+{
+	if (unlikely((tsc - ltt_buf->last_tsc) >> LTT_TSC_BITS))
+		return 1;
+	else
+		return 0;
+}
+#endif
+
+#ifdef CONFIG_LTT_VMCORE
+static __inline__ void ltt_vmcore_check_deliver(
+		struct ltt_channel_buf_struct *ltt_buf,
+		long commit_count, long idx)
+{
+	local_set(&ltt_buf->commit_seq[idx], commit_count);
+}
+#else
+static __inline__ void ltt_vmcore_check_deliver(
+		struct ltt_channel_buf_struct *ltt_buf,
+		long commit_count, long idx)
+{
+}
+#endif
+
+static __inline__ void ltt_check_deliver(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		long offset, long commit_count, long idx)
+{
+	/* Check if all commits have been done */
+	if (unlikely((BUFFER_TRUNC(offset, rchan)
+			>> ltt_channel->n_subbufs_order)
+			- ((commit_count - rchan->subbuf_size)
+			   & ltt_channel->commit_count_mask) == 0)) {
+		/*
+		 * Set noref flag for this subbuffer.
+		 */
+		ltt_set_noref_flag(rchan, buf, idx);
+		ltt_vmcore_check_deliver(ltt_buf, commit_count, idx);
+	}
+}
+
+
+static __inline__ int ltt_poll_deliver(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf)
+{
+	long consumed_old, consumed_idx, commit_count, write_offset;
+
+	consumed_old = atomic_long_read(&ltt_buf->consumed);
+	consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
+	commit_count = local_read(&ltt_buf->commit_count[consumed_idx]);
+	/*
+	 * No memory barrier here, since we are only interested
+	 * in a statistically correct polling result. The next poll will
+	 * get the data is we are racing. The mb() that ensures correct
+	 * memory order is in get_subbuf.
+	 */
+	write_offset = local_read(&ltt_buf->offset);
+
+	/*
+	 * Check that the subbuffer we are trying to consume has been
+	 * already fully committed.
+	 */
+
+	if (((commit_count - rchan->subbuf_size)
+	     & ltt_channel->commit_count_mask)
+	    - (BUFFER_TRUNC(consumed_old, buf->chan)
+	       >> ltt_channel->n_subbufs_order)
+	    != 0)
+		return 0;
+
+	/*
+	 * Check that we are not about to read the same subbuffer in
+	 * which the writer head is.
+	 */
+	if ((SUBBUF_TRUNC(write_offset, buf->chan)
+	   - SUBBUF_TRUNC(consumed_old, buf->chan))
+	   == 0)
+		return 0;
+
+	return 1;
+
+}
+
+/*
+ * returns 0 if reserve ok, or 1 if the slow path must be taken.
+ */
+static __inline__ int ltt_relay_try_reserve(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		size_t data_size,
+		u64 *tsc, unsigned int *rflags, int largest_align,
+		long *o_begin, long *o_end, long *o_old,
+		size_t *before_hdr_pad, size_t *size)
+{
+	*o_begin = local_read(&ltt_buf->offset);
+	*o_old = *o_begin;
+
+	*tsc = trace_clock_read64();
+
+#ifdef CONFIG_LTT_VMCORE
+	prefetch(&ltt_buf->commit_count[SUBBUF_INDEX(*o_begin, rchan)]);
+	prefetch(&ltt_buf->commit_seq[SUBBUF_INDEX(*o_begin, rchan)]);
+#else
+	prefetchw(&ltt_buf->commit_count[SUBBUF_INDEX(*o_begin, rchan)]);
+#endif
+	if (last_tsc_overflow(ltt_buf, *tsc))
+		*rflags = LTT_RFLAG_ID_SIZE_TSC;
+
+	if (unlikely(SUBBUF_OFFSET(*o_begin, buf->chan) == 0))
+		return 1;
+
+	*size = ltt_get_header_size(ltt_channel,
+				*o_begin, data_size,
+				before_hdr_pad, *rflags);
+	*size += ltt_align(*o_begin + *size, largest_align) + data_size;
+	if (unlikely((SUBBUF_OFFSET(*o_begin, buf->chan) + *size)
+		     > buf->chan->subbuf_size))
+		return 1;
+
+	/*
+	 * Event fits in the current buffer and we are not on a switch
+	 * boundary. It's safe to write.
+	 */
+	*o_end = *o_begin + *size;
+
+	if (unlikely((SUBBUF_OFFSET(*o_end, buf->chan)) == 0))
+		/*
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
+		 */
+		return 1;
+
+	return 0;
+}
+
+static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
+		unsigned int *rflags, int largest_align, int cpu)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct rchan_buf *buf = *transport_data = rchan->buf[cpu];
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	long o_begin, o_end, o_old;
+	size_t before_hdr_pad;
+	unsigned int nest;
+
+	raw_local_irq_save(ltt_buf->irqflags);
+
+	/*
+	 * Perform retryable operations.
+	 */
+	nest = __get_cpu_var(ltt_nesting);
+	if (unlikely(nest > 4 || (in_nmi() && nest > 1))) {
+		local_inc(&ltt_buf->events_lost);
+		raw_local_irq_restore(ltt_buf->irqflags);
+		return -EPERM;
+	}
+
+	if (unlikely(ltt_relay_try_reserve(ltt_channel, ltt_buf,
+			rchan, buf, data_size, tsc, rflags,
+			largest_align, &o_begin, &o_end, &o_old,
+			&before_hdr_pad, slot_size)))
+		goto slow_path;
+
+	local_set(&ltt_buf->offset, o_end);
+
+	/*
+	 * Atomically update last_tsc. This update races against concurrent
+	 * atomic updates, but the race will always cause supplementary full TSC
+	 * events, never the opposite (missing a full TSC event when it would be
+	 * needed).
+	 */
+	save_last_tsc(ltt_buf, *tsc);
+
+	/*
+	 * Clear noref flag for this subbuffer.
+	 */
+	ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(o_end - 1, rchan));
+
+	*buf_offset = o_begin + before_hdr_pad;
+	return 0;
+slow_path:
+	return ltt_reserve_slot_irqoff_slow(trace, ltt_channel,
+		transport_data, data_size, slot_size, buf_offset, tsc,
+		rflags, largest_align, cpu);
+}
+
+/*
+ * Force a sub-buffer switch for a per-cpu buffer. This operation is
+ * completely reentrant : can be called while tracing is active with
+ * absolutely no lock held.
+ *
+ * Note, however, that as a local_cmpxchg is used for some atomic
+ * operations, this function must be called from the CPU which owns the buffer
+ * for a ACTIVE flush.
+ */
+static __inline__ void ltt_force_switch(struct rchan_buf *buf,
+		enum force_switch_mode mode)
+{
+	return ltt_force_switch_irqoff_slow(buf, mode);
+}
+
+/*
+ * for flight recording. must be called after relay_commit.
+ * This function decrements de subbuffer's lost_size each time the commit count
+ * reaches back the reserve offset (module subbuffer size). It is useful for
+ * crash dump.
+ */
+#ifdef CONFIG_LTT_VMCORE
+static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
+		struct ltt_channel_buf_struct *ltt_buf,
+		long idx, long buf_offset, long commit_count, size_t data_size)
+{
+	long offset;
+
+	offset = buf_offset + data_size;
+
+	/*
+	 * SUBBUF_OFFSET includes commit_count_mask. We can simply
+	 * compare the offsets within the subbuffer without caring about
+	 * buffer full/empty mismatch because offset is never zero here
+	 * (subbuffer header and event headers have non-zero length).
+	 */
+	if (unlikely(SUBBUF_OFFSET(offset - commit_count, buf->chan)))
+		return;
+
+	local_set(&ltt_buf->commit_seq[idx], commit_count);
+}
+#else
+static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
+		struct ltt_channel_buf_struct *ltt_buf,
+		long idx, long buf_offset, long commit_count, size_t data_size)
+{
+}
+#endif
+
+/*
+ * Atomic unordered slot commit. Increments the commit count in the
+ * specified sub-buffer, and delivers it if necessary.
+ *
+ * Parameters:
+ *
+ * @ltt_channel : channel structure
+ * @transport_data: transport-specific data
+ * @buf_offset : offset following the event header.
+ * @data_size : size of the event data.
+ * @slot_size : size of the reserved slot.
+ */
+static __inline__ void ltt_commit_slot(
+		struct ltt_channel_struct *ltt_channel,
+		void **transport_data, long buf_offset,
+		size_t data_size, size_t slot_size)
+{
+	struct rchan_buf *buf = *transport_data;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct rchan *rchan = buf->chan;
+	long offset_end = buf_offset;
+	long endidx = SUBBUF_INDEX(offset_end - 1, rchan);
+	long commit_count;
+
+#ifdef LTT_NO_IPI_BARRIER
+	smp_wmb();
+#else
+	/*
+	 * Must write slot data before incrementing commit count.
+	 * This compiler barrier is upgraded into a smp_mb() by the IPI
+	 * sent by get_subbuf().
+	 */
+	barrier();
+#endif
+	commit_count = local_read(&ltt_buf->commit_count[endidx]) + slot_size;
+	local_set(&ltt_buf->commit_count[endidx], commit_count);
+
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offset_end - 1, commit_count, endidx);
+	/*
+	 * Update lost_size for each commit. It's needed only for extracting
+	 * ltt buffers from vmcore, after crash.
+	 */
+	ltt_write_commit_counter(buf, ltt_buf, endidx,
+				 buf_offset, commit_count, data_size);
+	raw_local_irq_restore(ltt_buf->irqflags);
+}
+
+#endif //_LTT_LTT_RELAY_IRQOFF_H
diff --git a/stblinux-2.6.31/ltt/ltt-relay-locked.c b/stblinux-2.6.31/ltt/ltt-relay-locked.c
new file mode 100644
index 0000000..a579986
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-relay-locked.c
@@ -0,0 +1,1620 @@
+/*
+ * ltt/ltt-relay-locked.c
+ *
+ * (C) Copyright 2005-2008 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng buffer space management (reader/writer) using spinlock and interrupt
+ * disable.
+ *
+ * Author:
+ *  Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *  Karim Yaghmour (karim@opersys.com)
+ *  Tom Zanussi (zanussi@us.ibm.com)
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  08/10/08, Fork from lockless mechanism, use spinlock and irqoff.
+ *  19/10/05, Complete lockless mechanism.
+ *  27/05/05, Modular redesign and rewrite.
+ *
+ * Userspace reader semantic :
+ * while (poll fd != POLLHUP) {
+ *   - ioctl RELAY_GET_SUBBUF_SIZE
+ *   while (1) {
+ *     - ioctl GET_SUBBUF
+ *     - splice 1 subbuffer worth of data to a pipe
+ *     - splice the data from pipe to disk/network
+ *     - ioctl PUT_SUBBUF, check error value
+ *       if err val < 0, previous subbuffer was corrupted.
+ *   }
+ * }
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/ltt-relay.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/debugfs.h>
+#include <linux/stat.h>
+#include <linux/cpu.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/splice.h>
+#include <linux/spinlock.h>
+
+#include "ltt-relay-locked.h"
+
+#if 0
+#define printk_dbg(fmt, args...) printk(fmt, args)
+#else
+#define printk_dbg(fmt, args...)
+#endif
+
+struct ltt_reserve_switch_offsets {
+	long begin, end, old;
+	long begin_switch, end_switch_current, end_switch_old;
+	long commit_count;
+	size_t before_hdr_pad, size;
+};
+
+static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_chan,
+		struct rchan_buf *buf,
+		unsigned int cpu,
+		unsigned int n_subbufs);
+
+static void ltt_relay_destroy_buffer(struct ltt_channel_struct *ltt_chan,
+		unsigned int cpu);
+
+static void ltt_force_switch(struct rchan_buf *buf,
+		enum force_switch_mode mode);
+
+static const struct file_operations ltt_file_operations;
+
+static void ltt_buffer_begin(struct rchan_buf *buf,
+			u64 tsc, unsigned int subbuf_idx)
+{
+	struct ltt_channel_struct *channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_offset_address(buf,
+				subbuf_idx * buf->chan->subbuf_size);
+
+	header->cycle_count_begin = tsc;
+	header->lost_size = 0xFFFFFFFF; /* for debugging */
+	header->buf_size = buf->chan->subbuf_size;
+	ltt_write_trace_header(channel->trace, header);
+}
+
+/*
+ * offset is assumed to never be 0 here : never deliver a completely empty
+ * subbuffer. The lost size is between 0 and subbuf_size-1.
+ */
+static void ltt_buffer_end(struct rchan_buf *buf,
+		u64 tsc, unsigned int offset, unsigned int subbuf_idx)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_offset_address(buf,
+				subbuf_idx * buf->chan->subbuf_size);
+
+	header->lost_size = SUBBUF_OFFSET((buf->chan->subbuf_size - offset),
+				buf->chan);
+	header->cycle_count_end = tsc;
+	header->events_lost = ltt_buf->events_lost;
+	header->subbuf_corrupt = ltt_buf->corrupted_subbuffers;
+}
+
+static struct dentry *ltt_create_buf_file_callback(const char *filename,
+		struct dentry *parent, int mode,
+		struct rchan_buf *buf)
+{
+	struct ltt_channel_struct *ltt_chan;
+	int err;
+	struct dentry *dentry;
+
+	ltt_chan = buf->chan->private_data;
+	err = ltt_relay_create_buffer(ltt_chan->trace, ltt_chan,
+					buf, buf->cpu,
+					buf->chan->n_subbufs);
+	if (err)
+		return ERR_PTR(err);
+
+	dentry = debugfs_create_file(filename, mode, parent, buf,
+			&ltt_file_operations);
+	if (!dentry)
+		goto error;
+	if (buf->cpu == 0)
+		buf->ascii_dentry = ltt_ascii_create(ltt_chan->trace, ltt_chan);
+	return dentry;
+error:
+	ltt_relay_destroy_buffer(ltt_chan, buf->cpu);
+	return NULL;
+}
+
+static int ltt_remove_buf_file_callback(struct dentry *dentry)
+{
+	struct rchan_buf *buf = dentry->d_inode->i_private;
+	struct ltt_channel_struct *ltt_chan = buf->chan->private_data;
+
+	ltt_ascii_remove(ltt_chan, buf->ascii_dentry);
+	debugfs_remove(dentry);
+	ltt_relay_destroy_buffer(ltt_chan, buf->cpu);
+
+	return 0;
+}
+
+/*
+ * Wake writers :
+ *
+ * This must be done after the trace is removed from the RCU list so that there
+ * are no stalled writers.
+ */
+static void ltt_relay_wake_writers(struct ltt_channel_buf_struct *ltt_buf)
+{
+
+	if (waitqueue_active(&ltt_buf->write_wait))
+		wake_up_interruptible(&ltt_buf->write_wait);
+}
+
+/*
+ * This function should not be called from NMI interrupt context
+ */
+static void ltt_buf_unfull(struct rchan_buf *buf,
+		unsigned int subbuf_idx,
+		long offset)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	ltt_relay_wake_writers(ltt_buf);
+}
+
+/*
+ * Reader API.
+ */
+static unsigned long get_offset(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	return ltt_buf->offset;
+}
+
+static unsigned long get_consumed(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	return ltt_buf->consumed;
+}
+
+static int _ltt_open(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	if (!atomic_long_add_unless(&ltt_buf->active_readers, 1, 1))
+		return -EBUSY;
+	ltt_relay_get_chan(buf->chan);
+	return 0;
+}
+
+static int _ltt_release(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	ltt_relay_put_chan(buf->chan);
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+	atomic_long_dec(&ltt_buf->active_readers);
+	return 0;
+}
+
+static int is_finalized(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	return ltt_buf->finalized;
+}
+
+static int get_subbuf(struct rchan_buf *buf, unsigned long *consumed)
+{
+	struct ltt_channel_struct *ltt_channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	long consumed_old, consumed_idx, commit_count, write_offset;
+	int ret;
+
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+
+	local_irq_disable();
+	__raw_spin_lock(&ltt_buf->lock);
+	consumed_old = ltt_buf->consumed;
+	consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
+	commit_count = ltt_buf->commit_count[consumed_idx];
+	write_offset = ltt_buf->offset;
+	/*
+	 * Check that the subbuffer we are trying to consume has been
+	 * already fully committed.
+	 */
+	if (((commit_count - buf->chan->subbuf_size)
+	     & ltt_channel->commit_count_mask)
+	    - (BUFFER_TRUNC(consumed_old, buf->chan)
+	       >> ltt_channel->n_subbufs_order)
+	    != 0) {
+		__raw_spin_unlock(&ltt_buf->lock);
+		local_irq_enable();
+		return -EAGAIN;
+	}
+	/*
+	 * Check that we are not about to read the same subbuffer in
+	 * which the writer head is.
+	 */
+	if ((SUBBUF_TRUNC(write_offset, buf->chan)
+	   - SUBBUF_TRUNC(consumed_old, buf->chan))
+	   == 0) {
+		__raw_spin_unlock(&ltt_buf->lock);
+		local_irq_enable();
+		return -EAGAIN;
+	}
+
+	ret = update_read_sb_index(buf, consumed_idx);
+	if (ret)
+		return ret;
+
+	__raw_spin_unlock(&ltt_buf->lock);
+	local_irq_enable();
+	*consumed = consumed_old;
+	return 0;
+}
+
+static int put_subbuf(struct rchan_buf *buf, unsigned long consumed)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	long consumed_new, consumed_old;
+
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+
+	local_irq_disable();
+	__raw_spin_lock(&ltt_buf->lock);
+	consumed_old = consumed;
+	consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(buf->rchan_rsb.pages));
+	RCHAN_SB_SET_NOREF(buf->rchan_rsb.pages);
+
+	if (ltt_buf->consumed != consumed_old) {
+		/* We have been pushed by the writer. */
+		__raw_spin_unlock(&ltt_buf->lock);
+		local_irq_enable();
+		/*
+		 * We exchanged the subbuffer pages. No corruption possible
+		 * even if the writer did push us. No more -EIO possible.
+		 */
+		return 0;
+	} else {
+		/* tell the client that buffer is now unfull */
+		int index;
+		long data;
+
+		ltt_buf->consumed = consumed_new;
+		index = SUBBUF_INDEX(consumed_old, buf->chan);
+		data = BUFFER_OFFSET(consumed_old, buf->chan);
+		ltt_buf_unfull(buf, index, data);
+		__raw_spin_unlock(&ltt_buf->lock);
+		local_irq_enable();
+	}
+	return 0;
+}
+
+static unsigned long get_n_subbufs(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+	return buf->chan->n_subbufs;
+}
+
+static unsigned long get_subbuf_size(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+	return buf->chan->subbuf_size;
+}
+
+static void switch_buffer(unsigned long data)
+{
+	struct ltt_channel_buf_struct *ltt_buf =
+		(struct ltt_channel_buf_struct *)data;
+	struct rchan_buf *buf = ltt_buf->rbuf;
+
+	if (buf)
+		ltt_force_switch(buf, FORCE_ACTIVE);
+
+	ltt_buf->switch_timer.expires += ltt_buf->switch_timer_interval;
+	add_timer_on(&ltt_buf->switch_timer, smp_processor_id());
+}
+
+static void start_switch_timer(struct ltt_channel_struct *ltt_channel)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	int cpu;
+
+	if (!ltt_channel->switch_timer_interval)
+		return;
+
+	// TODO : hotplug
+	for_each_online_cpu(cpu) {
+		struct ltt_channel_buf_struct *ltt_buf;
+		struct rchan_buf *buf;
+
+		buf = rchan->buf[cpu];
+		ltt_buf = buf->chan_private;
+		buf->random_access = 1;
+		ltt_buf->switch_timer_interval =
+			ltt_channel->switch_timer_interval;
+		init_timer(&ltt_buf->switch_timer);
+		ltt_buf->switch_timer.function = switch_buffer;
+		ltt_buf->switch_timer.expires = jiffies +
+					ltt_buf->switch_timer_interval;
+		ltt_buf->switch_timer.data = (unsigned long)ltt_buf;
+		add_timer_on(&ltt_buf->switch_timer, cpu);
+	}
+}
+
+/*
+ * Cannot use del_timer_sync with add_timer_on, so use an IPI to locally
+ * delete the timer.
+ */
+static void stop_switch_timer_ipi(void *info)
+{
+	struct ltt_channel_buf_struct *ltt_buf =
+		(struct ltt_channel_buf_struct *)info;
+
+	del_timer(&ltt_buf->switch_timer);
+}
+
+static void stop_switch_timer(struct ltt_channel_struct *ltt_channel)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	int cpu;
+
+	if (!ltt_channel->switch_timer_interval)
+		return;
+
+	// TODO : hotplug
+	for_each_online_cpu(cpu) {
+		struct ltt_channel_buf_struct *ltt_buf;
+		struct rchan_buf *buf;
+
+		buf = rchan->buf[cpu];
+		ltt_buf = buf->chan_private;
+		smp_call_function(stop_switch_timer_ipi, ltt_buf, 1);
+		buf->random_access = 0;
+	}
+}
+
+static struct ltt_channel_buf_access_ops ltt_channel_buf_accessor = {
+	.get_offset   = get_offset,
+	.get_consumed = get_consumed,
+	.get_subbuf = get_subbuf,
+	.put_subbuf = put_subbuf,
+	.is_finalized = is_finalized,
+	.get_n_subbufs = get_n_subbufs,
+	.get_subbuf_size = get_subbuf_size,
+	.open = _ltt_open,
+	.release = _ltt_release,
+	.start_switch_timer = start_switch_timer,
+	.stop_switch_timer = stop_switch_timer,
+};
+
+/**
+ *	ltt_open - open file op for ltt files
+ *	@inode: opened inode
+ *	@file: opened file
+ *
+ *	Open implementation. Makes sure only one open instance of a buffer is
+ *	done at a given moment.
+ */
+static int ltt_open(struct inode *inode, struct file *file)
+{
+	int ret;
+	struct rchan_buf *buf = inode->i_private;
+
+	ret = _ltt_open(buf);
+	if (!ret)
+		ret = ltt_relay_file_operations.open(inode, file);
+	return ret;
+}
+
+/**
+ *	ltt_release - release file op for ltt files
+ *	@inode: opened inode
+ *	@file: opened file
+ *
+ *	Release implementation.
+ */
+static int ltt_release(struct inode *inode, struct file *file)
+{
+	struct rchan_buf *buf = inode->i_private;
+	int ret;
+
+	_ltt_release(buf);
+	ret = ltt_relay_file_operations.release(inode, file);
+	WARN_ON(ret);
+	return ret;
+}
+
+/**
+ *	ltt_poll - file op for ltt files
+ *	@filp: the file
+ *	@wait: poll table
+ *
+ *	Poll implementation.
+ */
+static unsigned int ltt_poll(struct file *filp, poll_table *wait)
+{
+	unsigned int mask = 0, ret;
+	struct inode *inode = filp->f_dentry->d_inode;
+	struct rchan_buf *buf = inode->i_private;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	if (filp->f_mode & FMODE_READ) {
+		poll_wait_set_exclusive(wait);
+		poll_wait(filp, &ltt_buf->read_wait, wait);
+
+		local_irq_disable();
+		__raw_spin_lock(&ltt_buf->lock);
+		WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+		if (SUBBUF_TRUNC(ltt_buf->offset, buf->chan)
+		  - SUBBUF_TRUNC(ltt_buf->consumed, buf->chan)
+		  == 0) {
+			if (ltt_buf->finalized)
+				ret = POLLHUP;
+			else
+				ret = 0;
+			goto end;
+		} else {
+			struct rchan *rchan = buf->chan;
+
+			if (SUBBUF_TRUNC(ltt_buf->offset, buf->chan)
+			  - SUBBUF_TRUNC(ltt_buf->consumed, buf->chan)
+			  >= rchan->alloc_size)
+				ret = POLLPRI | POLLRDBAND;
+			else
+				ret = POLLIN | POLLRDNORM;
+			goto end;
+		}
+end:
+		__raw_spin_unlock(&ltt_buf->lock);
+		local_irq_enable();
+		return ret;
+	}
+	return mask;
+}
+
+/**
+ *	ltt_ioctl - control on the debugfs file
+ *
+ *	@inode: the inode
+ *	@filp: the file
+ *	@cmd: the command
+ *	@arg: command arg
+ *
+ *	This ioctl implements three commands necessary for a minimal
+ *	producer/consumer implementation :
+ *	RELAY_GET_SUBBUF
+ *		Get the next sub buffer that can be read. It never blocks.
+ *	RELAY_PUT_SUBBUF
+ *		Release the currently read sub-buffer. Parameter is the last
+ *		put subbuffer (returned by GET_SUBBUF).
+ *	RELAY_GET_N_BUBBUFS
+ *		returns the number of sub buffers in the per cpu channel.
+ *	RELAY_GET_SUBBUF_SIZE
+ *		returns the size of the sub buffers.
+ */
+static int ltt_ioctl(struct inode *inode, struct file *filp,
+		unsigned int cmd, unsigned long arg)
+{
+	struct rchan_buf *buf = inode->i_private;
+	u32 __user *argp = (u32 __user *)arg;
+
+	switch (cmd) {
+	case RELAY_GET_SUBBUF:
+	{
+		unsigned long consumed;
+		int ret;
+
+		ret = get_subbuf(buf, &consumed);
+		if (ret)
+			return ret;
+		else
+			return put_user((u32)consumed, argp);
+		break;
+	}
+	case RELAY_PUT_SUBBUF:
+	{
+		struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+		u32 uconsumed_old;
+		int ret;
+		long consumed_old;
+
+		ret = get_user(uconsumed_old, argp);
+		if (ret)
+			return ret; /* will return -EFAULT */
+
+		consumed_old = ltt_buf->consumed;
+		consumed_old = consumed_old & (~0xFFFFFFFFL);
+		consumed_old = consumed_old | uconsumed_old;
+		ret = put_subbuf(buf, consumed_old);
+		if (ret)
+			return ret;
+		break;
+	}
+	case RELAY_GET_N_SUBBUFS:
+		return put_user((u32)get_n_subbufs(buf), argp);
+		break;
+	case RELAY_GET_SUBBUF_SIZE:
+		return put_user((u32)get_subbuf_size(buf), argp);
+		break;
+	default:
+		return -ENOIOCTLCMD;
+	}
+	return 0;
+}
+
+#ifdef CONFIG_COMPAT
+static long ltt_compat_ioctl(struct file *file, unsigned int cmd,
+		unsigned long arg)
+{
+	long ret = -ENOIOCTLCMD;
+
+	lock_kernel();
+	ret = ltt_ioctl(file->f_dentry->d_inode, file, cmd, arg);
+	unlock_kernel();
+
+	return ret;
+}
+#endif
+
+static void ltt_relay_pipe_buf_release(struct pipe_inode_info *pipe,
+				   struct pipe_buffer *pbuf)
+{
+}
+
+static struct pipe_buf_operations ltt_relay_pipe_buf_ops = {
+	.can_merge = 0,
+	.map = generic_pipe_buf_map,
+	.unmap = generic_pipe_buf_unmap,
+	.confirm = generic_pipe_buf_confirm,
+	.release = ltt_relay_pipe_buf_release,
+	.steal = generic_pipe_buf_steal,
+	.get = generic_pipe_buf_get,
+};
+
+static void ltt_relay_page_release(struct splice_pipe_desc *spd, unsigned int i)
+{
+}
+
+/*
+ *	subbuf_splice_actor - splice up to one subbuf's worth of data
+ */
+static int subbuf_splice_actor(struct file *in,
+			       loff_t *ppos,
+			       struct pipe_inode_info *pipe,
+			       size_t len,
+			       unsigned int flags)
+{
+	struct rchan_buf *buf = in->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	unsigned int poff, subbuf_pages, nr_pages;
+	struct page *pages[PIPE_BUFFERS];
+	struct partial_page partial[PIPE_BUFFERS];
+	struct splice_pipe_desc spd = {
+		.pages = pages,
+		.nr_pages = 0,
+		.partial = partial,
+		.flags = flags,
+		.ops = &ltt_relay_pipe_buf_ops,
+		.spd_release = ltt_relay_page_release,
+	};
+	long consumed_old, consumed_idx, roffset;
+	unsigned long bytes_avail;
+
+	/*
+	 * Check that a GET_SUBBUF ioctl has been done before.
+	 */
+	local_irq_disable();
+	__raw_spin_lock(&ltt_buf->lock);
+	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+	consumed_old = ltt_buf->consumed;
+	consumed_old += *ppos;
+	consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
+
+	/*
+	 * Adjust read len, if longer than what is available.
+	 * Max read size is 1 subbuffer due to get_subbuf/put_subbuf for
+	 * protection.
+	 */
+	bytes_avail = buf->chan->subbuf_size;
+	WARN_ON(bytes_avail > buf->chan->alloc_size);
+	len = min_t(size_t, len, bytes_avail);
+	subbuf_pages = bytes_avail >> PAGE_SHIFT;
+	nr_pages = min_t(unsigned int, subbuf_pages, PIPE_BUFFERS);
+	roffset = consumed_old & PAGE_MASK;
+	poff = consumed_old & ~PAGE_MASK;
+	printk_dbg(KERN_DEBUG "SPLICE actor len %zu pos %zd write_pos %ld\n",
+		len, (ssize_t)*ppos, ltt_buf->offset);
+
+	for (; spd.nr_pages < nr_pages; spd.nr_pages++) {
+		unsigned int this_len;
+		struct page *page;
+
+		if (!len)
+			break;
+		printk_dbg(KERN_DEBUG "SPLICE actor loop len %zu roffset %ld\n",
+			len, roffset);
+
+		this_len = PAGE_SIZE - poff;
+		page = ltt_relay_read_get_page(buf, roffset);
+		spd.pages[spd.nr_pages] = page;
+		spd.partial[spd.nr_pages].offset = poff;
+		spd.partial[spd.nr_pages].len = this_len;
+
+		poff = 0;
+		roffset += PAGE_SIZE;
+		len -= this_len;
+	}
+	__raw_spin_unlock(&ltt_buf->lock);
+	local_irq_enable();
+
+	if (!spd.nr_pages)
+		return 0;
+
+	return splice_to_pipe(pipe, &spd);
+}
+
+static ssize_t ltt_relay_file_splice_read(struct file *in,
+				      loff_t *ppos,
+				      struct pipe_inode_info *pipe,
+				      size_t len,
+				      unsigned int flags)
+{
+	ssize_t spliced;
+	int ret;
+
+	ret = 0;
+	spliced = 0;
+
+	printk_dbg(KERN_DEBUG "SPLICE read len %zu pos %zd\n",
+		len, (ssize_t)*ppos);
+	while (len && !spliced) {
+		ret = subbuf_splice_actor(in, ppos, pipe, len, flags);
+		printk_dbg(KERN_DEBUG "SPLICE read loop ret %d\n", ret);
+		if (ret < 0)
+			break;
+		else if (!ret) {
+			if (flags & SPLICE_F_NONBLOCK)
+				ret = -EAGAIN;
+			break;
+		}
+
+		*ppos += ret;
+		if (ret > len)
+			len = 0;
+		else
+			len -= ret;
+		spliced += ret;
+	}
+
+	if (spliced)
+		return spliced;
+
+	return ret;
+}
+
+static void ltt_relay_print_subbuffer_errors(
+		struct ltt_channel_struct *ltt_chan,
+		long cons_off, unsigned int cpu)
+{
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
+	long cons_idx, commit_count, write_offset;
+
+	cons_idx = SUBBUF_INDEX(cons_off, rchan);
+	commit_count = ltt_buf->commit_count[cons_idx];
+	write_offset = ltt_buf->offset;
+	printk(KERN_WARNING
+		"LTT : unread channel %s offset is %ld "
+		"and cons_off : %ld (cpu %u)\n",
+		ltt_chan->channel_name, write_offset, cons_off, cpu);
+	/* Check each sub-buffer for non filled commit count */
+	if (((commit_count - rchan->subbuf_size) & ltt_chan->commit_count_mask)
+	    - (BUFFER_TRUNC(cons_off, rchan) >> ltt_chan->n_subbufs_order)
+	    != 0)
+		printk(KERN_ALERT
+			"LTT : %s : subbuffer %lu has non filled "
+			"commit count %lu.\n",
+			ltt_chan->channel_name, cons_idx, commit_count);
+	printk(KERN_ALERT "LTT : %s : commit count : %lu, subbuf size %zd\n",
+			ltt_chan->channel_name, commit_count,
+			rchan->subbuf_size);
+}
+
+static void ltt_relay_print_errors(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_chan, int cpu)
+{
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
+	long cons_off;
+
+	/*
+	 * Can be called in the error path of allocation when
+	 * trans_channel_data is not yet set.
+	 */
+	if (!rchan)
+		return;
+	for (cons_off = ltt_buf->consumed;
+			(SUBBUF_TRUNC(ltt_buf->offset, rchan) - cons_off) > 0;
+			cons_off = SUBBUF_ALIGN(cons_off, rchan))
+		ltt_relay_print_subbuffer_errors(ltt_chan, cons_off, cpu);
+}
+
+static void ltt_relay_print_buffer_errors(struct ltt_channel_struct *ltt_chan,
+		unsigned int cpu)
+{
+	struct ltt_trace_struct *trace = ltt_chan->trace;
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
+
+	if (ltt_buf->events_lost)
+		printk(KERN_ALERT
+			"LTT : %s : %ld events lost "
+			"in %s channel (cpu %u).\n",
+			ltt_chan->channel_name,
+			ltt_buf->events_lost,
+			ltt_chan->channel_name, cpu);
+	if (ltt_buf->corrupted_subbuffers)
+		printk(KERN_ALERT
+			"LTT : %s : %ld corrupted subbuffers "
+			"in %s channel (cpu %u).\n",
+			ltt_chan->channel_name,
+			ltt_buf->corrupted_subbuffers,
+			ltt_chan->channel_name, cpu);
+
+	ltt_relay_print_errors(trace, ltt_chan, cpu);
+}
+
+static void ltt_relay_remove_dirs(struct ltt_trace_struct *trace)
+{
+	ltt_ascii_remove_dir(trace);
+	debugfs_remove(trace->dentry.trace_root);
+}
+
+/*
+ * Create ltt buffer.
+ */
+static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_chan, struct rchan_buf *buf,
+		unsigned int cpu, unsigned int n_subbufs)
+{
+	struct ltt_channel_buf_struct *ltt_buf;
+	ltt_buf = kzalloc_node(sizeof(*ltt_buf), GFP_KERNEL, cpu_to_node(cpu));
+	if (!ltt_buf)
+		return -ENOMEM;
+
+	ltt_buf->commit_count =
+		kzalloc_node(ALIGN(sizeof(*ltt_buf->commit_count) * n_subbufs,
+				   1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(cpu));
+	if (!ltt_buf->commit_count) {
+		kfree(ltt_buf);
+		return -ENOMEM;
+	}
+
+#ifdef CONFIG_LTT_VMCORE
+	ltt_buf->commit_seq =
+		kzalloc_node(ALIGN(sizeof(*ltt_buf->commit_seq) * n_subbufs,
+				   1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(cpu));
+	if (!ltt_buf->commit_seq) {
+		kfree(ltt_buf->commit_count);
+		kfree(ltt_buf);
+		return -ENOMEM;
+	}
+#endif
+
+	buf->chan_private = ltt_buf;
+
+	kref_get(&trace->kref);
+	kref_get(&trace->ltt_transport_kref);
+	ltt_buf->offset = ltt_subbuffer_header_size();
+	atomic_long_set(&ltt_buf->active_readers, 0);
+	init_waitqueue_head(&ltt_buf->write_wait);
+	init_waitqueue_head(&ltt_buf->read_wait);
+
+	RCHAN_SB_CLEAR_NOREF(buf->rchan_wsb[0].pages);
+	ltt_buffer_begin(buf, trace->start_tsc, 0);
+	ltt_buf->commit_count[0] += ltt_subbuffer_header_size();
+	ltt_buf->lock = (raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
+	ltt_buf->rbuf = buf;
+
+	return 0;
+}
+
+static void ltt_relay_destroy_buffer(struct ltt_channel_struct *ltt_chan,
+		unsigned int cpu)
+{
+	struct ltt_trace_struct *trace = ltt_chan->trace;
+	struct rchan *rchan = ltt_chan->trans_channel_data;
+	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
+
+	kref_put(&ltt_chan->trace->ltt_transport_kref,
+		ltt_release_transport);
+	ltt_relay_print_buffer_errors(ltt_chan, cpu);
+#ifdef CONFIG_LTT_VMCORE
+	kfree(ltt_buf->commit_seq);
+#endif
+	kfree(ltt_buf->commit_count);
+	kfree(ltt_buf);
+	kref_put(&trace->kref, ltt_release_trace);
+	wake_up_interruptible(&trace->kref_wq);
+}
+
+/*
+ * Create channel.
+ */
+static int ltt_relay_create_channel(const char *trace_name,
+		struct ltt_trace_struct *trace, struct dentry *dir,
+		const char *channel_name, struct ltt_channel_struct *ltt_chan,
+		unsigned int subbuf_size, unsigned int n_subbufs,
+		int overwrite)
+{
+	char *tmpname;
+	unsigned int tmpname_len;
+	int err = 0;
+
+	tmpname = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (!tmpname)
+		return EPERM;
+	if (overwrite) {
+		strncpy(tmpname, LTT_FLIGHT_PREFIX, PATH_MAX-1);
+		strncat(tmpname, channel_name,
+			PATH_MAX-1-sizeof(LTT_FLIGHT_PREFIX));
+	} else {
+		strncpy(tmpname, channel_name, PATH_MAX-1);
+	}
+	strncat(tmpname, "_", PATH_MAX-1-strlen(tmpname));
+
+	ltt_chan->trace = trace;
+	ltt_chan->overwrite = overwrite;
+	ltt_chan->n_subbufs_order = get_count_order(n_subbufs);
+	ltt_chan->commit_count_mask = (~0UL >> ltt_chan->n_subbufs_order);
+	ltt_chan->trans_channel_data = ltt_relay_open(tmpname,
+			dir,
+			subbuf_size,
+			n_subbufs,
+			&trace->callbacks,
+			ltt_chan,
+			overwrite);
+	tmpname_len = strlen(tmpname);
+	if (tmpname_len > 0) {
+		/* Remove final _ for pretty printing */
+		tmpname[tmpname_len-1] = '\0';
+	}
+	if (ltt_chan->trans_channel_data == NULL) {
+		printk(KERN_ERR "LTT : Can't open %s channel for trace %s\n",
+				tmpname, trace_name);
+		goto relay_open_error;
+	}
+
+	ltt_chan->buf_access_ops = &ltt_channel_buf_accessor;
+
+	err = 0;
+	goto end;
+
+relay_open_error:
+	err = EPERM;
+end:
+	kfree(tmpname);
+	return err;
+}
+
+static int ltt_relay_create_dirs(struct ltt_trace_struct *new_trace)
+{
+	struct dentry *ltt_root_dentry;
+	int ret;
+
+	ltt_root_dentry = get_ltt_root();
+	if (!ltt_root_dentry)
+		return ENOENT;
+
+	new_trace->dentry.trace_root = debugfs_create_dir(new_trace->trace_name,
+			ltt_root_dentry);
+	put_ltt_root();
+	if (new_trace->dentry.trace_root == NULL) {
+		printk(KERN_ERR "LTT : Trace directory name %s already taken\n",
+				new_trace->trace_name);
+		return EEXIST;
+	}
+	ret = ltt_ascii_create_dir(new_trace);
+	if (ret)
+		printk(KERN_WARNING "LTT : Unable to create ascii output file "
+				    "for trace %s\n", new_trace->trace_name);
+
+	new_trace->callbacks.create_buf_file = ltt_create_buf_file_callback;
+	new_trace->callbacks.remove_buf_file = ltt_remove_buf_file_callback;
+
+	return 0;
+}
+
+/*
+ * LTTng channel flush function.
+ *
+ * Must be called when no tracing is active in the channel, because of
+ * accesses across CPUs.
+ */
+static void ltt_relay_buffer_flush(struct rchan_buf *buf)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+
+	ltt_buf->finalized = 1;
+	ltt_force_switch(buf, FORCE_FLUSH);
+}
+
+static void ltt_relay_async_wakeup_chan(struct ltt_channel_struct *ltt_channel)
+{
+	unsigned int i;
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+
+	for_each_possible_cpu(i) {
+		struct ltt_channel_buf_struct *ltt_buf;
+
+		if (!rchan->buf[i])
+			continue;
+
+		ltt_buf = rchan->buf[i]->chan_private;
+		if (ltt_buf->wakeup_readers == 1) {
+			ltt_buf->wakeup_readers = 0;
+			wake_up_interruptible(&ltt_buf->read_wait);
+		}
+	}
+}
+
+static void ltt_relay_finish_buffer(struct ltt_channel_struct *ltt_channel,
+		unsigned int cpu)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+
+	if (rchan->buf[cpu]) {
+		struct ltt_channel_buf_struct *ltt_buf =
+				rchan->buf[cpu]->chan_private;
+		ltt_relay_buffer_flush(rchan->buf[cpu]);
+		ltt_relay_wake_writers(ltt_buf);
+	}
+}
+
+
+static void ltt_relay_finish_channel(struct ltt_channel_struct *ltt_channel)
+{
+	unsigned int i;
+
+	for_each_possible_cpu(i)
+		ltt_relay_finish_buffer(ltt_channel, i);
+}
+
+static void ltt_relay_remove_channel(struct ltt_channel_struct *channel)
+{
+	struct rchan *rchan = channel->trans_channel_data;
+
+	ltt_relay_close(rchan);
+}
+
+/*
+ * This is called with preemption disabled when user space has requested
+ * blocking mode.  If one of the active traces has free space below a
+ * specific threshold value, we reenable preemption and block.
+ */
+static int ltt_relay_user_blocking(struct ltt_trace_struct *trace,
+		unsigned int chan_index, size_t data_size,
+		struct user_dbg_data *dbg)
+{
+	struct rchan *rchan;
+	struct ltt_channel_buf_struct *ltt_buf;
+	struct ltt_channel_struct *channel;
+	struct rchan_buf *relay_buf;
+	int cpu;
+	DECLARE_WAITQUEUE(wait, current);
+
+	channel = &trace->channels[chan_index];
+	rchan = channel->trans_channel_data;
+	cpu = smp_processor_id();
+	relay_buf = rchan->buf[cpu];
+	ltt_buf = relay_buf->chan_private;
+
+	/*
+	 * Check if data is too big for the channel : do not
+	 * block for it.
+	 */
+	if (LTT_RESERVE_CRITICAL + data_size > relay_buf->chan->subbuf_size)
+		return 0;
+
+	/*
+	 * If free space too low, we block. We restart from the
+	 * beginning after we resume (cpu id may have changed
+	 * while preemption is active).
+	 */
+	local_irq_disable();
+	__raw_spin_lock(&ltt_buf->lock);
+	if (!channel->overwrite) {
+		dbg->write = ltt_buf->offset;
+		dbg->read = ltt_buf->consumed;
+		dbg->avail_size = dbg->write + LTT_RESERVE_CRITICAL + data_size
+				  - SUBBUF_TRUNC(dbg->read,
+						 relay_buf->chan);
+		if (dbg->avail_size > rchan->alloc_size) {
+			__set_current_state(TASK_INTERRUPTIBLE);
+			add_wait_queue(&ltt_buf->write_wait, &wait);
+			__raw_spin_unlock(&ltt_buf->lock);
+			local_irq_enable();
+			preempt_enable();
+			schedule();
+			__set_current_state(TASK_RUNNING);
+			remove_wait_queue(&ltt_buf->write_wait, &wait);
+			if (signal_pending(current))
+				return -ERESTARTSYS;
+			preempt_disable();
+			return 1;
+		}
+	}
+	__raw_spin_unlock(&ltt_buf->lock);
+	local_irq_enable();
+	return 0;
+}
+
+static void ltt_relay_print_user_errors(struct ltt_trace_struct *trace,
+		unsigned int chan_index, size_t data_size,
+		struct user_dbg_data *dbg, int cpu)
+{
+	struct rchan *rchan;
+	struct ltt_channel_buf_struct *ltt_buf;
+	struct ltt_channel_struct *channel;
+	struct rchan_buf *relay_buf;
+
+	channel = &trace->channels[chan_index];
+	rchan = channel->trans_channel_data;
+	relay_buf = rchan->buf[cpu];
+	ltt_buf = relay_buf->chan_private;
+
+	printk(KERN_ERR "Error in LTT usertrace : "
+	"buffer full : event lost in blocking "
+	"mode. Increase LTT_RESERVE_CRITICAL.\n");
+	printk(KERN_ERR "LTT nesting level is %u.\n",
+		per_cpu(ltt_nesting, cpu));
+	printk(KERN_ERR "LTT avail size %lu.\n",
+		dbg->avail_size);
+	printk(KERN_ERR "avai write : %lu, read : %lu\n",
+			dbg->write, dbg->read);
+
+	dbg->write = ltt_buf->offset;
+	dbg->read = ltt_buf->consumed;
+
+	printk(KERN_ERR "LTT cur size %lu.\n",
+		dbg->write + LTT_RESERVE_CRITICAL + data_size
+		- SUBBUF_TRUNC(dbg->read, relay_buf->chan));
+	printk(KERN_ERR "cur write : %lu, read : %lu\n",
+			dbg->write, dbg->read);
+}
+
+static void ltt_reserve_push_reader(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets)
+{
+	long consumed_old, consumed_new;
+
+	consumed_old = ltt_buf->consumed;
+	/*
+	 * If buffer is in overwrite mode, push the reader consumed
+	 * count if the write position has reached it and we are not
+	 * at the first iteration (don't push the reader farther than
+	 * the writer). This operation can be done concurrently by many
+	 * writers in the same buffer, the writer being at the farthest
+	 * write position sub-buffer index in the buffer being the one
+	 * which will win this loop.
+	 * If the buffer is not in overwrite mode, pushing the reader
+	 * only happens if a sub-buffer is corrupted.
+	 */
+	if (unlikely((SUBBUF_TRUNC(offsets->end-1, buf->chan)
+	   - SUBBUF_TRUNC(consumed_old, buf->chan))
+	   >= rchan->alloc_size)) {
+		consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+		ltt_buf->consumed = consumed_new;
+	} else
+		return;
+}
+
+/*
+ * ltt_reserve_switch_old_subbuf: switch old subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ *
+ * Note : offset_old should never be 0 here.
+ */
+static void ltt_reserve_switch_old_subbuf(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long oldidx = SUBBUF_INDEX(offsets->old - 1, rchan);
+
+	ltt_buffer_end(buf, *tsc, offsets->old, oldidx);
+	ltt_buf->commit_count[oldidx] +=
+		rchan->subbuf_size
+		- (SUBBUF_OFFSET(offsets->old - 1, rchan)
+		+ 1);
+	offsets->commit_count = ltt_buf->commit_count[oldidx];
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+			  offsets->old - 1, offsets->commit_count, oldidx);
+}
+
+/*
+ * ltt_reserve_switch_new_subbuf: Populate new subbuffer.
+ *
+ * This code can be executed unordered : writers may already have written to the
+ * sub-buffer before this code gets executed, caution.  The commit makes sure
+ * that this code is executed before the deliver of this sub-buffer.
+ */
+static void ltt_reserve_switch_new_subbuf(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long beginidx = SUBBUF_INDEX(offsets->begin, rchan);
+
+	ltt_buffer_begin(buf, *tsc, beginidx);
+	ltt_buf->commit_count[beginidx] += ltt_subbuffer_header_size();
+	offsets->commit_count = ltt_buf->commit_count[beginidx];
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+			  offsets->begin, offsets->commit_count, beginidx);
+}
+
+/*
+ * ltt_reserve_end_switch_current: finish switching current subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ */
+static void ltt_reserve_end_switch_current(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long endidx = SUBBUF_INDEX(offsets->end - 1, rchan);
+
+	ltt_buffer_end(buf, *tsc, offsets->end, endidx);
+	ltt_buf->commit_count[endidx] +=
+		rchan->subbuf_size
+		- (SUBBUF_OFFSET(offsets->end - 1, rchan)
+		+ 1);
+	offsets->commit_count = ltt_buf->commit_count[endidx];
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+			  offsets->end - 1, offsets->commit_count, endidx);
+}
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static int ltt_relay_try_switch_slow(
+		enum force_switch_mode mode,
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets,
+		u64 *tsc)
+{
+	long subbuf_index;
+	long reserve_commit_diff;
+
+	offsets->begin = ltt_buf->offset;
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_old = 0;
+
+	*tsc = trace_clock_read64();
+
+	if (SUBBUF_OFFSET(offsets->begin, buf->chan) != 0) {
+		offsets->begin = SUBBUF_ALIGN(offsets->begin, buf->chan);
+		offsets->end_switch_old = 1;
+	} else {
+		/* we do not have to switch : buffer is empty */
+		return -1;
+	}
+	if (mode == FORCE_ACTIVE)
+		offsets->begin += ltt_subbuffer_header_size();
+	/*
+	 * Always begin_switch in FORCE_ACTIVE mode.
+	 * Test new buffer integrity
+	 */
+	subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
+	reserve_commit_diff =
+		(BUFFER_TRUNC(offsets->begin, buf->chan)
+		 >> ltt_channel->n_subbufs_order)
+		- (ltt_buf->commit_count[subbuf_index]
+		   & ltt_channel->commit_count_mask);
+	if (reserve_commit_diff == 0) {
+		/* Next buffer not corrupted. */
+		if (mode == FORCE_ACTIVE
+		    && !ltt_channel->overwrite
+		    && offsets->begin - ltt_buf->consumed
+		       >= rchan->alloc_size) {
+			/*
+			 * We do not overwrite non consumed buffers and we are
+			 * full : ignore switch while tracing is active.
+			 */
+			return -1;
+		}
+	} else {
+		/*
+		 * Next subbuffer corrupted. Force pushing reader even in normal
+		 * mode
+		 */
+	}
+	offsets->end = offsets->begin;
+	return 0;
+}
+
+/*
+ * Force a sub-buffer switch for a per-cpu buffer. This operation is
+ * completely reentrant : can be called while tracing is active with
+ * absolutely no lock held.
+ */
+void ltt_force_switch_locked_slow(struct rchan_buf *buf,
+		enum force_switch_mode mode)
+{
+	struct ltt_channel_struct *ltt_channel =
+			(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct ltt_reserve_switch_offsets offsets;
+	unsigned long flags;
+	u64 tsc;
+
+	offsets.size = 0;
+
+	raw_local_irq_save(flags);
+	__raw_spin_lock(&ltt_buf->lock);
+
+	/*
+	 * Perform retryable operations.
+	 */
+	if (ltt_relay_try_switch_slow(mode, ltt_channel, ltt_buf,
+			rchan, buf, &offsets, &tsc)) {
+		__raw_spin_unlock(&ltt_buf->lock);
+		raw_local_irq_restore(flags);
+		return;
+	}
+	ltt_buf->offset = offsets.end;
+
+	save_last_tsc(ltt_buf, tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	if (mode == FORCE_ACTIVE) {
+		ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan,
+					buf, &offsets);
+		ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.end - 1,
+							      rchan));
+	}
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (offsets.end_switch_old) {
+		ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.old - 1,
+							      rchan));
+		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
+			&offsets, &tsc);
+	}
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (mode == FORCE_ACTIVE)
+		ltt_reserve_switch_new_subbuf(ltt_channel,
+			ltt_buf, rchan, buf, &offsets, &tsc);
+
+	__raw_spin_unlock(&ltt_buf->lock);
+	raw_local_irq_restore(flags);
+}
+EXPORT_SYMBOL_GPL(ltt_force_switch_locked_slow);
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static int ltt_relay_try_reserve_slow(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, size_t data_size,
+		u64 *tsc, unsigned int *rflags, int largest_align)
+{
+	long reserve_commit_diff;
+
+	offsets->begin = ltt_buf->offset;
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_current = 0;
+	offsets->end_switch_old = 0;
+
+	*tsc = trace_clock_read64();
+	if (last_tsc_overflow(ltt_buf, *tsc))
+		*rflags = LTT_RFLAG_ID_SIZE_TSC;
+
+	if (unlikely(SUBBUF_OFFSET(offsets->begin, buf->chan) == 0)) {
+		offsets->begin_switch = 1;		/* For offsets->begin */
+	} else {
+		offsets->size = ltt_get_header_size(ltt_channel,
+					offsets->begin, data_size,
+					&offsets->before_hdr_pad, *rflags);
+		offsets->size += ltt_align(offsets->begin + offsets->size,
+					   largest_align)
+				 + data_size;
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan) +
+			      offsets->size) > buf->chan->subbuf_size)) {
+			offsets->end_switch_old = 1;	/* For offsets->old */
+			offsets->begin_switch = 1;	/* For offsets->begin */
+		}
+	}
+	if (unlikely(offsets->begin_switch)) {
+		long subbuf_index;
+
+		/*
+		 * We are typically not filling the previous buffer completely.
+		 */
+		if (likely(offsets->end_switch_old))
+			offsets->begin = SUBBUF_ALIGN(offsets->begin,
+						      buf->chan);
+		offsets->begin = offsets->begin + ltt_subbuffer_header_size();
+		/* Test new buffer integrity */
+		subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
+		reserve_commit_diff =
+			(BUFFER_TRUNC(offsets->begin, buf->chan)
+			 >> ltt_channel->n_subbufs_order)
+			- (ltt_buf->commit_count[subbuf_index]
+			   & ltt_channel->commit_count_mask);
+		if (likely(reserve_commit_diff == 0)) {
+			/* Next buffer not corrupted. */
+			if (unlikely(!ltt_channel->overwrite &&
+				(SUBBUF_TRUNC(offsets->begin, buf->chan)
+				- SUBBUF_TRUNC(ltt_buf->consumed, buf->chan))
+				>= rchan->alloc_size)) {
+				/*
+				 * We do not overwrite non consumed buffers
+				 * and we are full : event is lost.
+				 */
+				ltt_buf->events_lost++;
+				return -1;
+			} else {
+				/*
+				 * next buffer not corrupted, we are either in
+				 * overwrite mode or the buffer is not full.
+				 * It's safe to write in this new subbuffer.
+				 */
+			}
+		} else {
+			/*
+			 * Next subbuffer corrupted. Drop event in normal and
+			 * overwrite mode. Caused by either a writer OOPS or
+			 * too many nested writes over a reserve/commit pair.
+			 */
+			ltt_buf->events_lost++;
+			return -1;
+		}
+		offsets->size = ltt_get_header_size(ltt_channel,
+					offsets->begin, data_size,
+					&offsets->before_hdr_pad, *rflags);
+		offsets->size += ltt_align(offsets->begin + offsets->size,
+					   largest_align)
+				 + data_size;
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan)
+			      + offsets->size) > buf->chan->subbuf_size)) {
+			/*
+			 * Event too big for subbuffers, report error, don't
+			 * complete the sub-buffer switch.
+			 */
+			ltt_buf->events_lost++;
+			return -1;
+		} else {
+			/*
+			 * We just made a successful buffer switch and the event
+			 * fits in the new subbuffer. Let's write.
+			 */
+		}
+	} else {
+		/*
+		 * Event fits in the current buffer and we are not on a switch
+		 * boundary. It's safe to write.
+		 */
+	}
+	offsets->end = offsets->begin + offsets->size;
+
+	if (unlikely((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0)) {
+		/*
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
+		 */
+		offsets->end_switch_current = 1;	/* For offsets->begin */
+	}
+	return 0;
+}
+
+/**
+ * ltt_relay_reserve_slot_locked_slow - Atomic slot reservation in a buffer.
+ * @trace : the trace structure to log to.
+ * @ltt_channel : channel structure
+ * @transport_data : data structure specific to ltt relay
+ * @data_size : size of the variable length data to log.
+ * @slot_size : pointer to total size of the slot (out)
+ * @buf_offset : pointer to reserved buffer offset (out)
+ * @tsc : pointer to the tsc at the slot reservation (out)
+ * @cpu : cpuid
+ *
+ * Return : -ENOSPC if not enough space, else returns 0.
+ *
+ * It will take care of sub-buffer switching.
+ */
+int ltt_reserve_slot_locked_slow(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
+		unsigned int *rflags, int largest_align, int cpu,
+		unsigned long flags)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct rchan_buf *buf = *transport_data = rchan->buf[cpu];
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_reserve_switch_offsets offsets;
+
+	offsets.size = 0;
+
+	if (unlikely(ltt_relay_try_reserve_slow(ltt_channel, ltt_buf,
+			rchan, buf, &offsets, data_size, tsc, rflags,
+			largest_align))) {
+		__raw_spin_unlock(&ltt_buf->lock);
+		raw_local_irq_restore(flags);
+		return -ENOSPC;
+	}
+	ltt_buf->offset = offsets.end;
+
+	save_last_tsc(ltt_buf, *tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan, buf, &offsets);
+
+	/*
+	 * Clear noref flag for this subbuffer.
+	 */
+	ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.end - 1, rchan));
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (unlikely(offsets.end_switch_old)) {
+		ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.old - 1,
+							      rchan));
+		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
+			&offsets, tsc);
+	}
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (unlikely(offsets.begin_switch))
+		ltt_reserve_switch_new_subbuf(ltt_channel, ltt_buf, rchan,
+			buf, &offsets, tsc);
+
+	if (unlikely(offsets.end_switch_current))
+		ltt_reserve_end_switch_current(ltt_channel, ltt_buf, rchan,
+			buf, &offsets, tsc);
+
+	ltt_buf->irqflags = flags;
+	*slot_size = offsets.size;
+	*buf_offset = offsets.begin + offsets.before_hdr_pad;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ltt_reserve_slot_locked_slow);
+
+static struct ltt_transport ltt_relay_transport = {
+	.name = "relay",
+	.owner = THIS_MODULE,
+	.ops = {
+		.create_dirs = ltt_relay_create_dirs,
+		.remove_dirs = ltt_relay_remove_dirs,
+		.create_channel = ltt_relay_create_channel,
+		.finish_channel = ltt_relay_finish_channel,
+		.remove_channel = ltt_relay_remove_channel,
+		.wakeup_channel = ltt_relay_async_wakeup_chan,
+		.user_blocking = ltt_relay_user_blocking,
+		.user_errors = ltt_relay_print_user_errors,
+	},
+};
+
+static const struct file_operations ltt_file_operations = {
+	.open = ltt_open,
+	.release = ltt_release,
+	.poll = ltt_poll,
+	.splice_read = ltt_relay_file_splice_read,
+	.ioctl = ltt_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = ltt_compat_ioctl,
+#endif
+};
+
+static int __init ltt_relay_init(void)
+{
+	printk(KERN_INFO "LTT : ltt-relay-locked init\n");
+
+	ltt_transport_register(&ltt_relay_transport);
+
+	return 0;
+}
+
+static void __exit ltt_relay_exit(void)
+{
+	printk(KERN_INFO "LTT : ltt-relay-locked exit\n");
+
+	ltt_transport_unregister(&ltt_relay_transport);
+}
+
+module_init(ltt_relay_init);
+module_exit(ltt_relay_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Locked Relay");
diff --git a/stblinux-2.6.31/ltt/ltt-relay-locked.h b/stblinux-2.6.31/ltt/ltt-relay-locked.h
new file mode 100644
index 0000000..ce16cd5
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-relay-locked.h
@@ -0,0 +1,369 @@
+#ifndef _LTT_LTT_RELAY_LOCKED_H
+#define _LTT_LTT_RELAY_LOCKED_H
+
+/*
+ * ltt/ltt-relay-locked.h
+ *
+ * (C) Copyright 2005-2008 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng buffer space management (reader/writer) using spinlock and interrupt
+ * disable.
+ *
+ * Author:
+ *  Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *  Karim Yaghmour (karim@opersys.com)
+ *  Tom Zanussi (zanussi@us.ibm.com)
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  08/10/08, Fork from lockless mechanism, use spinlock and irqoff.
+ *  19/10/05, Complete lockless mechanism.
+ *  27/05/05, Modular redesign and rewrite.
+ *
+ * Userspace reader semantic :
+ * while (poll fd != POLLHUP) {
+ *   - ioctl RELAY_GET_SUBBUF_SIZE
+ *   while (1) {
+ *     - ioctl GET_SUBBUF
+ *     - splice 1 subbuffer worth of data to a pipe
+ *     - splice the data from pipe to disk/network
+ *     - ioctl PUT_SUBBUF, check error value
+ *       if err val < 0, previous subbuffer was corrupted.
+ *   }
+ * }
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/ltt-relay.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/debugfs.h>
+#include <linux/stat.h>
+#include <linux/cpu.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/splice.h>
+#include <linux/spinlock.h>
+#include <linux/hardirq.h>
+
+#if 0
+#define printk_dbg(fmt, args...) printk(fmt, args)
+#else
+#define printk_dbg(fmt, args...)
+#endif
+
+/* LTTng locked logging buffer info */
+struct ltt_channel_buf_struct {
+	/* First 32 bytes cache-hot cacheline */
+	long offset;			/* Current offset in the buffer */
+	long *commit_count;		/* Commit count per sub-buffer */
+	unsigned long irqflags;		/* IRQ flags saved by reserve */
+	raw_spinlock_t lock;		/* Spinlock protecting buffer */
+	/* End of first 32 bytes cacheline */
+#ifdef CONFIG_LTT_VMCORE
+	long *commit_seq;		/* Consecutive commits */
+#endif
+	unsigned long last_tsc;		/*
+					 * Last timestamp written in the buffer.
+					 */
+	long consumed;			/* Current offset in the buffer */
+	atomic_long_t active_readers;	/* Active readers count */
+	long events_lost;
+	long corrupted_subbuffers;
+	wait_queue_head_t write_wait;	/*
+					 * Wait queue for blocking user space
+					 * writers
+					 */
+	int wakeup_readers;		/* Boolean : wakeup readers waiting ? */
+	wait_queue_head_t read_wait;	/* reader wait queue */
+	unsigned int finalized;		/* buffer has been finalized */
+	struct timer_list switch_timer;	/* timer for periodical switch */
+	unsigned long switch_timer_interval;	/* in jiffies. 0 unset */
+	struct rchan_buf *rbuf;		/* Pointer to rchan_buf */
+} ____cacheline_internodealigned_in_smp;
+
+/*
+ * A switch is done during tracing or as a final flush after tracing (so it
+ * won't write in the new sub-buffer).
+ */
+enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
+
+extern int ltt_reserve_slot_locked_slow(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
+		unsigned int *rflags, int largest_align, int cpu,
+		unsigned long flags);
+
+extern void ltt_force_switch_locked_slow(struct rchan_buf *buf,
+		enum force_switch_mode mode);
+
+/*
+ * Last TSC comparison functions. Check if the current TSC overflows
+ * LTT_TSC_BITS bits from the last TSC read. Reads and writes last_tsc
+ * atomically.
+ */
+
+#if (BITS_PER_LONG == 32)
+static __inline__ void save_last_tsc(struct ltt_channel_buf_struct *ltt_buf,
+					u64 tsc)
+{
+	ltt_buf->last_tsc = (unsigned long)(tsc >> LTT_TSC_BITS);
+}
+
+static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
+					u64 tsc)
+{
+	unsigned long tsc_shifted = (unsigned long)(tsc >> LTT_TSC_BITS);
+
+	if (unlikely((tsc_shifted - ltt_buf->last_tsc)))
+		return 1;
+	else
+		return 0;
+}
+#else
+static __inline__ void save_last_tsc(struct ltt_channel_buf_struct *ltt_buf,
+					u64 tsc)
+{
+	ltt_buf->last_tsc = (unsigned long)tsc;
+}
+
+static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
+					u64 tsc)
+{
+	if (unlikely((tsc - ltt_buf->last_tsc) >> LTT_TSC_BITS))
+		return 1;
+	else
+		return 0;
+}
+#endif
+
+static __inline__ void ltt_check_deliver(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		long offset, long commit_count, long idx)
+{
+	/* Check if all commits have been done */
+	if (unlikely((BUFFER_TRUNC(offset, rchan)
+			>> ltt_channel->n_subbufs_order)
+			- ((commit_count - rchan->subbuf_size)
+			   & ltt_channel->commit_count_mask) == 0)) {
+		/*
+		 * Set noref flag for this subbuffer.
+		 */
+		ltt_set_noref_flag(rchan, buf, idx);
+#ifdef CONFIG_LTT_VMCORE
+		ltt_buf->commit_seq[subbuf_idx] = commit_count;
+#endif
+		ltt_buf->wakeup_readers = 1;
+	}
+}
+
+/*
+ * returns 0 if reserve ok, or 1 if the slow path must be taken.
+ */
+static __inline__ int ltt_relay_try_reserve(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		size_t data_size,
+		u64 *tsc, unsigned int *rflags, int largest_align,
+		long *o_begin, long *o_end, long *o_old,
+		size_t *before_hdr_pad, size_t *size)
+{
+	*o_begin = ltt_buf->offset;
+	*o_old = *o_begin;
+
+	*tsc = trace_clock_read64();
+	if (last_tsc_overflow(ltt_buf, *tsc))
+		*rflags = LTT_RFLAG_ID_SIZE_TSC;
+
+	if (unlikely(SUBBUF_OFFSET(*o_begin, buf->chan) == 0))
+		return 1;
+
+	*size = ltt_get_header_size(ltt_channel,
+				*o_begin, data_size,
+				before_hdr_pad, *rflags);
+	*size += ltt_align(*o_begin + *size, largest_align) + data_size;
+	if (unlikely((SUBBUF_OFFSET(*o_begin, buf->chan) + *size)
+		     > buf->chan->subbuf_size))
+		return 1;
+
+	/*
+	 * Event fits in the current buffer and we are not on a switch
+	 * boundary. It's safe to write.
+	 */
+	*o_end = *o_begin + *size;
+
+	if (unlikely((SUBBUF_OFFSET(*o_end, buf->chan)) == 0))
+		/*
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
+		 */
+		return 1;
+
+	return 0;
+}
+
+/**
+ * ltt_relay_reserve_slot - Atomic slot reservation in a LTTng buffer.
+ * @trace : the trace structure to log to.
+ * @ltt_channel : channel structure
+ * @transport_data : data structure specific to ltt relay
+ * @data_size : size of the variable length data to log.
+ * @slot_size : pointer to total size of the slot (out)
+ * @buf_offset : pointer to reserved buffer offset (out)
+ * @tsc : pointer to the tsc at the slot reservation (out)
+ * @cpu : cpuid
+ *
+ * Return : -ENOSPC if not enough space, else returns 0.
+ *
+ * It will take care of sub-buffer switching.
+ */
+static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
+		unsigned int *rflags, int largest_align, int cpu)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct rchan_buf *buf = *transport_data = rchan->buf[cpu];
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	long o_begin, o_end, o_old;
+	size_t before_hdr_pad;
+	unsigned long flags;
+	unsigned int nest;
+
+	raw_local_irq_save(flags);
+	__raw_spin_lock(&ltt_buf->lock);
+
+	/*
+	 * Perform retryable operations.
+	 */
+	nest = __get_cpu_var(ltt_nesting);
+	if (unlikely(nest > 4 || (in_nmi() && nest > 1))) {
+		ltt_buf->events_lost++;
+		__raw_spin_unlock(&ltt_buf->lock);
+		raw_local_irq_restore(flags);
+		return -EPERM;
+	}
+
+	if (unlikely(ltt_relay_try_reserve(ltt_channel, ltt_buf,
+			rchan, buf, data_size, tsc, rflags,
+			largest_align, &o_begin, &o_end, &o_old,
+			&before_hdr_pad, slot_size)))
+		goto slow_path;
+
+	ltt_buf->offset = o_end;
+
+	save_last_tsc(ltt_buf, *tsc);
+
+	ltt_buf->irqflags = flags;
+	*buf_offset = o_begin + before_hdr_pad;
+	return 0;
+slow_path:
+	return ltt_reserve_slot_locked_slow(trace, ltt_channel,
+		transport_data, data_size, slot_size, buf_offset, tsc,
+		rflags, largest_align, cpu, flags);
+}
+
+/*
+ * Force a sub-buffer switch for a per-cpu buffer. This operation is
+ * completely reentrant : can be called while tracing is active with
+ * absolutely no lock held.
+ */
+static __inline__ void ltt_force_switch(struct rchan_buf *buf,
+		enum force_switch_mode mode)
+{
+	return ltt_force_switch_locked_slow(buf, mode);
+}
+
+/*
+ * for flight recording. must be called after relay_commit.
+ * This function decrements de subbuffer's lost_size each time the commit count
+ * reaches back the reserve offset (module subbuffer size). It is useful for
+ * crash dump.
+ * We use slot_size - 1 to make sure we deal correctly with the case where we
+ * fill the subbuffer completely (so the subbuf index stays in the previous
+ * subbuffer).
+ */
+#ifdef CONFIG_LTT_VMCORE
+static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
+		struct ltt_channel_buf_struct *ltt_buf,
+		long idx, long buf_offset, long commit_count, size_t data_size)
+{
+	long offset;
+
+	offset = buf_offset + data_size;
+
+	/*
+	 * SUBBUF_OFFSET includes commit_count_mask. We can simply
+	 * compare the offsets within the subbuffer without caring about
+	 * buffer full/empty mismatch because offset is never zero here
+	 * (subbuffer header and event headers have non-zero length).
+	 */
+	if (unlikely(SUBBUF_OFFSET(offset - commit_count, buf->chan)))
+		return;
+
+	ltt_buf->commit_seq[idx] = commit_count;
+}
+#else
+static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
+		struct ltt_channel_buf_struct *ltt_buf,
+		long idx, long buf_offset, long commit_count, size_t data_size)
+{
+}
+#endif
+
+/*
+ * Atomic unordered slot commit. Increments the commit count in the
+ * specified sub-buffer, and delivers it if necessary.
+ *
+ * Parameters:
+ *
+ * @ltt_channel : channel structure
+ * @transport_data: transport-specific data
+ * @buf_offset : offset following the event header.
+ * @data_size : size of the event data.
+ * @slot_size : size of the reserved slot.
+ */
+static __inline__ void ltt_commit_slot(
+		struct ltt_channel_struct *ltt_channel,
+		void **transport_data, long buf_offset,
+		size_t data_size, size_t slot_size)
+{
+	struct rchan_buf *buf = *transport_data;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct rchan *rchan = buf->chan;
+	unsigned int offset_end = buf_offset;
+	long endidx = SUBBUF_INDEX(offset_end - 1, rchan);
+	long commit_count;
+
+	ltt_buf->commit_count[endidx] += slot_size;
+	commit_count = ltt_buf->commit_count[endidx];
+
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+			  offset_end - 1, commit_count, endidx);
+	/*
+	 * Update lost_size for each commit. It's needed only for extracting
+	 * ltt buffers from vmcore, after crash.
+	 */
+	ltt_write_commit_counter(buf, ltt_buf, endidx,
+				 buf_offset, commit_count, data_size);
+	__raw_spin_unlock(&ltt_buf->lock);
+	raw_local_irq_restore(ltt_buf->irqflags);
+}
+
+#endif /* _LTT_LTT_RELAY_LOCKED_H */
diff --git a/stblinux-2.6.31/ltt/ltt-relay-lockless.c b/stblinux-2.6.31/ltt/ltt-relay-lockless.c
new file mode 100644
index 0000000..d8b1a9c
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-relay-lockless.c
@@ -0,0 +1,1368 @@
+/*
+ * ltt/ltt-relay-lockless.c
+ *
+ * (C) Copyright 2005-2008 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng lockless buffer space management (reader/writer).
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *  Karim Yaghmour (karim@opersys.com)
+ *  Tom Zanussi (zanussi@us.ibm.com)
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  08/10/08, Cleanup.
+ *  19/10/05, Complete lockless mechanism.
+ *  27/05/05, Modular redesign and rewrite.
+ *
+ * Userspace reader semantic :
+ * while (poll fd != POLLHUP) {
+ *   - ioctl RELAY_GET_SUBBUF_SIZE
+ *   while (1) {
+ *     - ioctl GET_SUBBUF
+ *     - splice 1 subbuffer worth of data to a pipe
+ *     - splice the data from pipe to disk/network
+ *     - ioctl PUT_SUBBUF, check error value
+ *       if err val < 0, previous subbuffer was corrupted.
+ *   }
+ * }
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/ltt-relay.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/rcupdate.h>
+#include <linux/timer.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/smp_lock.h>
+#include <linux/stat.h>
+#include <linux/cpu.h>
+#include <linux/idle.h>
+#include <linux/delay.h>
+#include <asm/atomic.h>
+#include <asm/local.h>
+#include <linux/notifier.h>
+
+#include "ltt-relay-lockless.h"
+
+#if 0
+#define printk_dbg(fmt, args...) printk(fmt, args)
+#else
+#define printk_dbg(fmt, args...)
+#endif
+
+struct ltt_reserve_switch_offsets {
+	long begin, end, old;
+	long begin_switch, end_switch_current, end_switch_old;
+	size_t before_hdr_pad, size;
+};
+
+static
+void ltt_force_switch(struct ltt_chanbuf *buf, enum force_switch_mode mode);
+
+static
+void ltt_relay_print_buffer_errors(struct ltt_chan *chan, unsigned int cpu);
+
+static const struct file_operations ltt_file_operations;
+
+static
+void ltt_buffer_begin(struct ltt_chanbuf *buf, u64 tsc, unsigned int subbuf_idx)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_offset_address(&buf->a,
+				subbuf_idx * chan->a.sb_size);
+
+	header->cycle_count_begin = tsc;
+	header->data_size = 0xFFFFFFFF; /* for debugging */
+	ltt_write_trace_header(chan->a.trace, header);
+}
+
+/*
+ * offset is assumed to never be 0 here : never deliver a completely empty
+ * subbuffer. The lost size is between 0 and subbuf_size-1.
+ */
+static
+void ltt_buffer_end(struct ltt_chanbuf *buf, u64 tsc, unsigned int offset,
+		    unsigned int subbuf_idx)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_offset_address(&buf->a,
+				subbuf_idx * chan->a.sb_size);
+	u32 data_size = SUBBUF_OFFSET(offset - 1, chan) + 1;
+
+	header->data_size = data_size;
+	header->sb_size = PAGE_ALIGN(data_size);
+	header->cycle_count_end = tsc;
+	header->events_lost = local_read(&buf->events_lost);
+	header->subbuf_corrupt = local_read(&buf->corrupted_subbuffers);
+}
+
+/*
+ * Must be called under trace lock or cpu hotplug protection.
+ */
+void ltt_chanbuf_free(struct ltt_chanbuf *buf)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+
+	ltt_relay_print_buffer_errors(chan, buf->a.cpu);
+#ifdef CONFIG_LTT_VMCORE
+	kfree(buf->commit_seq);
+#endif
+	kfree(buf->commit_count);
+
+	ltt_chanbuf_alloc_free(&buf->a);
+}
+
+/*
+ * Must be called under trace lock or cpu hotplug protection.
+ */
+int ltt_chanbuf_create(struct ltt_chanbuf *buf, struct ltt_chan_alloc *chana,
+		       int cpu)
+{
+	struct ltt_chan *chan = container_of(chana, struct ltt_chan, a);
+	struct ltt_trace *trace = chana->trace;
+	unsigned int j, n_sb;
+	int ret;
+
+	/* Test for cpu hotplug */
+	if (buf->a.allocated)
+		return 0;
+
+	ret = ltt_chanbuf_alloc_create(&buf->a, &chan->a, cpu);
+	if (ret)
+		return ret;
+
+	buf->commit_count =
+		kzalloc_node(ALIGN(sizeof(*buf->commit_count) * chan->a.n_sb,
+				   1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(cpu));
+	if (!buf->commit_count) {
+		ret = -ENOMEM;
+		goto free_chanbuf;
+	}
+
+#ifdef CONFIG_LTT_VMCORE
+	buf->commit_seq =
+		kzalloc_node(ALIGN(sizeof(*buf->commit_seq) * chan->a.n_sb,
+				   1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(cpu));
+	if (!buf->commit_seq) {
+		kfree(buf->commit_count);
+		ret = -ENOMEM;
+		goto free_commit;
+	}
+#endif
+
+	local_set(&buf->offset, ltt_sb_header_size());
+	atomic_long_set(&buf->consumed, 0);
+	atomic_long_set(&buf->active_readers, 0);
+	n_sb = chan->a.n_sb;
+	for (j = 0; j < n_sb; j++) {
+		local_set(&buf->commit_count[j].cc, 0);
+		local_set(&buf->commit_count[j].cc_sb, 0);
+		local_set(&buf->commit_count[j].events, 0);
+	}
+	init_waitqueue_head(&buf->write_wait);
+	init_waitqueue_head(&buf->read_wait);
+	spin_lock_init(&buf->full_lock);
+
+	RCHAN_SB_CLEAR_NOREF(buf->a.buf_wsb[0].pages);
+	ltt_buffer_begin(buf, trace->start_tsc, 0);
+	/* atomic_add made on local variable on data that belongs to
+	 * various CPUs : ok because tracing not started (for this cpu). */
+	local_add(ltt_sb_header_size(), &buf->commit_count[0].cc);
+
+	local_set(&buf->events_lost, 0);
+	local_set(&buf->corrupted_subbuffers, 0);
+	buf->finalized = 0;
+
+	ret = ltt_chanbuf_create_file(chan->a.filename, chan->a.parent,
+				      S_IRUSR, buf);
+	if (ret)
+		goto free_init;
+
+	/*
+	 * Ensure the buffer is ready before setting it to allocated.
+	 * Used for cpu hotplug vs async wakeup.
+	 */
+	smp_wmb();
+	buf->a.allocated = 1;
+
+	return 0;
+
+	/* Error handling */
+free_init:
+#ifdef CONFIG_LTT_VMCORE
+	kfree(buf->commit_seq);
+free_commit:
+#endif
+	kfree(buf->commit_count);
+free_chanbuf:
+	ltt_chanbuf_alloc_free(&buf->a);
+	return ret;
+}
+
+void ltt_chan_free(struct kref *kref)
+{
+	struct ltt_chan *chan = container_of(kref, struct ltt_chan, a.kref);
+
+	ltt_ascii_remove(chan);
+	ltt_chan_alloc_free(&chan->a);
+}
+EXPORT_SYMBOL_GPL(ltt_chan_free);
+
+/**
+ * ltt_chan_create - Create channel.
+ */
+int ltt_chan_create(const char *base_filename,
+		    struct ltt_chan *chan, struct dentry *parent,
+		    size_t sb_size, size_t n_sb,
+		    int overwrite, struct ltt_trace *trace)
+{
+	int ret;
+
+	chan->overwrite = overwrite;
+
+	ret = ltt_chan_alloc_init(&chan->a, trace, base_filename, parent,
+				  sb_size, n_sb, overwrite, overwrite);
+	if (ret)
+		goto error;
+
+	chan->commit_count_mask = (~0UL >> chan->a.n_sb_order);
+
+	ret = ltt_ascii_create(chan);
+	if (ret)
+		goto error_chan_alloc_free;
+
+	return ret;
+
+error_chan_alloc_free:
+	ltt_chan_alloc_free(&chan->a);
+error:
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_chan_create);
+
+int ltt_chanbuf_open_read(struct ltt_chanbuf *buf)
+{
+	kref_get(&buf->a.chan->kref);
+	if (!atomic_long_add_unless(&buf->active_readers, 1, 1)) {
+		kref_put(&buf->a.chan->kref, ltt_chan_free);
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+void ltt_chanbuf_release_read(struct ltt_chanbuf *buf)
+{
+	//ltt_relay_destroy_buffer(&buf->a.chan->a, buf->a.cpu);
+	WARN_ON(atomic_long_read(&buf->active_readers) != 1);
+	atomic_long_dec(&buf->active_readers);
+	kref_put(&buf->a.chan->kref, ltt_chan_free);
+}
+
+/*
+ * Wake writers :
+ *
+ * This must be done after the trace is removed from the RCU list so that there
+ * are no stalled writers.
+ */
+static void ltt_relay_wake_writers(struct ltt_chanbuf *buf)
+{
+
+	if (waitqueue_active(&buf->write_wait))
+		wake_up_interruptible(&buf->write_wait);
+}
+
+/*
+ * This function should not be called from NMI interrupt context
+ */
+static void ltt_buf_unfull(struct ltt_chanbuf *buf)
+{
+	ltt_relay_wake_writers(buf);
+}
+
+/*
+ * Promote compiler barrier to a smp_mb().
+ * For the specific LTTng case, this IPI call should be removed if the
+ * architecture does not reorder writes.  This should eventually be provided by
+ * a separate architecture-specific infrastructure.
+ */
+static void remote_mb(void *info)
+{
+	smp_mb();
+}
+
+int ltt_chanbuf_get_subbuf(struct ltt_chanbuf *buf, unsigned long *consumed)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+	long consumed_old, consumed_idx, commit_count, write_offset;
+	int ret;
+
+	consumed_old = atomic_long_read(&buf->consumed);
+	consumed_idx = SUBBUF_INDEX(consumed_old, chan);
+	commit_count = local_read(&buf->commit_count[consumed_idx].cc_sb);
+	/*
+	 * Make sure we read the commit count before reading the buffer
+	 * data and the write offset. Correct consumed offset ordering
+	 * wrt commit count is insured by the use of cmpxchg to update
+	 * the consumed offset.
+	 * smp_call_function_single can fail if the remote CPU is offline,
+	 * this is OK because then there is no wmb to execute there.
+	 * If our thread is executing on the same CPU as the on the buffers
+	 * belongs to, we don't have to synchronize it at all. If we are
+	 * migrated, the scheduler will take care of the memory barriers.
+	 * Normally, smp_call_function_single() should ensure program order when
+	 * executing the remote function, which implies that it surrounds the
+	 * function execution with :
+	 * smp_mb()
+	 * send IPI
+	 * csd_lock_wait
+	 *                recv IPI
+	 *                smp_mb()
+	 *                exec. function
+	 *                smp_mb()
+	 *                csd unlock
+	 * smp_mb()
+	 *
+	 * However, smp_call_function_single() does not seem to clearly execute
+	 * such barriers. It depends on spinlock semantic to provide the barrier
+	 * before executing the IPI and, when busy-looping, csd_lock_wait only
+	 * executes smp_mb() when it has to wait for the other CPU.
+	 *
+	 * I don't trust this code. Therefore, let's add the smp_mb() sequence
+	 * required ourself, even if duplicated. It has no performance impact
+	 * anyway.
+	 *
+	 * smp_mb() is needed because smp_rmb() and smp_wmb() only order read vs
+	 * read and write vs write. They do not ensure core synchronization. We
+	 * really have to ensure total order between the 3 barriers running on
+	 * the 2 CPUs.
+	 */
+#ifdef LTT_NO_IPI_BARRIER
+	/*
+	 * Local rmb to match the remote wmb to read the commit count before the
+	 * buffer data and the write offset.
+	 */
+	smp_rmb();
+#else
+	if (raw_smp_processor_id() != buf->a.cpu) {
+		smp_mb();	/* Total order with IPI handler smp_mb() */
+		smp_call_function_single(buf->a.cpu, remote_mb, NULL, 1);
+		smp_mb();	/* Total order with IPI handler smp_mb() */
+	}
+#endif
+	write_offset = local_read(&buf->offset);
+	/*
+	 * Check that the subbuffer we are trying to consume has been
+	 * already fully committed.
+	 */
+	if (((commit_count - chan->a.sb_size)
+	     & chan->commit_count_mask)
+	    - (BUFFER_TRUNC(consumed_old, chan)
+	       >> chan->a.n_sb_order)
+	    != 0) {
+		return -EAGAIN;
+	}
+	/*
+	 * Check that we are not about to read the same subbuffer in
+	 * which the writer head is.
+	 */
+	if ((SUBBUF_TRUNC(write_offset, chan)
+	   - SUBBUF_TRUNC(consumed_old, chan))
+	   == 0) {
+		return -EAGAIN;
+	}
+
+	ret = update_read_sb_index(&buf->a, &chan->a, consumed_idx);
+	if (ret)
+		return ret;
+
+	*consumed = consumed_old;
+	return 0;
+}
+
+int ltt_chanbuf_put_subbuf(struct ltt_chanbuf *buf, unsigned long consumed)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+	long consumed_new, consumed_old;
+
+	WARN_ON(atomic_long_read(&buf->active_readers) != 1);
+
+	consumed_old = consumed;
+	consumed_new = SUBBUF_ALIGN(consumed_old, chan);
+	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(buf->a.buf_rsb.pages));
+	RCHAN_SB_SET_NOREF(buf->a.buf_rsb.pages);
+
+	spin_lock(&buf->full_lock);
+	if (atomic_long_cmpxchg(&buf->consumed, consumed_old, consumed_new)
+	    != consumed_old) {
+		/* We have been pushed by the writer. */
+		spin_unlock(&buf->full_lock);
+		/*
+		 * We exchanged the subbuffer pages. No corruption possible
+		 * even if the writer did push us. No more -EIO possible.
+		 */
+		return 0;
+	} else {
+		/* tell the client that buffer is now unfull */
+		int index;
+		long data;
+		index = SUBBUF_INDEX(consumed_old, chan);
+		data = BUFFER_OFFSET(consumed_old, chan);
+		ltt_buf_unfull(buf);
+		spin_unlock(&buf->full_lock);
+	}
+	return 0;
+}
+
+static void switch_buffer(unsigned long data)
+{
+	struct ltt_chanbuf *buf = (struct ltt_chanbuf *)data;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+
+	/*
+	 * Only flush buffers periodically if readers are active.
+	 */
+	if (atomic_long_read(&buf->active_readers))
+		ltt_force_switch(buf, FORCE_ACTIVE);
+
+	del_timer(&buf->switch_timer);
+	buf->switch_timer.expires += chan->switch_timer_interval;
+	add_timer_on(&buf->switch_timer, smp_processor_id());
+}
+
+static void ltt_chanbuf_start_switch_timer(struct ltt_chanbuf *buf)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+
+	if (!chan->switch_timer_interval)
+		return;
+
+	init_timer_deferrable(&buf->switch_timer);
+	buf->switch_timer.function = switch_buffer;
+	buf->switch_timer.expires = jiffies + chan->switch_timer_interval;
+	buf->switch_timer.data = (unsigned long)buf;
+	add_timer_on(&buf->switch_timer, buf->a.cpu);
+}
+
+/*
+ * called with ltt traces lock held.
+ */
+void ltt_chan_start_switch_timer(struct ltt_chan *chan)
+{
+	int cpu;
+
+	if (!chan->switch_timer_interval)
+		return;
+
+	for_each_online_cpu(cpu) {
+		struct ltt_chanbuf *buf;
+
+		buf = per_cpu_ptr(chan->a.buf, cpu);
+		ltt_chanbuf_start_switch_timer(buf);
+	}
+}
+/*
+ * Cannot use del_timer_sync with add_timer_on, so use an IPI to locally
+ * delete the timer.
+ */
+static void stop_switch_timer_ipi(void *info)
+{
+	struct ltt_chanbuf *buf = (struct ltt_chanbuf *)info;
+
+	del_timer(&buf->switch_timer);
+}
+
+static void ltt_chanbuf_stop_switch_timer(struct ltt_chanbuf *buf)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+
+	if (!chan->switch_timer_interval)
+		return;
+
+	smp_call_function(stop_switch_timer_ipi, buf, 1);
+}
+
+/*
+ * called with ltt traces lock held.
+ */
+void ltt_chan_stop_switch_timer(struct ltt_chan *chan)
+{
+	int cpu;
+
+	if (!chan->switch_timer_interval)
+		return;
+
+	for_each_online_cpu(cpu) {
+		struct ltt_chanbuf *buf;
+
+		buf = per_cpu_ptr(chan->a.buf, cpu);
+		ltt_chanbuf_stop_switch_timer(buf);
+	}
+}
+
+static void ltt_chanbuf_idle_switch(struct ltt_chanbuf *buf)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+
+	if (chan->switch_timer_interval)
+		ltt_force_switch(buf, FORCE_ACTIVE);
+}
+
+/*
+ * ltt_chanbuf_switch is called from a remote CPU to ensure that the buffers of
+ * a cpu which went down are flushed. Note that if we execute concurrently
+ * with trace allocation, a buffer might appear be unallocated (because it
+ * detects that the target CPU is offline).
+ */
+static void ltt_chanbuf_switch(struct ltt_chanbuf *buf)
+{
+	if (buf->a.allocated)
+		ltt_force_switch(buf, FORCE_ACTIVE);
+}
+
+/**
+ *	ltt_chanbuf_hotcpu_callback - CPU hotplug callback
+ *	@nb: notifier block
+ *	@action: hotplug action to take
+ *	@hcpu: CPU number
+ *
+ *	Returns the success/failure of the operation. (%NOTIFY_OK, %NOTIFY_BAD)
+ */
+static
+int ltt_chanbuf_hotcpu_callback(struct notifier_block *nb,
+					  unsigned long action,
+					  void *hcpu)
+{
+	unsigned int cpu = (unsigned long)hcpu;
+
+	switch (action) {
+	case CPU_DOWN_FAILED:
+	case CPU_DOWN_FAILED_FROZEN:
+	case CPU_ONLINE:
+	case CPU_ONLINE_FROZEN:
+		/*
+		 * CPU hotplug lock protects trace lock from this callback.
+		 */
+		ltt_chan_for_each_channel(ltt_chanbuf_start_switch_timer, cpu);
+		return NOTIFY_OK;
+
+	case CPU_DOWN_PREPARE:
+	case CPU_DOWN_PREPARE_FROZEN:
+		/*
+		 * Performs an IPI to delete the timer locally on the target
+		 * CPU.	CPU hotplug lock protects trace lock from this
+		 * callback.
+		 */
+		ltt_chan_for_each_channel(ltt_chanbuf_stop_switch_timer, cpu);
+		return NOTIFY_OK;
+
+	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
+		/*
+		 * Performing a buffer switch on a remote CPU. Performed by
+		 * the CPU responsible for doing the hotunplug after the target
+		 * CPU stopped running completely. Ensures that all data
+		 * from that remote CPU is flushed. CPU hotplug lock protects
+		 * trace lock from this callback.
+		 */
+		ltt_chan_for_each_channel(ltt_chanbuf_switch, cpu);
+		return NOTIFY_OK;
+
+	default:
+		return NOTIFY_DONE;
+	}
+}
+
+static int pm_idle_entry_callback(struct notifier_block *self,
+				  unsigned long val, void *data)
+{
+	if (val == IDLE_START) {
+		rcu_read_lock_sched_notrace();
+		ltt_chan_for_each_channel(ltt_chanbuf_idle_switch,
+					  smp_processor_id());
+		rcu_read_unlock_sched_notrace();
+	}
+	return 0;
+}
+
+struct notifier_block pm_idle_entry_notifier = {
+	.notifier_call = pm_idle_entry_callback,
+	.priority = ~0U,	/* smallest prio, run after tracing events */
+};
+
+static
+void ltt_relay_print_written(struct ltt_chan *chan, long cons_off,
+			     unsigned int cpu)
+{
+	struct ltt_chanbuf *buf = per_cpu_ptr(chan->a.buf, cpu);
+	long cons_idx, events_count;
+
+	cons_idx = SUBBUF_INDEX(cons_off, chan);
+	events_count = local_read(&buf->commit_count[cons_idx].events);
+
+	if (events_count)
+		printk(KERN_INFO
+			"LTT: %lu events written in channel %s "
+			"(cpu %u, index %lu)\n",
+			events_count, chan->a.filename, cpu, cons_idx);
+}
+
+static
+void ltt_relay_print_subbuffer_errors(struct ltt_chanbuf *buf,
+				      struct ltt_chan *chan, long cons_off,
+				      unsigned int cpu)
+{
+	long cons_idx, commit_count, commit_count_sb, write_offset;
+
+	cons_idx = SUBBUF_INDEX(cons_off, chan);
+	commit_count = local_read(&buf->commit_count[cons_idx].cc);
+	commit_count_sb = local_read(&buf->commit_count[cons_idx].cc_sb);
+	/*
+	 * No need to order commit_count and write_offset reads because we
+	 * execute after trace is stopped when there are no readers left.
+	 */
+	write_offset = local_read(&buf->offset);
+	printk(KERN_WARNING
+	       "LTT : unread channel %s offset is %ld "
+	       "and cons_off : %ld (cpu %u)\n",
+	       chan->a.filename, write_offset, cons_off, cpu);
+	/* Check each sub-buffer for non filled commit count */
+	if (((commit_count - chan->a.sb_size) & chan->commit_count_mask)
+	    - (BUFFER_TRUNC(cons_off, chan) >> chan->a.n_sb_order)
+	    != 0)
+		printk(KERN_ALERT
+		       "LTT : %s : subbuffer %lu has non filled "
+		       "commit count [cc, cc_sb] [%lu,%lu].\n",
+		       chan->a.filename, cons_idx, commit_count,
+		       commit_count_sb);
+	printk(KERN_ALERT "LTT : %s : commit count : %lu, subbuf size %lu\n",
+	       chan->a.filename, commit_count, chan->a.sb_size);
+}
+
+static
+void ltt_relay_print_errors(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			    struct ltt_trace *trace, int cpu)
+{
+	long cons_off;
+
+	/*
+	 * Can be called in the error path of allocation when
+	 * trans_channel_data is not yet set.
+	 */
+	if (!chan)
+		return;
+	for (cons_off = 0; cons_off < chan->a.buf_size;
+	     cons_off = SUBBUF_ALIGN(cons_off, chan))
+		ltt_relay_print_written(chan, cons_off, cpu);
+	for (cons_off = atomic_long_read(&buf->consumed);
+			(SUBBUF_TRUNC(local_read(&buf->offset), chan)
+			 - cons_off) > 0;
+			cons_off = SUBBUF_ALIGN(cons_off, chan))
+		ltt_relay_print_subbuffer_errors(buf, chan, cons_off, cpu);
+}
+
+static
+void ltt_relay_print_buffer_errors(struct ltt_chan *chan, unsigned int cpu)
+{
+	struct ltt_trace *trace = chan->a.trace;
+	struct ltt_chanbuf *buf = per_cpu_ptr(chan->a.buf, cpu);
+
+	if (local_read(&buf->events_lost))
+		printk(KERN_ALERT
+		       "LTT : %s : %ld events lost "
+		       "in %s channel (cpu %u).\n",
+		       chan->a.filename, local_read(&buf->events_lost),
+		       chan->a.filename, cpu);
+	if (local_read(&buf->corrupted_subbuffers))
+		printk(KERN_ALERT
+		       "LTT : %s : %ld corrupted subbuffers "
+		       "in %s channel (cpu %u).\n",
+		       chan->a.filename,
+		       local_read(&buf->corrupted_subbuffers),
+		       chan->a.filename, cpu);
+
+	ltt_relay_print_errors(buf, chan, trace, cpu);
+}
+
+static void ltt_relay_remove_dirs(struct ltt_trace *trace)
+{
+	ltt_ascii_remove_dir(trace);
+	debugfs_remove(trace->dentry.trace_root);
+}
+
+static int ltt_relay_create_dirs(struct ltt_trace *new_trace)
+{
+	struct dentry *ltt_root_dentry;
+	int ret;
+
+	ltt_root_dentry = get_ltt_root();
+	if (!ltt_root_dentry)
+		return ENOENT;
+
+	new_trace->dentry.trace_root = debugfs_create_dir(new_trace->trace_name,
+							  ltt_root_dentry);
+	put_ltt_root();
+	if (new_trace->dentry.trace_root == NULL) {
+		printk(KERN_ERR "LTT : Trace directory name %s already taken\n",
+		       new_trace->trace_name);
+		return EEXIST;
+	}
+	ret = ltt_ascii_create_dir(new_trace);
+	if (ret)
+		printk(KERN_WARNING "LTT : Unable to create ascii output file "
+				    "for trace %s\n", new_trace->trace_name);
+
+	return 0;
+}
+
+/*
+ * LTTng channel flush function.
+ *
+ * Must be called when no tracing is active in the channel, because of
+ * accesses across CPUs.
+ */
+static notrace void ltt_relay_buffer_flush(struct ltt_chanbuf *buf)
+{
+	buf->finalized = 1;
+	ltt_force_switch(buf, FORCE_FLUSH);
+}
+
+static void ltt_relay_async_wakeup_chan(struct ltt_chan *chan)
+{
+	unsigned int i;
+
+	for_each_possible_cpu(i) {
+		struct ltt_chanbuf *buf;
+
+		buf = per_cpu_ptr(chan->a.buf, i);
+		if (!buf->a.allocated)
+			continue;
+		/*
+		 * Ensure the buffer has been allocated before reading its
+		 * content. Sync cpu hotplug vs async wakeup.
+		 */
+		smp_rmb();
+		if (ltt_poll_deliver(buf, chan))
+			wake_up_interruptible(&buf->read_wait);
+	}
+}
+
+static void ltt_relay_finish_buffer(struct ltt_chan *chan, unsigned int cpu)
+{
+	struct ltt_chanbuf *buf = per_cpu_ptr(chan->a.buf, cpu);
+
+	if (buf->a.allocated) {
+		ltt_relay_buffer_flush(buf);
+		ltt_relay_wake_writers(buf);
+	}
+}
+
+
+static void ltt_relay_finish_channel(struct ltt_chan *chan)
+{
+	unsigned int i;
+
+	for_each_possible_cpu(i)
+		ltt_relay_finish_buffer(chan, i);
+}
+
+/*
+ * This is called with preemption disabled when user space has requested
+ * blocking mode.  If one of the active traces has free space below a
+ * specific threshold value, we reenable preemption and block.
+ */
+static
+int ltt_relay_user_blocking(struct ltt_trace *trace, unsigned int chan_index,
+			    size_t data_size, struct user_dbg_data *dbg)
+{
+	struct ltt_chanbuf *buf;
+	struct ltt_chan *chan;
+	int cpu;
+	DECLARE_WAITQUEUE(wait, current);
+
+	chan = &trace->channels[chan_index];
+	cpu = smp_processor_id();
+	buf = per_cpu_ptr(chan->a.buf, cpu);
+
+	/*
+	 * Check if data is too big for the channel : do not
+	 * block for it.
+	 */
+	if (LTT_RESERVE_CRITICAL + data_size > chan->a.sb_size)
+		return 0;
+
+	/*
+	 * If free space too low, we block. We restart from the
+	 * beginning after we resume (cpu id may have changed
+	 * while preemption is active).
+	 */
+	spin_lock(&buf->full_lock);
+	if (!chan->overwrite) {
+		dbg->write = local_read(&buf->offset);
+		dbg->read = atomic_long_read(&buf->consumed);
+		dbg->avail_size = dbg->write + LTT_RESERVE_CRITICAL + data_size
+				  - SUBBUF_TRUNC(dbg->read, chan);
+		if (dbg->avail_size > chan->a.buf_size) {
+			__set_current_state(TASK_INTERRUPTIBLE);
+			add_wait_queue(&buf->write_wait, &wait);
+			spin_unlock(&buf->full_lock);
+			preempt_enable();
+			schedule();
+			__set_current_state(TASK_RUNNING);
+			remove_wait_queue(&buf->write_wait, &wait);
+			if (signal_pending(current))
+				return -ERESTARTSYS;
+			preempt_disable();
+			return 1;
+		}
+	}
+	spin_unlock(&buf->full_lock);
+	return 0;
+}
+
+static
+void ltt_relay_print_user_errors(struct ltt_trace *trace,
+				 unsigned int chan_index, size_t data_size,
+				 struct user_dbg_data *dbg, int cpu)
+{
+	struct ltt_chanbuf *buf;
+	struct ltt_chan *chan;
+
+	chan = &trace->channels[chan_index];
+	buf = per_cpu_ptr(chan->a.buf, cpu);
+
+	printk(KERN_ERR "Error in LTT usertrace : "
+	       "buffer full : event lost in blocking "
+	       "mode. Increase LTT_RESERVE_CRITICAL.\n");
+	printk(KERN_ERR "LTT nesting level is %u.\n",
+	       per_cpu(ltt_nesting, cpu));
+	printk(KERN_ERR "LTT available size %lu.\n",
+	       dbg->avail_size);
+	printk(KERN_ERR "available write : %lu, read : %lu\n",
+	       dbg->write, dbg->read);
+
+	dbg->write = local_read(&buf->offset);
+	dbg->read = atomic_long_read(&buf->consumed);
+
+	printk(KERN_ERR "LTT current size %lu.\n",
+		dbg->write + LTT_RESERVE_CRITICAL + data_size
+		- SUBBUF_TRUNC(dbg->read, chan));
+	printk(KERN_ERR "current write : %lu, read : %lu\n",
+			dbg->write, dbg->read);
+}
+
+/*
+ * ltt_reserve_switch_old_subbuf: switch old subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ *
+ * Note : offset_old should never be 0 here.
+ */
+static
+void ltt_reserve_switch_old_subbuf(struct ltt_chanbuf *buf,
+				   struct ltt_chan *chan,
+				   struct ltt_reserve_switch_offsets *offsets,
+				   u64 *tsc)
+{
+	long oldidx = SUBBUF_INDEX(offsets->old - 1, chan);
+	long commit_count, padding_size;
+
+	padding_size = chan->a.sb_size
+			- (SUBBUF_OFFSET(offsets->old - 1, chan) + 1);
+	ltt_buffer_end(buf, *tsc, offsets->old, oldidx);
+
+	/*
+	 * Must write slot data before incrementing commit count.
+	 * This compiler barrier is upgraded into a smp_wmb() by the IPI
+	 * sent by get_subbuf() when it does its smp_rmb().
+	 */
+	barrier();
+	local_add(padding_size, &buf->commit_count[oldidx].cc);
+	commit_count = local_read(&buf->commit_count[oldidx].cc);
+	ltt_check_deliver(buf, chan, offsets->old - 1, commit_count, oldidx);
+	ltt_write_commit_counter(buf, chan, oldidx, offsets->old, commit_count,
+				 padding_size);
+}
+
+/*
+ * ltt_reserve_switch_new_subbuf: Populate new subbuffer.
+ *
+ * This code can be executed unordered : writers may already have written to the
+ * sub-buffer before this code gets executed, caution.  The commit makes sure
+ * that this code is executed before the deliver of this sub-buffer.
+ */
+static
+void ltt_reserve_switch_new_subbuf(struct ltt_chanbuf *buf,
+				   struct ltt_chan *chan,
+				   struct ltt_reserve_switch_offsets *offsets,
+				   u64 *tsc)
+{
+	long beginidx = SUBBUF_INDEX(offsets->begin, chan);
+	long commit_count;
+
+	ltt_buffer_begin(buf, *tsc, beginidx);
+
+	/*
+	 * Must write slot data before incrementing commit count.
+	 * This compiler barrier is upgraded into a smp_wmb() by the IPI
+	 * sent by get_subbuf() when it does its smp_rmb().
+	 */
+	barrier();
+	local_add(ltt_sb_header_size(), &buf->commit_count[beginidx].cc);
+	commit_count = local_read(&buf->commit_count[beginidx].cc);
+	/* Check if the written buffer has to be delivered */
+	ltt_check_deliver(buf, chan, offsets->begin, commit_count, beginidx);
+	ltt_write_commit_counter(buf, chan, beginidx, offsets->begin,
+				 commit_count, ltt_sb_header_size());
+}
+
+
+/*
+ * ltt_reserve_end_switch_current: finish switching current subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ */
+static
+void ltt_reserve_end_switch_current(struct ltt_chanbuf *buf,
+				    struct ltt_chan *chan,
+				    struct ltt_reserve_switch_offsets *offsets,
+				    u64 *tsc)
+{
+	long endidx = SUBBUF_INDEX(offsets->end - 1, chan);
+	long commit_count, padding_size;
+
+	padding_size = chan->a.sb_size
+			- (SUBBUF_OFFSET(offsets->end - 1, chan) + 1);
+
+	ltt_buffer_end(buf, *tsc, offsets->end, endidx);
+
+	/*
+	 * Must write slot data before incrementing commit count.
+	 * This compiler barrier is upgraded into a smp_wmb() by the IPI
+	 * sent by get_subbuf() when it does its smp_rmb().
+	 */
+	barrier();
+	local_add(padding_size, &buf->commit_count[endidx].cc);
+	commit_count = local_read(&buf->commit_count[endidx].cc);
+	ltt_check_deliver(buf, chan, offsets->end - 1, commit_count, endidx);
+	ltt_write_commit_counter(buf, chan, endidx, offsets->end, commit_count,
+				 padding_size);
+}
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static
+int ltt_relay_try_switch_slow(enum force_switch_mode mode,
+			      struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			      struct ltt_reserve_switch_offsets *offsets,
+			      u64 *tsc)
+{
+	long sb_index;
+	long reserve_commit_diff;
+	long off;
+
+	offsets->begin = local_read(&buf->offset);
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_old = 0;
+
+	*tsc = trace_clock_read64();
+
+	off = SUBBUF_OFFSET(offsets->begin, chan);
+	if ((mode != FORCE_ACTIVE && off > 0) || off > ltt_sb_header_size()) {
+		offsets->begin = SUBBUF_ALIGN(offsets->begin, chan);
+		offsets->end_switch_old = 1;
+	} else {
+		/* we do not have to switch : buffer is empty */
+		return -1;
+	}
+	if (mode == FORCE_ACTIVE)
+		offsets->begin += ltt_sb_header_size();
+	/*
+	 * Always begin_switch in FORCE_ACTIVE mode.
+	 * Test new buffer integrity
+	 */
+	sb_index = SUBBUF_INDEX(offsets->begin, chan);
+	reserve_commit_diff =
+		(BUFFER_TRUNC(offsets->begin, chan)
+		 >> chan->a.n_sb_order)
+		- (local_read(&buf->commit_count[sb_index].cc_sb)
+			& chan->commit_count_mask);
+	if (reserve_commit_diff == 0) {
+		/* Next buffer not corrupted. */
+		if (mode == FORCE_ACTIVE
+		    && !chan->overwrite
+		    && offsets->begin - atomic_long_read(&buf->consumed)
+		       >= chan->a.buf_size) {
+			/*
+			 * We do not overwrite non consumed buffers and we are
+			 * full : ignore switch while tracing is active.
+			 */
+			return -1;
+		}
+	} else {
+		/*
+		 * Next subbuffer corrupted. Force pushing reader even in normal
+		 * mode
+		 */
+	}
+	offsets->end = offsets->begin;
+	return 0;
+}
+
+/*
+ * Force a sub-buffer switch for a per-cpu buffer. This operation is
+ * completely reentrant : can be called while tracing is active with
+ * absolutely no lock held.
+ *
+ * Note, however, that as a local_cmpxchg is used for some atomic
+ * operations, this function must be called from the CPU which owns the buffer
+ * for a ACTIVE flush.
+ */
+void ltt_force_switch_lockless_slow(struct ltt_chanbuf *buf,
+				    enum force_switch_mode mode)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+	struct ltt_reserve_switch_offsets offsets;
+	u64 tsc;
+
+	offsets.size = 0;
+
+	/*
+	 * Perform retryable operations.
+	 */
+	do {
+		if (ltt_relay_try_switch_slow(mode, buf, chan, &offsets, &tsc))
+			return;
+	} while (local_cmpxchg(&buf->offset, offsets.old, offsets.end)
+		 != offsets.old);
+
+	/*
+	 * Atomically update last_tsc. This update races against concurrent
+	 * atomic updates, but the race will always cause supplementary full TSC
+	 * events, never the opposite (missing a full TSC event when it would be
+	 * needed).
+	 */
+	save_last_tsc(buf, tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	if (mode == FORCE_ACTIVE) {
+		ltt_reserve_push_reader(buf, chan, offsets.end - 1);
+		ltt_clear_noref_flag(&buf->a, SUBBUF_INDEX(offsets.end - 1,
+							   chan));
+	}
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (offsets.end_switch_old) {
+		ltt_clear_noref_flag(&buf->a, SUBBUF_INDEX(offsets.old - 1,
+							   chan));
+		ltt_reserve_switch_old_subbuf(buf, chan, &offsets, &tsc);
+	}
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (mode == FORCE_ACTIVE)
+		ltt_reserve_switch_new_subbuf(buf, chan, &offsets, &tsc);
+}
+EXPORT_SYMBOL_GPL(ltt_force_switch_lockless_slow);
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static
+int ltt_relay_try_reserve_slow(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			       struct ltt_reserve_switch_offsets *offsets,
+			       size_t data_size, u64 *tsc, unsigned int *rflags,
+			       int largest_align)
+{
+	long reserve_commit_diff;
+
+	offsets->begin = local_read(&buf->offset);
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_current = 0;
+	offsets->end_switch_old = 0;
+
+	*tsc = trace_clock_read64();
+	if (last_tsc_overflow(buf, *tsc))
+		*rflags = LTT_RFLAG_ID_SIZE_TSC;
+
+	if (unlikely(SUBBUF_OFFSET(offsets->begin, chan) == 0)) {
+		offsets->begin_switch = 1;		/* For offsets->begin */
+	} else {
+		offsets->size = ltt_get_header_size(chan, offsets->begin,
+						    data_size,
+						    &offsets->before_hdr_pad,
+						    *rflags);
+		offsets->size += ltt_align(offsets->begin + offsets->size,
+					   largest_align)
+				 + data_size;
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, chan) +
+			     offsets->size) > chan->a.sb_size)) {
+			offsets->end_switch_old = 1;	/* For offsets->old */
+			offsets->begin_switch = 1;	/* For offsets->begin */
+		}
+	}
+	if (unlikely(offsets->begin_switch)) {
+		long sb_index;
+
+		/*
+		 * We are typically not filling the previous buffer completely.
+		 */
+		if (likely(offsets->end_switch_old))
+			offsets->begin = SUBBUF_ALIGN(offsets->begin, chan);
+		offsets->begin = offsets->begin + ltt_sb_header_size();
+		/* Test new buffer integrity */
+		sb_index = SUBBUF_INDEX(offsets->begin, chan);
+		reserve_commit_diff =
+		  (BUFFER_TRUNC(offsets->begin, chan)
+		   >> chan->a.n_sb_order)
+		  - (local_read(&buf->commit_count[sb_index].cc_sb)
+				& chan->commit_count_mask);
+		if (likely(reserve_commit_diff == 0)) {
+			/* Next buffer not corrupted. */
+			if (unlikely(!chan->overwrite &&
+				(SUBBUF_TRUNC(offsets->begin, chan)
+				 - SUBBUF_TRUNC(atomic_long_read(&buf->consumed),
+						chan))
+				>= chan->a.buf_size)) {
+				/*
+				 * We do not overwrite non consumed buffers
+				 * and we are full : event is lost.
+				 */
+				local_inc(&buf->events_lost);
+				return -1;
+			} else {
+				/*
+				 * next buffer not corrupted, we are either in
+				 * overwrite mode or the buffer is not full.
+				 * It's safe to write in this new subbuffer.
+				 */
+			}
+		} else {
+			/*
+			 * Next subbuffer corrupted. Drop event in normal and
+			 * overwrite mode. Caused by either a writer OOPS or
+			 * too many nested writes over a reserve/commit pair.
+			 */
+			local_inc(&buf->events_lost);
+			return -1;
+		}
+		offsets->size = ltt_get_header_size(chan, offsets->begin,
+						    data_size,
+						    &offsets->before_hdr_pad,
+						    *rflags);
+		offsets->size += ltt_align(offsets->begin + offsets->size,
+					   largest_align)
+				 + data_size;
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, chan)
+			     + offsets->size) > chan->a.sb_size)) {
+			/*
+			 * Event too big for subbuffers, report error, don't
+			 * complete the sub-buffer switch.
+			 */
+			local_inc(&buf->events_lost);
+			return -1;
+		} else {
+			/*
+			 * We just made a successful buffer switch and the event
+			 * fits in the new subbuffer. Let's write.
+			 */
+		}
+	} else {
+		/*
+		 * Event fits in the current buffer and we are not on a switch
+		 * boundary. It's safe to write.
+		 */
+	}
+	offsets->end = offsets->begin + offsets->size;
+
+	if (unlikely((SUBBUF_OFFSET(offsets->end, chan)) == 0)) {
+		/*
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
+		 */
+		offsets->end_switch_current = 1;	/* For offsets->begin */
+	}
+	return 0;
+}
+
+/**
+ * ltt_relay_reserve_slot_lockless_slow - Atomic slot reservation in a buffer.
+ * @trace: the trace structure to log to.
+ * @ltt_channel: channel structure
+ * @transport_data: data structure specific to ltt relay
+ * @data_size: size of the variable length data to log.
+ * @slot_size: pointer to total size of the slot (out)
+ * @buf_offset : pointer to reserved buffer offset (out)
+ * @tsc: pointer to the tsc at the slot reservation (out)
+ * @cpu: cpuid
+ *
+ * Return : -ENOSPC if not enough space, else returns 0.
+ * It will take care of sub-buffer switching.
+ */
+int ltt_reserve_slot_lockless_slow(struct ltt_chan *chan,
+				   struct ltt_trace *trace, size_t data_size,
+				   int largest_align, int cpu,
+				   struct ltt_chanbuf **ret_buf,
+				   size_t *slot_size, long *buf_offset,
+				   u64 *tsc, unsigned int *rflags)
+{
+	struct ltt_chanbuf *buf = *ret_buf = per_cpu_ptr(chan->a.buf, cpu);
+	struct ltt_reserve_switch_offsets offsets;
+
+	offsets.size = 0;
+
+	do {
+		if (unlikely(ltt_relay_try_reserve_slow(buf, chan, &offsets,
+							data_size, tsc, rflags,
+							largest_align)))
+			return -ENOSPC;
+	} while (unlikely(local_cmpxchg(&buf->offset, offsets.old, offsets.end)
+			  != offsets.old));
+
+	/*
+	 * Atomically update last_tsc. This update races against concurrent
+	 * atomic updates, but the race will always cause supplementary full TSC
+	 * events, never the opposite (missing a full TSC event when it would be
+	 * needed).
+	 */
+	save_last_tsc(buf, *tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	ltt_reserve_push_reader(buf, chan, offsets.end - 1);
+
+	/*
+	 * Clear noref flag for this subbuffer.
+	 */
+	ltt_clear_noref_flag(&buf->a, SUBBUF_INDEX(offsets.end - 1, chan));
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (unlikely(offsets.end_switch_old)) {
+		ltt_clear_noref_flag(&buf->a, SUBBUF_INDEX(offsets.old - 1,
+							  chan));
+		ltt_reserve_switch_old_subbuf(buf, chan, &offsets, tsc);
+	}
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (unlikely(offsets.begin_switch))
+		ltt_reserve_switch_new_subbuf(buf, chan, &offsets, tsc);
+
+	if (unlikely(offsets.end_switch_current))
+		ltt_reserve_end_switch_current(buf, chan, &offsets, tsc);
+
+	*slot_size = offsets.size;
+	*buf_offset = offsets.begin + offsets.before_hdr_pad;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ltt_reserve_slot_lockless_slow);
+
+static struct ltt_transport ltt_relay_transport = {
+	.name = "relay",
+	.owner = THIS_MODULE,
+	.ops = {
+		.create_dirs = ltt_relay_create_dirs,
+		.remove_dirs = ltt_relay_remove_dirs,
+		.create_channel = ltt_chan_create,
+		.finish_channel = ltt_relay_finish_channel,
+		.remove_channel = ltt_chan_free,
+		.wakeup_channel = ltt_relay_async_wakeup_chan,
+		.user_blocking = ltt_relay_user_blocking,
+		.user_errors = ltt_relay_print_user_errors,
+		.start_switch_timer = ltt_chan_start_switch_timer,
+		.stop_switch_timer = ltt_chan_stop_switch_timer,
+	},
+};
+
+static struct notifier_block fn_ltt_chanbuf_hotcpu_callback = {
+	.notifier_call = ltt_chanbuf_hotcpu_callback,
+	.priority = 6,
+};
+
+static int __init ltt_relay_init(void)
+{
+	printk(KERN_INFO "LTT : ltt-relay init\n");
+
+	ltt_transport_register(&ltt_relay_transport);
+	register_cpu_notifier(&fn_ltt_chanbuf_hotcpu_callback);
+	register_idle_notifier(&pm_idle_entry_notifier);
+
+	return 0;
+}
+
+static void __exit ltt_relay_exit(void)
+{
+	printk(KERN_INFO "LTT : ltt-relay exit\n");
+
+	unregister_idle_notifier(&pm_idle_entry_notifier);
+	unregister_cpu_notifier(&fn_ltt_chanbuf_hotcpu_callback);
+	ltt_transport_unregister(&ltt_relay_transport);
+}
+
+module_init(ltt_relay_init);
+module_exit(ltt_relay_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Lockless Relay");
diff --git a/stblinux-2.6.31/ltt/ltt-relay-lockless.h b/stblinux-2.6.31/ltt/ltt-relay-lockless.h
new file mode 100644
index 0000000..dae9023
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-relay-lockless.h
@@ -0,0 +1,544 @@
+#ifndef _LTT_LTT_RELAY_LOCKLESS_H
+#define _LTT_LTT_RELAY_LOCKLESS_H
+
+/*
+ * ltt/ltt-relay-lockless.h
+ *
+ * (C) Copyright 2005-2008 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng lockless buffer space management (reader/writer).
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *  Karim Yaghmour (karim@opersys.com)
+ *  Tom Zanussi (zanussi@us.ibm.com)
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  08/10/08, Cleanup.
+ *  19/10/05, Complete lockless mechanism.
+ *  27/05/05, Modular redesign and rewrite.
+ *
+ * Userspace reader semantic :
+ * while (poll fd != POLLHUP) {
+ *   - ioctl RELAY_GET_SUBBUF_SIZE
+ *   while (1) {
+ *     - ioctl GET_SUBBUF
+ *     - splice 1 subbuffer worth of data to a pipe
+ *     - splice the data from pipe to disk/network
+ *     - ioctl PUT_SUBBUF, check error value
+ *       if err val < 0, previous subbuffer was corrupted.
+ *   }
+ * }
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/cache.h>
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/ltt-relay.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/rcupdate.h>
+#include <linux/timer.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/debugfs.h>
+#include <linux/stat.h>
+#include <linux/cpu.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/splice.h>
+#include <asm/atomic.h>
+#include <asm/local.h>
+
+#if 0
+#define printk_dbg(fmt, args...) printk(fmt, args)
+#else
+#define printk_dbg(fmt, args...)
+#endif
+
+struct commit_counters {
+	local_t cc;
+	local_t cc_sb;			/* Incremented _once_ at sb switch */
+	local_t events;			/* Event count */
+};
+
+/* LTTng lockless logging buffer info */
+struct ltt_chanbuf {
+	struct ltt_chanbuf_alloc a;	/* Parent. First field. */
+	/* First 32 bytes cache-hot cacheline */
+	local_t offset;			/* Current offset in the buffer */
+	struct commit_counters *commit_count;
+					/* Commit count per sub-buffer */
+	atomic_long_t consumed;		/*
+					 * Current offset in the buffer
+					 * standard atomic access (shared)
+					 */
+	unsigned long last_tsc;		/*
+					 * Last timestamp written in the buffer.
+					 */
+	/* End of first 32 bytes cacheline */
+#ifdef CONFIG_LTT_VMCORE
+	local_t *commit_seq;		/* Consecutive commits */
+#endif
+	atomic_long_t active_readers;	/*
+					 * Active readers count
+					 * standard atomic access (shared)
+					 */
+	local_t events_lost;
+	local_t corrupted_subbuffers;
+	spinlock_t full_lock;		/*
+					 * buffer full condition spinlock, only
+					 * for userspace tracing blocking mode
+					 * synchronization with reader.
+					 */
+	wait_queue_head_t write_wait;	/*
+					 * Wait queue for blocking user space
+					 * writers
+					 */
+	wait_queue_head_t read_wait;	/* reader wait queue */
+	unsigned int finalized;		/* buffer has been finalized */
+	struct timer_list switch_timer;	/* timer for periodical switch */
+};
+
+/*
+ * A switch is done during tracing or as a final flush after tracing (so it
+ * won't write in the new sub-buffer).
+ */
+enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
+
+extern
+int ltt_reserve_slot_lockless_slow(struct ltt_chan *chan,
+				   struct ltt_trace *trace, size_t data_size,
+				   int largest_align, int cpu,
+				   struct ltt_chanbuf **ret_buf,
+				   size_t *slot_size, long *buf_offset,
+				   u64 *tsc, unsigned int *rflags);
+
+extern void ltt_force_switch_lockless_slow(struct ltt_chanbuf *buf,
+					   enum force_switch_mode mode);
+
+/*
+ * Last TSC comparison functions. Check if the current TSC overflows
+ * LTT_TSC_BITS bits from the last TSC read. Reads and writes last_tsc
+ * atomically.
+ */
+
+#if (BITS_PER_LONG == 32)
+static __inline__ void save_last_tsc(struct ltt_chanbuf *buf, u64 tsc)
+{
+	buf->last_tsc = (unsigned long)(tsc >> LTT_TSC_BITS);
+}
+
+static __inline__ int last_tsc_overflow(struct ltt_chanbuf *buf, u64 tsc)
+{
+	unsigned long tsc_shifted = (unsigned long)(tsc >> LTT_TSC_BITS);
+
+	if (unlikely((tsc_shifted - buf->last_tsc)))
+		return 1;
+	else
+		return 0;
+}
+#else
+static __inline__ void save_last_tsc(struct ltt_chanbuf *buf, u64 tsc)
+{
+	buf->last_tsc = (unsigned long)tsc;
+}
+
+static __inline__ int last_tsc_overflow(struct ltt_chanbuf *buf, u64 tsc)
+{
+	if (unlikely((tsc - buf->last_tsc) >> LTT_TSC_BITS))
+		return 1;
+	else
+		return 0;
+}
+#endif
+
+extern
+int ltt_chanbuf_create(struct ltt_chanbuf *buf, struct ltt_chan_alloc *chana,
+		       int cpu);
+extern void ltt_chanbuf_free(struct ltt_chanbuf *buf);
+extern int ltt_chan_create(const char *base_filename, struct ltt_chan *chan,
+			   struct dentry *parent, size_t sb_size, size_t n_sb,
+			   int overwrite, struct ltt_trace *trace);
+extern void ltt_chan_free(struct kref *kref);
+
+/* Buffer access operations */
+
+extern int ltt_chanbuf_open_read(struct ltt_chanbuf *buf);
+extern void ltt_chanbuf_release_read(struct ltt_chanbuf *buf);
+extern int ltt_chanbuf_get_subbuf(struct ltt_chanbuf *buf,
+				  unsigned long *consumed);
+extern int ltt_chanbuf_put_subbuf(struct ltt_chanbuf *buf,
+				  unsigned long consumed);
+extern void ltt_chan_start_switch_timer(struct ltt_chan *chan);
+extern void ltt_chan_stop_switch_timer(struct ltt_chan *chan);
+
+static __inline__
+unsigned long ltt_chanbuf_get_offset(struct ltt_chanbuf *buf)
+{
+	return local_read(&buf->offset);
+}
+
+static __inline__
+unsigned long ltt_chanbuf_get_consumed(struct ltt_chanbuf *buf)
+{
+	return atomic_long_read(&buf->consumed);
+}
+
+static __inline__
+int ltt_chanbuf_is_finalized(struct ltt_chanbuf *buf)
+{
+	return buf->finalized;
+}
+
+static __inline__
+void ltt_reserve_push_reader(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			     long offset)
+{
+	long consumed_old, consumed_new;
+
+	do {
+		consumed_old = atomic_long_read(&buf->consumed);
+		/*
+		 * If buffer is in overwrite mode, push the reader consumed
+		 * count if the write position has reached it and we are not
+		 * at the first iteration (don't push the reader farther than
+		 * the writer). This operation can be done concurrently by many
+		 * writers in the same buffer, the writer being at the farthest
+		 * write position sub-buffer index in the buffer being the one
+		 * which will win this loop.
+		 * If the buffer is not in overwrite mode, pushing the reader
+		 * only happens if a sub-buffer is corrupted.
+		 */
+		if (unlikely((SUBBUF_TRUNC(offset, chan)
+			      - SUBBUF_TRUNC(consumed_old, chan))
+			     >= chan->a.buf_size))
+			consumed_new = SUBBUF_ALIGN(consumed_old, chan);
+		else
+			return;
+	} while (unlikely(atomic_long_cmpxchg(&buf->consumed, consumed_old,
+					      consumed_new) != consumed_old));
+}
+
+#ifdef CONFIG_LTT_VMCORE
+static __inline__
+void ltt_vmcore_check_deliver(struct ltt_chanbuf *buf, long commit_count,
+			      long idx)
+{
+	local_set(&buf->commit_seq[idx], commit_count);
+}
+#else
+static __inline__
+void ltt_vmcore_check_deliver(struct ltt_chanbuf *buf, long commit_count,
+			      long idx)
+{
+}
+#endif
+
+static __inline__
+void ltt_check_deliver(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+		       long offset, long commit_count, long idx)
+{
+	long old_commit_count = commit_count - chan->a.sb_size;
+
+	/* Check if all commits have been done */
+	if (unlikely((BUFFER_TRUNC(offset, chan) >> chan->a.n_sb_order)
+		     - (old_commit_count & chan->commit_count_mask) == 0)) {
+		/*
+		 * If we succeeded in updating the cc_sb, we are delivering
+		 * the subbuffer. Deals with concurrent updates of the "cc"
+		 * value without adding a add_return atomic operation to the
+		 * fast path.
+		 */
+		if (likely(local_cmpxchg(&buf->commit_count[idx].cc_sb,
+					 old_commit_count, commit_count)
+			   == old_commit_count)) {
+			/*
+			 * Set noref flag for this subbuffer.
+			 */
+			ltt_set_noref_flag(&buf->a, idx);
+			ltt_vmcore_check_deliver(buf, commit_count, idx);
+		}
+	}
+}
+
+
+static __inline__
+int ltt_poll_deliver(struct ltt_chanbuf *buf, struct ltt_chan *chan)
+{
+	long consumed_old, consumed_idx, commit_count, write_offset;
+
+	consumed_old = atomic_long_read(&buf->consumed);
+	consumed_idx = SUBBUF_INDEX(consumed_old, chan);
+	commit_count = local_read(&buf->commit_count[consumed_idx].cc_sb);
+	/*
+	 * No memory barrier here, since we are only interested
+	 * in a statistically correct polling result. The next poll will
+	 * get the data is we are racing. The mb() that ensures correct
+	 * memory order is in get_subbuf.
+	 */
+	write_offset = local_read(&buf->offset);
+
+	/*
+	 * Check that the subbuffer we are trying to consume has been
+	 * already fully committed.
+	 */
+
+	if (((commit_count - chan->a.sb_size)
+	     & chan->commit_count_mask)
+	    - (BUFFER_TRUNC(consumed_old, chan)
+	       >> chan->a.n_sb_order)
+	    != 0)
+		return 0;
+
+	/*
+	 * Check that we are not about to read the same subbuffer in
+	 * which the writer head is.
+	 */
+	if ((SUBBUF_TRUNC(write_offset, chan)
+	   - SUBBUF_TRUNC(consumed_old, chan))
+	   == 0)
+		return 0;
+
+	return 1;
+
+}
+
+static __inline__
+u32 get_read_sb_size(struct ltt_chanbuf *buf)
+{
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_read_offset_address(&buf->a, 0);
+	return header->sb_size;
+}
+
+/*
+ * returns 0 if reserve ok, or 1 if the slow path must be taken.
+ */
+static __inline__
+int ltt_relay_try_reserve(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			  size_t data_size, u64 *tsc, unsigned int *rflags,
+			  int largest_align, long *o_begin, long *o_end,
+			  long *o_old, size_t *before_hdr_pad, size_t *size)
+{
+	*o_begin = local_read(&buf->offset);
+	*o_old = *o_begin;
+
+	*tsc = trace_clock_read64();
+
+#ifdef CONFIG_LTT_VMCORE
+	prefetch(&buf->commit_count[SUBBUF_INDEX(*o_begin, chan)]);
+	prefetch(&buf->commit_seq[SUBBUF_INDEX(*o_begin, chan)]);
+#else
+	prefetchw(&buf->commit_count[SUBBUF_INDEX(*o_begin, chan)]);
+#endif
+	if (last_tsc_overflow(buf, *tsc))
+		*rflags = LTT_RFLAG_ID_SIZE_TSC;
+
+	if (unlikely(SUBBUF_OFFSET(*o_begin, chan) == 0))
+		return 1;
+
+	*size = ltt_get_header_size(chan, *o_begin, data_size, before_hdr_pad,
+				    *rflags);
+	*size += ltt_align(*o_begin + *size, largest_align) + data_size;
+	if (unlikely((SUBBUF_OFFSET(*o_begin, chan) + *size) > chan->a.sb_size))
+		return 1;
+
+	/*
+	 * Event fits in the current buffer and we are not on a switch
+	 * boundary. It's safe to write.
+	 */
+	*o_end = *o_begin + *size;
+
+	if (unlikely((SUBBUF_OFFSET(*o_end, chan)) == 0))
+		/*
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
+		 */
+		return 1;
+
+	return 0;
+}
+
+static __inline__
+int ltt_reserve_slot(struct ltt_chan *chan,
+		     struct ltt_trace *trace, size_t data_size,
+		     int largest_align, int cpu,
+		     struct ltt_chanbuf **ret_buf,
+		     size_t *slot_size, long *buf_offset, u64 *tsc,
+		     unsigned int *rflags)
+{
+	struct ltt_chanbuf *buf = *ret_buf = per_cpu_ptr(chan->a.buf, cpu);
+	long o_begin, o_end, o_old;
+	size_t before_hdr_pad;
+
+	/*
+	 * Perform retryable operations.
+	 */
+	if (unlikely(__get_cpu_var(ltt_nesting) > 4)) {
+		local_inc(&buf->events_lost);
+		return -EPERM;
+	}
+
+	if (unlikely(ltt_relay_try_reserve(buf, chan, data_size, tsc, rflags,
+					   largest_align, &o_begin, &o_end,
+					   &o_old, &before_hdr_pad, slot_size)))
+		goto slow_path;
+
+	if (unlikely(local_cmpxchg(&buf->offset, o_old, o_end) != o_old))
+		goto slow_path;
+
+	/*
+	 * Atomically update last_tsc. This update races against concurrent
+	 * atomic updates, but the race will always cause supplementary full TSC
+	 * events, never the opposite (missing a full TSC event when it would be
+	 * needed).
+	 */
+	save_last_tsc(buf, *tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	ltt_reserve_push_reader(buf, chan, o_end - 1);
+
+	/*
+	 * Clear noref flag for this subbuffer.
+	 */
+	ltt_clear_noref_flag(&buf->a, SUBBUF_INDEX(o_end - 1, chan));
+
+	*buf_offset = o_begin + before_hdr_pad;
+	return 0;
+slow_path:
+	return ltt_reserve_slot_lockless_slow(chan, trace, data_size,
+					      largest_align, cpu, ret_buf,
+					      slot_size, buf_offset, tsc,
+					      rflags);
+}
+
+/*
+ * Force a sub-buffer switch for a per-cpu buffer. This operation is
+ * completely reentrant : can be called while tracing is active with
+ * absolutely no lock held.
+ *
+ * Note, however, that as a local_cmpxchg is used for some atomic
+ * operations, this function must be called from the CPU which owns the buffer
+ * for a ACTIVE flush.
+ */
+static __inline__
+void ltt_force_switch(struct ltt_chanbuf *buf, enum force_switch_mode mode)
+{
+	return ltt_force_switch_lockless_slow(buf, mode);
+}
+
+/*
+ * for flight recording. must be called after relay_commit.
+ * This function increments the subbuffer's commit_seq counter each time the
+ * commit count reaches back the reserve offset (module subbuffer size). It is
+ * useful for crash dump.
+ */
+#ifdef CONFIG_LTT_VMCORE
+static __inline__
+void ltt_write_commit_counter(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			      long idx, long buf_offset, long commit_count,
+			      size_t data_size)
+{
+	long offset;
+	long commit_seq_old;
+
+	offset = buf_offset + data_size;
+
+	/*
+	 * SUBBUF_OFFSET includes commit_count_mask. We can simply
+	 * compare the offsets within the subbuffer without caring about
+	 * buffer full/empty mismatch because offset is never zero here
+	 * (subbuffer header and event headers have non-zero length).
+	 */
+	if (unlikely(SUBBUF_OFFSET(offset - commit_count, chan)))
+		return;
+
+	commit_seq_old = local_read(&buf->commit_seq[idx]);
+	while (commit_seq_old < commit_count)
+		commit_seq_old = local_cmpxchg(&buf->commit_seq[idx],
+					 commit_seq_old, commit_count);
+}
+#else
+static __inline__
+void ltt_write_commit_counter(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			      long idx, long buf_offset, long commit_count,
+			      size_t data_size)
+{
+}
+#endif
+
+/*
+ * Atomic unordered slot commit. Increments the commit count in the
+ * specified sub-buffer, and delivers it if necessary.
+ *
+ * Parameters:
+ *
+ * @buf: buffer.
+ * @chan: channel.
+ * @buf_offset : offset following the event header.
+ * @data_size : size of the event data.
+ * @slot_size : size of the reserved slot.
+ */
+static __inline__
+void ltt_commit_slot(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+		     long buf_offset, size_t data_size, size_t slot_size)
+{
+	long offset_end = buf_offset;
+	long endidx = SUBBUF_INDEX(offset_end - 1, chan);
+	long commit_count;
+
+#ifdef LTT_NO_IPI_BARRIER
+	smp_wmb();
+#else
+	/*
+	 * Must write slot data before incrementing commit count.
+	 * This compiler barrier is upgraded into a smp_mb() by the IPI
+	 * sent by get_subbuf().
+	 */
+	barrier();
+#endif
+	local_add(slot_size, &buf->commit_count[endidx].cc);
+	local_inc(&buf->commit_count[endidx].events);
+	/*
+	 * commit count read can race with concurrent OOO commit count updates.
+	 * This is only needed for ltt_check_deliver (for non-polling delivery
+	 * only) and for ltt_write_commit_counter. The race can only cause the
+	 * counter to be read with the same value more than once, which could
+	 * cause :
+	 * - Multiple delivery for the same sub-buffer (which is handled
+	 *   gracefully by the reader code) if the value is for a full
+	 *   sub-buffer. It's important that we can never miss a sub-buffer
+	 *   delivery. Re-reading the value after the local_add ensures this.
+	 * - Reading a commit_count with a higher value that what was actually
+	 *   added to it for the ltt_write_commit_counter call (again caused by
+	 *   a concurrent committer). It does not matter, because this function
+	 *   is interested in the fact that the commit count reaches back the
+	 *   reserve offset for a specific sub-buffer, which is completely
+	 *   independent of the order.
+	 */
+	commit_count = local_read(&buf->commit_count[endidx].cc);
+
+	ltt_check_deliver(buf, chan, offset_end - 1, commit_count, endidx);
+	/*
+	 * Update data_size for each commit. It's needed only for extracting
+	 * ltt buffers from vmcore, after crash.
+	 */
+	ltt_write_commit_counter(buf, chan, endidx, buf_offset,
+				 commit_count, data_size);
+}
+
+#endif //_LTT_LTT_RELAY_LOCKLESS_H
diff --git a/stblinux-2.6.31/ltt/ltt-relay-select.h b/stblinux-2.6.31/ltt/ltt-relay-select.h
new file mode 100644
index 0000000..1ff00be
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-relay-select.h
@@ -0,0 +1,14 @@
+#ifndef _LTT_LTT_RELAY_SELECT_H
+#define _LTT_LTT_RELAY_SELECT_H
+
+#ifdef CONFIG_LTT_RELAY_LOCKLESS
+#include "ltt-relay-lockless.h"
+#elif defined(CONFIG_LTT_RELAY_IRQOFF)
+#include "ltt-relay-irqoff.h"
+#elif defined(CONFIG_LTT_RELAY_LOCKED)
+#include "ltt-relay-locked.h"
+#else
+#error "At least one LTTng relay transport should be selected."
+#endif
+
+#endif /* _LTT_LTT_RELAY_SELECT_H */
diff --git a/stblinux-2.6.31/ltt/ltt-relay-splice.c b/stblinux-2.6.31/ltt/ltt-relay-splice.c
new file mode 100644
index 0000000..115da7e
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-relay-splice.c
@@ -0,0 +1,154 @@
+/*
+ * Copyright (C) 2002-2005 - Tom Zanussi (zanussi@us.ibm.com), IBM Corp
+ * Copyright (C) 1999-2005 - Karim Yaghmour (karim@opersys.com)
+ * Copyright (C) 2008-2009 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Re-using content from kernel/relay.c
+ *
+ * This file is released under the GPL.
+ */
+
+#include <linux/errno.h>
+#include <linux/stddef.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/ltt-relay.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+#include <linux/cpu.h>
+#include <linux/splice.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/bitops.h>
+
+#include "ltt-relay-select.h"
+
+static void ltt_relay_pipe_buf_release(struct pipe_inode_info *pipe,
+				       struct pipe_buffer *pbuf)
+{
+}
+
+static struct pipe_buf_operations ltt_relay_pipe_buf_ops = {
+	.can_merge = 0,
+	.map = generic_pipe_buf_map,
+	.unmap = generic_pipe_buf_unmap,
+	.confirm = generic_pipe_buf_confirm,
+	.release = ltt_relay_pipe_buf_release,
+	.steal = generic_pipe_buf_steal,
+	.get = generic_pipe_buf_get,
+};
+
+static void ltt_relay_page_release(struct splice_pipe_desc *spd, unsigned int i)
+{
+}
+
+/*
+ *	subbuf_splice_actor - splice up to one subbuf's worth of data
+ */
+static int subbuf_splice_actor(struct file *in,
+			       loff_t *ppos,
+			       struct pipe_inode_info *pipe,
+			       size_t len,
+			       unsigned int flags)
+{
+	struct ltt_chanbuf *buf = in->private_data;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+	unsigned int poff, subbuf_pages, nr_pages;
+	struct page *pages[PIPE_BUFFERS];
+	struct partial_page partial[PIPE_BUFFERS];
+	struct splice_pipe_desc spd = {
+		.pages = pages,
+		.nr_pages = 0,
+		.partial = partial,
+		.flags = flags,
+		.ops = &ltt_relay_pipe_buf_ops,
+		.spd_release = ltt_relay_page_release,
+	};
+	long consumed_old, consumed_idx, roffset;
+	unsigned long bytes_avail;
+
+	/*
+	 * Check that a GET_SUBBUF ioctl has been done before.
+	 */
+	WARN_ON(atomic_long_read(&buf->active_readers) != 1);
+	consumed_old = atomic_long_read(&buf->consumed);
+	consumed_old += *ppos;
+	consumed_idx = SUBBUF_INDEX(consumed_old, chan);
+
+	/*
+	 * Adjust read len, if longer than what is available.
+	 * Max read size is 1 subbuffer due to get_subbuf/put_subbuf for
+	 * protection.
+	 */
+	bytes_avail = chan->a.sb_size;
+	WARN_ON(bytes_avail > chan->a.buf_size);
+	len = min_t(size_t, len, bytes_avail);
+	subbuf_pages = bytes_avail >> PAGE_SHIFT;
+	nr_pages = min_t(unsigned int, subbuf_pages, PIPE_BUFFERS);
+	roffset = consumed_old & PAGE_MASK;
+	poff = consumed_old & ~PAGE_MASK;
+	printk_dbg(KERN_DEBUG "SPLICE actor len %zu pos %zd write_pos %ld\n",
+		   len, (ssize_t)*ppos, local_read(&buf->offset));
+
+	for (; spd.nr_pages < nr_pages; spd.nr_pages++) {
+		unsigned int this_len;
+		struct page *page;
+
+		if (!len)
+			break;
+		printk_dbg(KERN_DEBUG "SPLICE actor loop len %zu roffset %ld\n",
+			   len, roffset);
+
+		this_len = PAGE_SIZE - poff;
+		page = ltt_relay_read_get_page(&buf->a, roffset);
+		spd.pages[spd.nr_pages] = page;
+		spd.partial[spd.nr_pages].offset = poff;
+		spd.partial[spd.nr_pages].len = this_len;
+
+		poff = 0;
+		roffset += PAGE_SIZE;
+		len -= this_len;
+	}
+
+	if (!spd.nr_pages)
+		return 0;
+
+	return splice_to_pipe(pipe, &spd);
+}
+
+ssize_t ltt_relay_file_splice_read(struct file *in, loff_t *ppos,
+				   struct pipe_inode_info *pipe, size_t len,
+				   unsigned int flags)
+{
+	ssize_t spliced;
+	int ret;
+
+	ret = 0;
+	spliced = 0;
+
+	printk_dbg(KERN_DEBUG "SPLICE read len %zu pos %zd\n", len,
+		   (ssize_t)*ppos);
+	while (len && !spliced) {
+		ret = subbuf_splice_actor(in, ppos, pipe, len, flags);
+		printk_dbg(KERN_DEBUG "SPLICE read loop ret %d\n", ret);
+		if (ret < 0)
+			break;
+		else if (!ret) {
+			if (flags & SPLICE_F_NONBLOCK)
+				ret = -EAGAIN;
+			break;
+		}
+
+		*ppos += ret;
+		if (ret > len)
+			len = 0;
+		else
+			len -= ret;
+		spliced += ret;
+	}
+
+	if (spliced)
+		return spliced;
+
+	return ret;
+}
diff --git a/stblinux-2.6.31/ltt/ltt-relay-vfs.c b/stblinux-2.6.31/ltt/ltt-relay-vfs.c
new file mode 100644
index 0000000..3036ae1
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-relay-vfs.c
@@ -0,0 +1,233 @@
+/*
+ * ltt/ltt-relay-vfs.c
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng VFS interface.
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/debugfs.h>
+#include <linux/ltt-tracer.h>
+#include <linux/ltt-relay.h>
+#include <linux/ltt-channels.h>
+
+#include <asm/atomic.h>
+
+#include "ltt-relay-select.h"
+
+/**
+ *	ltt_open - open file op for ltt files
+ *	@inode: opened inode
+ *	@file: opened file
+ *
+ *	Open implementation. Makes sure only one open instance of a buffer is
+ *	done at a given moment.
+ */
+static int ltt_open(struct inode *inode, struct file *file)
+{
+	struct ltt_chanbuf *buf = inode->i_private;
+	int ret;
+
+	ret = ltt_chanbuf_open_read(buf);
+	if (ret)
+		goto end;
+
+	file->private_data = buf;
+	ret = nonseekable_open(inode, file);
+end:
+	return ret;
+}
+
+/**
+ *	ltt_release - release file op for ltt files
+ *	@inode: opened inode
+ *	@file: opened file
+ *
+ *	Release implementation.
+ */
+static int ltt_release(struct inode *inode, struct file *file)
+{
+	struct ltt_chanbuf *buf = inode->i_private;
+
+	ltt_chanbuf_release_read(buf);
+
+	return 0;
+}
+
+/**
+ *	ltt_poll - file op for ltt files
+ *	@filp: the file
+ *	@wait: poll table
+ *
+ *	Poll implementation.
+ */
+static unsigned int ltt_poll(struct file *filp, poll_table *wait)
+{
+	unsigned int mask = 0;
+	struct inode *inode = filp->f_dentry->d_inode;
+	struct ltt_chanbuf *buf = inode->i_private;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+
+	if (filp->f_mode & FMODE_READ) {
+		poll_wait_set_exclusive(wait);
+		poll_wait(filp, &buf->read_wait, wait);
+
+		WARN_ON(atomic_long_read(&buf->active_readers) != 1);
+		if (SUBBUF_TRUNC(ltt_chanbuf_get_offset(buf), chan)
+		  - SUBBUF_TRUNC(ltt_chanbuf_get_consumed(buf), chan)
+		  == 0) {
+			if (buf->finalized)
+				return POLLHUP;
+			else
+				return 0;
+		} else {
+			if (SUBBUF_TRUNC(ltt_chanbuf_get_offset(buf), chan)
+			  - SUBBUF_TRUNC(ltt_chanbuf_get_consumed(buf), chan)
+			  >= chan->a.buf_size)
+				return POLLPRI | POLLRDBAND;
+			else
+				return POLLIN | POLLRDNORM;
+		}
+	}
+	return mask;
+}
+
+/**
+ *	ltt_ioctl - control on the debugfs file
+ *
+ *	@inode: the inode
+ *	@filp: the file
+ *	@cmd: the command
+ *	@arg: command arg
+ *
+ *	This ioctl implements three commands necessary for a minimal
+ *	producer/consumer implementation :
+ *	RELAY_GET_SB
+ *		Get the next sub-buffer that can be read. It never blocks.
+ *	RELAY_PUT_SB
+ *		Release the currently read sub-buffer. Parameter is the last
+ *		put subbuffer (returned by GET_SUBBUF).
+ *	RELAY_GET_N_SB
+ *		returns the number of sub-buffers in the per cpu channel.
+ *	RELAY_GET_SB_SIZE
+ *		returns the size of the current sub-buffer.
+ *	RELAY_GET_MAX_SB_SIZE
+ *		returns the maximum size for sub-buffers.
+ */
+static
+int ltt_ioctl(struct inode *inode, struct file *filp, unsigned int cmd,
+	      unsigned long arg)
+{
+	struct ltt_chanbuf *buf = inode->i_private;
+	u32 __user *argp = (u32 __user *)arg;
+
+	switch (cmd) {
+	case RELAY_GET_SB:
+	{
+		unsigned long consumed;
+		int ret;
+
+		ret = ltt_chanbuf_get_subbuf(buf, &consumed);
+		if (ret)
+			return ret;
+		else
+			return put_user((u32)consumed, argp);
+		break;
+	}
+	case RELAY_PUT_SB:
+	{
+		u32 uconsumed_old;
+		int ret;
+		long consumed_old;
+
+		ret = get_user(uconsumed_old, argp);
+		if (ret)
+			return ret; /* will return -EFAULT */
+
+		consumed_old = ltt_chanbuf_get_consumed(buf);
+		consumed_old = consumed_old & (~0xFFFFFFFFL);
+		consumed_old = consumed_old | uconsumed_old;
+		ret = ltt_chanbuf_put_subbuf(buf, consumed_old);
+		if (ret)
+			return ret;
+		break;
+	}
+	case RELAY_GET_N_SB:
+		return put_user((u32)buf->a.chan->n_sb, argp);
+		break;
+	case RELAY_GET_SB_SIZE:
+		return put_user(get_read_sb_size(buf), argp);
+		break;
+	case RELAY_GET_MAX_SB_SIZE:
+		return put_user((u32)buf->a.chan->sb_size, argp);
+		break;
+	default:
+		return -ENOIOCTLCMD;
+	}
+	return 0;
+}
+
+#ifdef CONFIG_COMPAT
+static
+long ltt_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	long ret = -ENOIOCTLCMD;
+
+	lock_kernel();
+	ret = ltt_ioctl(file->f_dentry->d_inode, file, cmd, arg);
+	unlock_kernel();
+
+	return ret;
+}
+#endif
+
+static const struct file_operations ltt_file_operations = {
+	.open = ltt_open,
+	.release = ltt_release,
+	.poll = ltt_poll,
+	.splice_read = ltt_relay_file_splice_read,
+	.ioctl = ltt_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = ltt_compat_ioctl,
+#endif
+};
+
+int ltt_chanbuf_create_file(const char *filename, struct dentry *parent,
+			    int mode, struct ltt_chanbuf *buf)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+	char *tmpname;
+	int ret = 0;
+
+	tmpname = kzalloc(NAME_MAX + 1, GFP_KERNEL);
+	if (!tmpname) {
+		ret = -ENOMEM;
+		goto end;
+	}
+	snprintf(tmpname, NAME_MAX, "%s_%d", chan->a.filename, buf->a.cpu);
+
+	buf->a.dentry = debugfs_create_file(tmpname, mode, parent, buf,
+					    &ltt_file_operations);
+	if (!buf->a.dentry) {
+		ret = -ENOMEM;
+		goto free_name;
+	}
+free_name:
+	kfree(tmpname);
+end:
+	return ret;
+}
+
+int ltt_chanbuf_remove_file(struct ltt_chanbuf *buf)
+{
+	debugfs_remove(buf->a.dentry);
+
+	return 0;
+}
diff --git a/stblinux-2.6.31/ltt/ltt-serialize.c b/stblinux-2.6.31/ltt/ltt-serialize.c
new file mode 100644
index 0000000..83fb490
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-serialize.c
@@ -0,0 +1,915 @@
+/*
+ * LTTng serializing code.
+ *
+ * Copyright Mathieu Desnoyers, March 2007.
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ *
+ * See this discussion about weirdness about passing va_list and then va_list to
+ * functions. (related to array argument passing). va_list seems to be
+ * implemented as an array on x86_64, but not on i386... This is why we pass a
+ * va_list * to ltt_vtrace.
+ */
+
+#include <stdarg.h>
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/module.h>
+#include <linux/ltt-tracer.h>
+
+#include "ltt-relay-select.h"
+
+enum ltt_type {
+	LTT_TYPE_SIGNED_INT,
+	LTT_TYPE_UNSIGNED_INT,
+	LTT_TYPE_STRING,
+	LTT_TYPE_NONE,
+};
+
+#define LTT_ATTRIBUTE_NETWORK_BYTE_ORDER (1<<1)
+
+/*
+ * Inspired from vsnprintf
+ *
+ * The serialization format string supports the basic printf format strings.
+ * In addition, it defines new formats that can be used to serialize more
+ * complex/non portable data structures.
+ *
+ * Typical use:
+ *
+ * field_name %ctype
+ * field_name #tracetype %ctype
+ * field_name #tracetype %ctype1 %ctype2 ...
+ *
+ * A conversion is performed between format string types supported by GCC and
+ * the trace type requested. GCC type is used to perform type checking on format
+ * strings. Trace type is used to specify the exact binary representation
+ * in the trace. A mapping is done between one or more GCC types to one trace
+ * type. Sign extension, if required by the conversion, is performed following
+ * the trace type.
+ *
+ * If a gcc format is not declared with a trace format, the gcc format is
+ * also used as binary representation in the trace.
+ *
+ * Strings are supported with %s.
+ * A single tracetype (sequence) can take multiple c types as parameter.
+ *
+ * c types:
+ *
+ * see printf(3).
+ *
+ * Note: to write a uint32_t in a trace, the following expression is recommended
+ * si it can be portable:
+ *
+ * ("#4u%lu", (unsigned long)var)
+ *
+ * trace types:
+ *
+ * Serialization specific formats :
+ *
+ * Fixed size integers
+ * #1u     writes uint8_t
+ * #2u     writes uint16_t
+ * #4u     writes uint32_t
+ * #8u     writes uint64_t
+ * #1d     writes int8_t
+ * #2d     writes int16_t
+ * #4d     writes int32_t
+ * #8d     writes int64_t
+ * i.e.:
+ * #1u%lu #2u%lu #4d%lu #8d%lu #llu%hu #d%lu
+ *
+ * * Attributes:
+ *
+ * n:  (for network byte order)
+ * #ntracetype%ctype
+ *            is written in the trace in network byte order.
+ *
+ * i.e.: #bn4u%lu, #n%lu, #b%u
+ *
+ * TODO (eventually)
+ * Variable length sequence
+ * #a #tracetype1 #tracetype2 %array_ptr %elem_size %num_elems
+ *            In the trace:
+ *            #a specifies that this is a sequence
+ *            #tracetype1 is the type of elements in the sequence
+ *            #tracetype2 is the type of the element count
+ *            GCC input:
+ *            array_ptr is a pointer to an array that contains members of size
+ *            elem_size.
+ *            num_elems is the number of elements in the array.
+ * i.e.: #a #lu #lu %p %lu %u
+ *
+ * Callback
+ * #k         callback (taken from the probe data)
+ *            The following % arguments are exepected by the callback
+ *
+ * i.e.: #a #lu #lu #k %p
+ *
+ * Note: No conversion is done from floats to integers, nor from integers to
+ * floats between c types and trace types. float conversion from double to float
+ * or from float to double is also not supported.
+ *
+ * REMOVE
+ * %*b     expects sizeof(data), data
+ *         where sizeof(data) is 1, 2, 4 or 8
+ *
+ * Fixed length struct, union or array.
+ * FIXME: unable to extract those sizes statically.
+ * %*r     expects sizeof(*ptr), ptr
+ * %*.*r   expects sizeof(*ptr), __alignof__(*ptr), ptr
+ * struct and unions removed.
+ * Fixed length array:
+ * [%p]#a[len #tracetype]
+ * i.e.: [%p]#a[12 #lu]
+ *
+ * Variable length sequence
+ * %*.*:*v expects sizeof(*ptr), __alignof__(*ptr), elem_num, ptr
+ *         where elem_num is the number of elements in the sequence
+ */
+static inline
+const char *parse_trace_type(const char *fmt, char *trace_size,
+			     enum ltt_type *trace_type,
+			     unsigned long *attributes)
+{
+	int qualifier;		/* 'h', 'l', or 'L' for integer fields */
+				/* 'z' support added 23/7/1999 S.H.    */
+				/* 'z' changed to 'Z' --davidm 1/25/99 */
+				/* 't' added for ptrdiff_t */
+
+	/* parse attributes. */
+repeat:
+	switch (*fmt) {
+	case 'n':
+		*attributes |= LTT_ATTRIBUTE_NETWORK_BYTE_ORDER;
+		++fmt;
+		goto repeat;
+	}
+
+	/* get the conversion qualifier */
+	qualifier = -1;
+	if (*fmt == 'h' || *fmt == 'l' || *fmt == 'L' ||
+	    *fmt == 'Z' || *fmt == 'z' || *fmt == 't' ||
+	    *fmt == 'S' || *fmt == '1' || *fmt == '2' ||
+	    *fmt == '4' || *fmt == 8) {
+		qualifier = *fmt;
+		++fmt;
+		if (qualifier == 'l' && *fmt == 'l') {
+			qualifier = 'L';
+			++fmt;
+		}
+	}
+
+	switch (*fmt) {
+	case 'c':
+		*trace_type = LTT_TYPE_UNSIGNED_INT;
+		*trace_size = sizeof(unsigned char);
+		goto parse_end;
+	case 's':
+		*trace_type = LTT_TYPE_STRING;
+		goto parse_end;
+	case 'p':
+		*trace_type = LTT_TYPE_UNSIGNED_INT;
+		*trace_size = sizeof(void *);
+		goto parse_end;
+	case 'd':
+	case 'i':
+		*trace_type = LTT_TYPE_SIGNED_INT;
+		break;
+	case 'o':
+	case 'u':
+	case 'x':
+	case 'X':
+		*trace_type = LTT_TYPE_UNSIGNED_INT;
+		break;
+	default:
+		if (!*fmt)
+			--fmt;
+		goto parse_end;
+	}
+	switch (qualifier) {
+	case 'L':
+		*trace_size = sizeof(long long);
+		break;
+	case 'l':
+		*trace_size = sizeof(long);
+		break;
+	case 'Z':
+	case 'z':
+		*trace_size = sizeof(size_t);
+		break;
+	case 't':
+		*trace_size = sizeof(ptrdiff_t);
+		break;
+	case 'h':
+		*trace_size = sizeof(short);
+		break;
+	case '1':
+		*trace_size = sizeof(uint8_t);
+		break;
+	case '2':
+		*trace_size = sizeof(uint16_t);
+		break;
+	case '4':
+		*trace_size = sizeof(uint32_t);
+		break;
+	case '8':
+		*trace_size = sizeof(uint64_t);
+		break;
+	default:
+		*trace_size = sizeof(int);
+	}
+
+parse_end:
+	return fmt;
+}
+
+/*
+ * Restrictions:
+ * Field width and precision are *not* supported.
+ * %n not supported.
+ */
+static inline
+const char *parse_c_type(const char *fmt, char *c_size, enum ltt_type *c_type,
+			 char *outfmt)
+{
+	int qualifier;		/* 'h', 'l', or 'L' for integer fields */
+				/* 'z' support added 23/7/1999 S.H.    */
+				/* 'z' changed to 'Z' --davidm 1/25/99 */
+				/* 't' added for ptrdiff_t */
+
+	/* process flags : ignore standard print formats for now. */
+repeat:
+	switch (*fmt) {
+	case '-':
+	case '+':
+	case ' ':
+	case '#':
+	case '0':
+		++fmt;
+		goto repeat;
+	}
+
+	/* get the conversion qualifier */
+	qualifier = -1;
+	if (*fmt == 'h' || *fmt == 'l' || *fmt == 'L' ||
+	    *fmt == 'Z' || *fmt == 'z' || *fmt == 't' ||
+	    *fmt == 'S') {
+		qualifier = *fmt;
+		++fmt;
+		if (qualifier == 'l' && *fmt == 'l') {
+			qualifier = 'L';
+			++fmt;
+		}
+	}
+
+	if (outfmt) {
+		if (qualifier != -1)
+			*outfmt++ = (char)qualifier;
+		*outfmt++ = *fmt;
+		*outfmt = 0;
+	}
+
+	switch (*fmt) {
+	case 'c':
+		*c_type = LTT_TYPE_UNSIGNED_INT;
+		*c_size = sizeof(unsigned char);
+		goto parse_end;
+	case 's':
+		*c_type = LTT_TYPE_STRING;
+		goto parse_end;
+	case 'p':
+		*c_type = LTT_TYPE_UNSIGNED_INT;
+		*c_size = sizeof(void *);
+		goto parse_end;
+	case 'd':
+	case 'i':
+		*c_type = LTT_TYPE_SIGNED_INT;
+		break;
+	case 'o':
+	case 'u':
+	case 'x':
+	case 'X':
+		*c_type = LTT_TYPE_UNSIGNED_INT;
+		break;
+	default:
+		if (!*fmt)
+			--fmt;
+		goto parse_end;
+	}
+	switch (qualifier) {
+	case 'L':
+		*c_size = sizeof(long long);
+		break;
+	case 'l':
+		*c_size = sizeof(long);
+		break;
+	case 'Z':
+	case 'z':
+		*c_size = sizeof(size_t);
+		break;
+	case 't':
+		*c_size = sizeof(ptrdiff_t);
+		break;
+	case 'h':
+		*c_size = sizeof(short);
+		break;
+	default:
+		*c_size = sizeof(int);
+	}
+
+parse_end:
+	return fmt;
+}
+
+static inline
+size_t serialize_trace_data(struct ltt_chanbuf *buf, size_t buf_offset,
+			    char trace_size, enum ltt_type trace_type,
+			    char c_size, enum ltt_type c_type,
+			    int *largest_align, va_list *args)
+{
+	union {
+		unsigned long v_ulong;
+		uint64_t v_uint64;
+		struct {
+			const char *s;
+			size_t len;
+		} v_string;
+	} tmp;
+
+	/*
+	 * Be careful about sign extension here.
+	 * Sign extension is done with the destination (trace) type.
+	 */
+	switch (trace_type) {
+	case LTT_TYPE_SIGNED_INT:
+		switch (c_size) {
+		case 1:
+			tmp.v_ulong = (long)(int8_t)va_arg(*args, int);
+			break;
+		case 2:
+			tmp.v_ulong = (long)(int16_t)va_arg(*args, int);
+			break;
+		case 4:
+			tmp.v_ulong = (long)(int32_t)va_arg(*args, int);
+			break;
+		case 8:
+			tmp.v_uint64 = va_arg(*args, int64_t);
+			break;
+		default:
+			BUG();
+		}
+		break;
+	case LTT_TYPE_UNSIGNED_INT:
+		switch (c_size) {
+		case 1:
+			tmp.v_ulong = (unsigned long)(uint8_t)va_arg(*args, unsigned int);
+			break;
+		case 2:
+			tmp.v_ulong = (unsigned long)(uint16_t)va_arg(*args, unsigned int);
+			break;
+		case 4:
+			tmp.v_ulong = (unsigned long)(uint32_t)va_arg(*args, unsigned int);
+			break;
+		case 8:
+			tmp.v_uint64 = va_arg(*args, uint64_t);
+			break;
+		default:
+			BUG();
+		}
+		break;
+	case LTT_TYPE_STRING:
+		tmp.v_string.s = va_arg(*args, const char *);
+		if ((unsigned long)tmp.v_string.s < PAGE_SIZE)
+			tmp.v_string.s = "<NULL>";
+		tmp.v_string.len = strlen(tmp.v_string.s)+1;
+		if (buf)
+			ltt_relay_write(&buf->a, buf->a.chan, buf_offset,
+					tmp.v_string.s, tmp.v_string.len);
+		buf_offset += tmp.v_string.len;
+		goto copydone;
+	default:
+		BUG();
+	}
+
+	/*
+	 * If trace_size is lower or equal to 4 bytes, there is no sign
+	 * extension to do because we are already encoded in a long. Therefore,
+	 * we can combine signed and unsigned ops. 4 bytes float also works
+	 * with this, because we do a simple copy of 4 bytes into 4 bytes
+	 * without manipulation (and we do not support conversion from integers
+	 * to floats).
+	 * It is also the case if c_size is 8 bytes, which is the largest
+	 * possible integer.
+	 */
+	if (ltt_get_alignment()) {
+		buf_offset += ltt_align(buf_offset, trace_size);
+		if (largest_align)
+			*largest_align = max_t(int, *largest_align, trace_size);
+	}
+	if (trace_size <= 4 || c_size == 8) {
+		if (buf) {
+			switch (trace_size) {
+			case 1:
+				if (c_size == 8)
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
+					(uint8_t[]){ (uint8_t)tmp.v_uint64 },
+					sizeof(uint8_t));
+				else
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
+					(uint8_t[]){ (uint8_t)tmp.v_ulong },
+					sizeof(uint8_t));
+				break;
+			case 2:
+				if (c_size == 8)
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
+					(uint16_t[]){ (uint16_t)tmp.v_uint64 },
+					sizeof(uint16_t));
+				else
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
+					(uint16_t[]){ (uint16_t)tmp.v_ulong },
+					sizeof(uint16_t));
+				break;
+			case 4:
+				if (c_size == 8)
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
+					(uint32_t[]){ (uint32_t)tmp.v_uint64 },
+					sizeof(uint32_t));
+				else
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
+					(uint32_t[]){ (uint32_t)tmp.v_ulong },
+					sizeof(uint32_t));
+				break;
+			case 8:
+				/*
+				 * c_size cannot be other than 8 here because
+				 * trace_size > 4.
+				 */
+				ltt_relay_write(&buf->a, buf->a.chan, buf_offset,
+				(uint64_t[]){ (uint64_t)tmp.v_uint64 },
+				sizeof(uint64_t));
+				break;
+			default:
+				BUG();
+			}
+		}
+		buf_offset += trace_size;
+		goto copydone;
+	} else {
+		/*
+		 * Perform sign extension.
+		 */
+		if (buf) {
+			switch (trace_type) {
+			case LTT_TYPE_SIGNED_INT:
+				ltt_relay_write(&buf->a, buf->a.chan, buf_offset,
+					(int64_t[]){ (int64_t)tmp.v_ulong },
+					sizeof(int64_t));
+				break;
+			case LTT_TYPE_UNSIGNED_INT:
+				ltt_relay_write(&buf->a, buf->a.chan, buf_offset,
+					(uint64_t[]){ (uint64_t)tmp.v_ulong },
+					sizeof(uint64_t));
+				break;
+			default:
+				BUG();
+			}
+		}
+		buf_offset += trace_size;
+		goto copydone;
+	}
+
+copydone:
+	return buf_offset;
+}
+
+notrace size_t
+ltt_serialize_data(struct ltt_chanbuf *buf, size_t buf_offset,
+		   struct ltt_serialize_closure *closure,
+		   void *serialize_private, int *largest_align,
+		   const char *fmt, va_list *args)
+{
+	char trace_size = 0, c_size = 0;	/*
+						 * 0 (unset), 1, 2, 4, 8 bytes.
+						 */
+	enum ltt_type trace_type = LTT_TYPE_NONE, c_type = LTT_TYPE_NONE;
+	unsigned long attributes = 0;
+
+	for (; *fmt ; ++fmt) {
+		switch (*fmt) {
+		case '#':
+			/* tracetypes (#) */
+			++fmt;			/* skip first '#' */
+			if (*fmt == '#')	/* Escaped ## */
+				break;
+			attributes = 0;
+			fmt = parse_trace_type(fmt, &trace_size, &trace_type,
+					       &attributes);
+			break;
+		case '%':
+			/* c types (%) */
+			++fmt;			/* skip first '%' */
+			if (*fmt == '%')	/* Escaped %% */
+				break;
+			fmt = parse_c_type(fmt, &c_size, &c_type, NULL);
+			/*
+			 * Output c types if no trace types has been
+			 * specified.
+			 */
+			if (!trace_size)
+				trace_size = c_size;
+			if (trace_type == LTT_TYPE_NONE)
+				trace_type = c_type;
+			if (c_type == LTT_TYPE_STRING)
+				trace_type = LTT_TYPE_STRING;
+			/* perform trace write */
+			buf_offset = serialize_trace_data(buf, buf_offset,
+							  trace_size,
+							  trace_type, c_size,
+							  c_type,
+							  largest_align, args);
+			trace_size = 0;
+			c_size = 0;
+			trace_type = LTT_TYPE_NONE;
+			c_size = LTT_TYPE_NONE;
+			attributes = 0;
+			break;
+			/* default is to skip the text, doing nothing */
+		}
+	}
+	return buf_offset;
+}
+EXPORT_SYMBOL_GPL(ltt_serialize_data);
+
+static inline
+uint64_t unserialize_base_type(struct ltt_chanbuf *buf,
+			       size_t *ppos, char trace_size,
+			       enum ltt_type trace_type)
+{
+	uint64_t tmp;
+
+	*ppos += ltt_align(*ppos, trace_size);
+	ltt_relay_read(&buf->a, *ppos, &tmp, trace_size);
+	*ppos += trace_size;
+
+	switch (trace_type) {
+	case LTT_TYPE_SIGNED_INT:
+		switch (trace_size) {
+		case 1:
+			return (uint64_t)*(int8_t *)&tmp;
+		case 2:
+			return (uint64_t)*(int16_t *)&tmp;
+		case 4:
+			return (uint64_t)*(int32_t *)&tmp;
+		case 8:
+			return tmp;
+		}
+		break;
+	case LTT_TYPE_UNSIGNED_INT:
+		switch (trace_size) {
+		case 1:
+			return (uint64_t)*(uint8_t *)&tmp;
+		case 2:
+			return (uint64_t)*(uint16_t *)&tmp;
+		case 4:
+			return (uint64_t)*(uint32_t *)&tmp;
+		case 8:
+			return tmp;
+		}
+		break;
+	default:
+		break;
+	}
+
+	BUG();
+	return 0;
+}
+
+static
+int serialize_printf_data(struct ltt_chanbuf *buf, size_t *ppos,
+			  char trace_size, enum ltt_type trace_type,
+			  char c_size, enum ltt_type c_type, char *output,
+			  ssize_t outlen, const char *outfmt)
+{
+	u64 value;
+	outlen = outlen < 0 ? 0 : outlen;
+
+	if (trace_type == LTT_TYPE_STRING) {
+		size_t len = ltt_relay_read_cstr(&buf->a, *ppos, output,
+						 outlen);
+		*ppos += len + 1;
+		return len;
+	}
+
+	value = unserialize_base_type(buf, ppos, trace_size, trace_type);
+
+	if (c_size == 8)
+		return snprintf(output, outlen, outfmt, value);
+	else
+		return snprintf(output, outlen, outfmt, (unsigned int)value);
+}
+
+/**
+ * ltt_serialize_printf - Format a string and place it in a buffer
+ * @buf: The ltt-relay buffer that store binary data
+ * @buf_offset: binary data's offset in @buf (should be masked to use as offset)
+ * @msg_size: return message's length
+ * @output: The buffer to place the result into
+ * @outlen: The size of the buffer, including the trailing '\0'
+ * @fmt: The format string to use
+ *
+ * The return value is the number of characters which would
+ * be generated for the given input, excluding the trailing
+ * '\0', as per ISO C99. If the return is greater than or equal to @outlen,
+ * the resulting string is truncated.
+ */
+size_t ltt_serialize_printf(struct ltt_chanbuf *buf, unsigned long buf_offset,
+			    size_t *msg_size, char *output, size_t outlen,
+			    const char *fmt)
+{
+	char trace_size = 0, c_size = 0;	/*
+						 * 0 (unset), 1, 2, 4, 8 bytes.
+						 */
+	enum ltt_type trace_type = LTT_TYPE_NONE, c_type = LTT_TYPE_NONE;
+	unsigned long attributes = 0;
+	char outfmt[4] = "%";
+	size_t outpos = 0;
+	size_t len;
+	size_t msgpos = buf_offset;
+
+	for (; *fmt ; ++fmt) {
+		switch (*fmt) {
+		case '#':
+			/* tracetypes (#) */
+			++fmt;			/* skip first '#' */
+			if (*fmt == '#') {	/* Escaped ## */
+				if (outpos < outlen)
+					output[outpos] = '#';
+				outpos++;
+				break;
+			}
+			attributes = 0;
+			fmt = parse_trace_type(fmt, &trace_size, &trace_type,
+					       &attributes);
+			break;
+		case '%':
+			/* c types (%) */
+			++fmt;			/* skip first '%' */
+			if (*fmt == '%') {	/* Escaped %% */
+				if (outpos < outlen)
+					output[outpos] = '%';
+				outpos++;
+				break;
+			}
+			fmt = parse_c_type(fmt, &c_size, &c_type, outfmt + 1);
+			/*
+			 * Output c types if no trace types has been
+			 * specified.
+			 */
+			if (!trace_size)
+				trace_size = c_size;
+			if (trace_type == LTT_TYPE_NONE)
+				trace_type = c_type;
+			if (c_type == LTT_TYPE_STRING)
+				trace_type = LTT_TYPE_STRING;
+
+			/* perform trace printf */
+			len = serialize_printf_data(buf, &msgpos, trace_size,
+						    trace_type, c_size, c_type,
+						    output + outpos,
+						    outlen - outpos, outfmt);
+			outpos += len;
+			trace_size = 0;
+			c_size = 0;
+			trace_type = LTT_TYPE_NONE;
+			c_size = LTT_TYPE_NONE;
+			attributes = 0;
+			break;
+		default:
+			if (outpos < outlen)
+				output[outpos] = *fmt;
+			outpos++;
+			break;
+		}
+	}
+	if (msg_size)
+		*msg_size = (size_t)(msgpos - buf_offset);
+	/*
+	 * Make sure we end output with terminating \0 when truncated.
+	 */
+	if (outpos >= outlen + 1)
+		output[outlen] = '\0';
+	return outpos;
+}
+EXPORT_SYMBOL_GPL(ltt_serialize_printf);
+
+#ifdef CONFIG_LTT_ALIGNMENT
+
+unsigned int ltt_fmt_largest_align(size_t align_drift, const char *fmt)
+{
+	char trace_size = 0, c_size = 0;
+	enum ltt_type trace_type = LTT_TYPE_NONE, c_type = LTT_TYPE_NONE;
+	unsigned long attributes = 0;
+	int largest_align = 1;
+
+	for (; *fmt ; ++fmt) {
+		switch (*fmt) {
+		case '#':
+			/* tracetypes (#) */
+			++fmt;			/* skip first '#' */
+			if (*fmt == '#')	/* Escaped ## */
+				break;
+			attributes = 0;
+			fmt = parse_trace_type(fmt, &trace_size, &trace_type,
+					       &attributes);
+
+			largest_align = max_t(int, largest_align, trace_size);
+			break;
+		case '%':
+			/* c types (%) */
+			++fmt;			/* skip first '%' */
+			if (*fmt == '%')	/* Escaped %% */
+				break;
+			fmt = parse_c_type(fmt, &c_size, &c_type, NULL);
+			/*
+			 * Output c types if no trace types has been
+			 * specified.
+			 */
+			if (!trace_size)
+				trace_size = c_size;
+			if (trace_type == LTT_TYPE_NONE)
+				trace_type = c_type;
+			if (c_type == LTT_TYPE_STRING)
+				trace_type = LTT_TYPE_STRING;
+
+			largest_align = max_t(int, largest_align, trace_size);
+
+			trace_size = 0;
+			c_size = 0;
+			trace_type = LTT_TYPE_NONE;
+			c_size = LTT_TYPE_NONE;
+			break;
+		}
+	}
+
+	return (largest_align - align_drift) & (largest_align - 1);
+}
+EXPORT_SYMBOL_GPL(ltt_fmt_largest_align);
+
+#endif
+
+/*
+ * Calculate data size
+ * Assume that the padding for alignment starts at a sizeof(void *) address.
+ */
+static notrace
+size_t ltt_get_data_size(struct ltt_serialize_closure *closure,
+			 void *serialize_private, int *largest_align,
+			 const char *fmt, va_list *args)
+{
+	ltt_serialize_cb cb = closure->callbacks[0];
+	closure->cb_idx = 0;
+	return (size_t)cb(NULL, 0, closure, serialize_private, largest_align,
+			  fmt, args);
+}
+
+static notrace
+void ltt_write_event_data(struct ltt_chanbuf *buf, size_t buf_offset,
+			  struct ltt_serialize_closure *closure,
+			  void *serialize_private, int largest_align,
+			  const char *fmt, va_list *args)
+{
+	ltt_serialize_cb cb = closure->callbacks[0];
+	closure->cb_idx = 0;
+	buf_offset += ltt_align(buf_offset, largest_align);
+	cb(buf, buf_offset, closure, serialize_private, NULL, fmt, args);
+}
+
+
+notrace
+void ltt_vtrace(const struct marker *mdata, void *probe_data, void *call_data,
+		const char *fmt, va_list *args)
+{
+	int largest_align, ret;
+	struct ltt_active_marker *pdata;
+	uint16_t eID;
+	size_t data_size, slot_size;
+	unsigned int chan_index;
+	struct ltt_chanbuf *buf;
+	struct ltt_chan *chan;
+	struct ltt_trace *trace, *dest_trace = NULL;
+	uint64_t tsc;
+	long buf_offset;
+	va_list args_copy;
+	struct ltt_serialize_closure closure;
+	struct ltt_probe_private_data *private_data = call_data;
+	void *serialize_private = NULL;
+	int cpu;
+	unsigned int rflags;
+
+	/*
+	 * This test is useful for quickly exiting static tracing when no trace
+	 * is active. We expect to have an active trace when we get here.
+	 */
+	if (unlikely(ltt_traces.num_active_traces == 0))
+		return;
+
+	rcu_read_lock_sched_notrace();
+	cpu = smp_processor_id();
+	__get_cpu_var(ltt_nesting)++;
+
+	pdata = (struct ltt_active_marker *)probe_data;
+	eID = mdata->event_id;
+	chan_index = mdata->channel_id;
+	closure.callbacks = pdata->probe->callbacks;
+
+	if (unlikely(private_data)) {
+		dest_trace = private_data->trace;
+		if (private_data->serializer)
+			closure.callbacks = &private_data->serializer;
+		serialize_private = private_data->serialize_private;
+	}
+
+	va_copy(args_copy, *args);
+	/*
+	 * Assumes event payload to start on largest_align alignment.
+	 */
+	largest_align = 1;	/* must be non-zero for ltt_align */
+	data_size = ltt_get_data_size(&closure, serialize_private,
+				      &largest_align, fmt, &args_copy);
+	va_end(args_copy);
+
+	/* Iterate on each trace */
+	list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		/*
+		 * Expect the filter to filter out events. If we get here,
+		 * we went through tracepoint activation as a first step.
+		 */
+		if (unlikely(dest_trace && trace != dest_trace))
+			continue;
+		if (unlikely(!trace->active))
+			continue;
+		if (unlikely(!ltt_run_filter(trace, eID)))
+			continue;
+#ifdef CONFIG_LTT_DEBUG_EVENT_SIZE
+		rflags = LTT_RFLAG_ID_SIZE;
+#else
+		if (unlikely(eID >= LTT_FREE_EVENTS))
+			rflags = LTT_RFLAG_ID;
+		else
+			rflags = 0;
+#endif
+		/*
+		 * Skip channels added after trace creation.
+		 */
+		if (unlikely(chan_index >= trace->nr_channels))
+			continue;
+		chan = &trace->channels[chan_index];
+		if (!chan->active)
+			continue;
+
+		/* reserve space : header and data */
+		ret = ltt_reserve_slot(chan, trace, data_size, largest_align,
+				       cpu, &buf, &slot_size, &buf_offset,
+				       &tsc, &rflags);
+		if (unlikely(ret < 0))
+			continue; /* buffer full */
+
+		va_copy(args_copy, *args);
+		/* Out-of-order write : header and data */
+		buf_offset = ltt_write_event_header(&buf->a, &chan->a,
+						    buf_offset, eID, data_size,
+						    tsc, rflags);
+		ltt_write_event_data(buf, buf_offset, &closure,
+				     serialize_private, largest_align, fmt,
+				     &args_copy);
+		va_end(args_copy);
+		/* Out-of-order commit */
+		ltt_commit_slot(buf, chan, buf_offset, data_size, slot_size);
+	}
+	__get_cpu_var(ltt_nesting)--;
+	rcu_read_unlock_sched_notrace();
+}
+EXPORT_SYMBOL_GPL(ltt_vtrace);
+
+notrace
+void ltt_trace(const struct marker *mdata, void *probe_data, void *call_data,
+	       const char *fmt, ...)
+{
+	va_list args;
+
+	va_start(args, fmt);
+	ltt_vtrace(mdata, probe_data, call_data, fmt, &args);
+	va_end(args);
+}
+EXPORT_SYMBOL_GPL(ltt_trace);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Serializer");
diff --git a/stblinux-2.6.31/ltt/ltt-statedump.c b/stblinux-2.6.31/ltt/ltt-statedump.c
new file mode 100644
index 0000000..8c079aa
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-statedump.c
@@ -0,0 +1,438 @@
+/*
+ * Linux Trace Toolkit Kernel State Dump
+ *
+ * Copyright 2005 -
+ * Jean-Hugues Deschenes <jean-hugues.deschenes@polymtl.ca>
+ *
+ * Changes:
+ *	Eric Clement:                   Add listing of network IP interface
+ *	2006, 2007 Mathieu Desnoyers	Fix kernel threads
+ *	                                Various updates
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/ltt-tracer.h>
+#include <linux/netlink.h>
+#include <linux/inet.h>
+#include <linux/ip.h>
+#include <linux/kthread.h>
+#include <linux/proc_fs.h>
+#include <linux/file.h>
+#include <linux/interrupt.h>
+#include <linux/irqnr.h>
+#include <linux/cpu.h>
+#include <linux/netdevice.h>
+#include <linux/inetdevice.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/marker.h>
+#include <linux/fdtable.h>
+#include <linux/swap.h>
+#include <linux/wait.h>
+#include <linux/mutex.h>
+
+#ifdef CONFIG_GENERIC_HARDIRQS
+#include <linux/irq.h>
+#endif
+
+#define NB_PROC_CHUNK 20
+
+/*
+ * Protected by the trace lock.
+ */
+static struct delayed_work cpu_work[NR_CPUS];
+static DECLARE_WAIT_QUEUE_HEAD(statedump_wq);
+static atomic_t kernel_threads_to_run;
+
+static void empty_cb(void *call_data)
+{
+}
+
+static DEFINE_MUTEX(statedump_cb_mutex);
+static void (*ltt_dump_kprobes_table_cb)(void *call_data) = empty_cb;
+
+enum lttng_thread_type {
+	LTTNG_USER_THREAD = 0,
+	LTTNG_KERNEL_THREAD = 1,
+};
+
+enum lttng_execution_mode {
+	LTTNG_USER_MODE = 0,
+	LTTNG_SYSCALL = 1,
+	LTTNG_TRAP = 2,
+	LTTNG_IRQ = 3,
+	LTTNG_SOFTIRQ = 4,
+	LTTNG_MODE_UNKNOWN = 5,
+};
+
+enum lttng_execution_submode {
+	LTTNG_NONE = 0,
+	LTTNG_UNKNOWN = 1,
+};
+
+enum lttng_process_status {
+	LTTNG_UNNAMED = 0,
+	LTTNG_WAIT_FORK = 1,
+	LTTNG_WAIT_CPU = 2,
+	LTTNG_EXIT = 3,
+	LTTNG_ZOMBIE = 4,
+	LTTNG_WAIT = 5,
+	LTTNG_RUN = 6,
+	LTTNG_DEAD = 7,
+};
+
+#ifdef CONFIG_INET
+static void ltt_enumerate_device(struct ltt_probe_private_data *call_data,
+				 struct net_device *dev)
+{
+	struct in_device *in_dev;
+	struct in_ifaddr *ifa;
+
+	if (dev->flags & IFF_UP) {
+		in_dev = in_dev_get(dev);
+		if (in_dev) {
+			for (ifa = in_dev->ifa_list; ifa != NULL;
+			     ifa = ifa->ifa_next)
+				__trace_mark(0, netif_state,
+					     network_ipv4_interface,
+					     call_data,
+					     "name %s address #n4u%lu up %d",
+					     dev->name,
+					     (unsigned long)ifa->ifa_address,
+					     0);
+			in_dev_put(in_dev);
+		}
+	} else
+		__trace_mark(0, netif_state, network_ip_interface,
+			     call_data, "name %s address #n4u%lu up %d",
+			     dev->name, 0UL, 0);
+}
+
+static inline int
+ltt_enumerate_network_ip_interface(struct ltt_probe_private_data *call_data)
+{
+	struct net_device *dev;
+
+	read_lock(&dev_base_lock);
+	for_each_netdev(&init_net, dev)
+		ltt_enumerate_device(call_data, dev);
+	read_unlock(&dev_base_lock);
+
+	return 0;
+}
+#else /* CONFIG_INET */
+static inline int
+ltt_enumerate_network_ip_interface(struct ltt_probe_private_data *call_data)
+{
+	return 0;
+}
+#endif /* CONFIG_INET */
+
+
+static inline void
+ltt_enumerate_task_fd(struct ltt_probe_private_data *call_data,
+		      struct task_struct *t, char *tmp)
+{
+	struct fdtable *fdt;
+	struct file *filp;
+	unsigned int i;
+	const unsigned char *path;
+
+	if (!t->files)
+		return;
+
+	spin_lock(&t->files->file_lock);
+	fdt = files_fdtable(t->files);
+	for (i = 0; i < fdt->max_fds; i++) {
+		filp = fcheck_files(t->files, i);
+		if (!filp)
+			continue;
+		path = d_path(&filp->f_path, tmp, PAGE_SIZE);
+		/* Make sure we give at least some info */
+		__trace_mark(0, fd_state, file_descriptor, call_data,
+			     "filename %s pid %d fd %u",
+			     (IS_ERR(path))?(filp->f_dentry->d_name.name):(path),
+			     t->pid, i);
+	}
+	spin_unlock(&t->files->file_lock);
+}
+
+static inline int
+ltt_enumerate_file_descriptors(struct ltt_probe_private_data *call_data)
+{
+	struct task_struct *t = &init_task;
+	char *tmp = (char *)__get_free_page(GFP_KERNEL);
+
+	/* Enumerate active file descriptors */
+	do {
+		read_lock(&tasklist_lock);
+		if (t != &init_task)
+			atomic_dec(&t->usage);
+		t = next_task(t);
+		atomic_inc(&t->usage);
+		read_unlock(&tasklist_lock);
+		task_lock(t);
+		ltt_enumerate_task_fd(call_data, t, tmp);
+		task_unlock(t);
+	} while (t != &init_task);
+	free_page((unsigned long)tmp);
+	return 0;
+}
+
+static inline void
+ltt_enumerate_task_vm_maps(struct ltt_probe_private_data *call_data,
+		struct task_struct *t)
+{
+	struct mm_struct *mm;
+	struct vm_area_struct *map;
+	unsigned long ino;
+
+	/* get_task_mm does a task_lock... */
+	mm = get_task_mm(t);
+	if (!mm)
+		return;
+
+	map = mm->mmap;
+	if (map) {
+		down_read(&mm->mmap_sem);
+		while (map) {
+			if (map->vm_file)
+				ino = map->vm_file->f_dentry->d_inode->i_ino;
+			else
+				ino = 0;
+			__trace_mark(0, vm_state, vm_map, call_data,
+				     "pid %d start %lu end %lu flags %lu "
+				     "pgoff %lu inode %lu",
+				     t->pid, map->vm_start, map->vm_end,
+				     map->vm_flags, map->vm_pgoff << PAGE_SHIFT,
+				     ino);
+			map = map->vm_next;
+		}
+		up_read(&mm->mmap_sem);
+	}
+	mmput(mm);
+}
+
+static inline int
+ltt_enumerate_vm_maps(struct ltt_probe_private_data *call_data)
+{
+	struct task_struct *t = &init_task;
+
+	do {
+		read_lock(&tasklist_lock);
+		if (t != &init_task)
+			atomic_dec(&t->usage);
+		t = next_task(t);
+		atomic_inc(&t->usage);
+		read_unlock(&tasklist_lock);
+		ltt_enumerate_task_vm_maps(call_data, t);
+	} while (t != &init_task);
+	return 0;
+}
+
+#ifdef CONFIG_GENERIC_HARDIRQS
+static inline void list_interrupts(struct ltt_probe_private_data *call_data)
+{
+	unsigned int irq;
+	unsigned long flags = 0;
+	struct irq_desc *desc;
+
+	/* needs irq_desc */
+	for_each_irq_desc(irq, desc) {
+		struct irqaction *action;
+		const char *irq_chip_name =
+			desc->chip->name ? : "unnamed_irq_chip";
+
+		spin_lock_irqsave(&desc->lock, flags);
+		for (action = desc->action; action; action = action->next)
+			__trace_mark(0, irq_state, interrupt, call_data,
+				     "name %s action %s irq_id %u",
+				     irq_chip_name, action->name, irq);
+		spin_unlock_irqrestore(&desc->lock, flags);
+	}
+}
+#else
+static inline void list_interrupts(struct ltt_probe_private_data *call_data)
+{
+}
+#endif
+
+static inline int
+ltt_enumerate_process_states(struct ltt_probe_private_data *call_data)
+{
+	struct task_struct *t = &init_task;
+	struct task_struct *p = t;
+	enum lttng_process_status status;
+	enum lttng_thread_type type;
+	enum lttng_execution_mode mode;
+	enum lttng_execution_submode submode;
+
+	do {
+		mode = LTTNG_MODE_UNKNOWN;
+		submode = LTTNG_UNKNOWN;
+
+		read_lock(&tasklist_lock);
+		if (t != &init_task) {
+			atomic_dec(&t->usage);
+			t = next_thread(t);
+		}
+		if (t == p) {
+			p = next_task(t);
+			t = p;
+		}
+		atomic_inc(&t->usage);
+		read_unlock(&tasklist_lock);
+
+		task_lock(t);
+
+		if (t->exit_state == EXIT_ZOMBIE)
+			status = LTTNG_ZOMBIE;
+		else if (t->exit_state == EXIT_DEAD)
+			status = LTTNG_DEAD;
+		else if (t->state == TASK_RUNNING) {
+			/* Is this a forked child that has not run yet? */
+			if (list_empty(&t->rt.run_list))
+				status = LTTNG_WAIT_FORK;
+			else
+				/*
+				 * All tasks are considered as wait_cpu;
+				 * the viewer will sort out if the task was
+				 * really running at this time.
+				 */
+				status = LTTNG_WAIT_CPU;
+		} else if (t->state &
+			(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)) {
+			/* Task is waiting for something to complete */
+			status = LTTNG_WAIT;
+		} else
+			status = LTTNG_UNNAMED;
+		submode = LTTNG_NONE;
+
+		/*
+		 * Verification of t->mm is to filter out kernel threads;
+		 * Viewer will further filter out if a user-space thread was
+		 * in syscall mode or not.
+		 */
+		if (t->mm)
+			type = LTTNG_USER_THREAD;
+		else
+			type = LTTNG_KERNEL_THREAD;
+
+		__trace_mark(0, task_state, process_state, call_data,
+			     "pid %d parent_pid %d name %s type %d mode %d "
+			     "submode %d status %d tgid %d",
+			     t->pid, t->parent->pid, t->comm,
+			     type, mode, submode, status, t->tgid);
+		task_unlock(t);
+	} while (t != &init_task);
+
+	return 0;
+}
+
+void ltt_statedump_register_kprobes_dump(void (*callback)(void *call_data))
+{
+	mutex_lock(&statedump_cb_mutex);
+	ltt_dump_kprobes_table_cb = callback;
+	mutex_unlock(&statedump_cb_mutex);
+}
+EXPORT_SYMBOL_GPL(ltt_statedump_register_kprobes_dump);
+
+void ltt_statedump_unregister_kprobes_dump(void (*callback)(void *call_data))
+{
+	mutex_lock(&statedump_cb_mutex);
+	ltt_dump_kprobes_table_cb = empty_cb;
+	mutex_unlock(&statedump_cb_mutex);
+}
+EXPORT_SYMBOL_GPL(ltt_statedump_unregister_kprobes_dump);
+
+void ltt_statedump_work_func(struct work_struct *work)
+{
+	if (atomic_dec_and_test(&kernel_threads_to_run))
+		/* If we are the last thread, wake up do_ltt_statedump */
+		wake_up(&statedump_wq);
+}
+
+static int do_ltt_statedump(struct ltt_probe_private_data *call_data)
+{
+	int cpu;
+	struct module *cb_owner;
+
+	printk(KERN_DEBUG "LTT state dump thread start\n");
+	ltt_enumerate_process_states(call_data);
+	ltt_enumerate_file_descriptors(call_data);
+	list_modules(call_data);
+	ltt_enumerate_vm_maps(call_data);
+	list_interrupts(call_data);
+	ltt_enumerate_network_ip_interface(call_data);
+	ltt_dump_swap_files(call_data);
+	ltt_dump_sys_call_table(call_data);
+	ltt_dump_softirq_vec(call_data);
+	ltt_dump_idt_table(call_data);
+
+	mutex_lock(&statedump_cb_mutex);
+
+	cb_owner = __module_address((unsigned long)ltt_dump_kprobes_table_cb);
+	__module_get(cb_owner);
+	ltt_dump_kprobes_table_cb(call_data);
+	module_put(cb_owner);
+
+	mutex_unlock(&statedump_cb_mutex);
+
+	/*
+	 * Fire off a work queue on each CPU. Their sole purpose in life
+	 * is to guarantee that each CPU has been in a state where is was in
+	 * syscall mode (i.e. not in a trap, an IRQ or a soft IRQ).
+	 */
+	get_online_cpus();
+	atomic_set(&kernel_threads_to_run, num_online_cpus());
+	for_each_online_cpu(cpu) {
+		INIT_DELAYED_WORK(&cpu_work[cpu], ltt_statedump_work_func);
+		schedule_delayed_work_on(cpu, &cpu_work[cpu], 0);
+	}
+	/* Wait for all threads to run */
+	__wait_event(statedump_wq, (atomic_read(&kernel_threads_to_run) != 0));
+	put_online_cpus();
+	/* Our work is done */
+	printk(KERN_DEBUG "LTT state dump end\n");
+	__trace_mark(0, global_state, statedump_end,
+		     call_data, MARK_NOARGS);
+	return 0;
+}
+
+/*
+ * Called with trace lock held.
+ */
+int ltt_statedump_start(struct ltt_trace *trace)
+{
+	struct ltt_probe_private_data call_data;
+	printk(KERN_DEBUG "LTT state dump begin\n");
+
+	call_data.trace = trace;
+	call_data.serializer = NULL;
+	return do_ltt_statedump(&call_data);
+}
+
+static int __init statedump_init(void)
+{
+	int ret;
+	printk(KERN_DEBUG "LTT : State dump init\n");
+	ret = ltt_module_register(LTT_FUNCTION_STATEDUMP,
+			ltt_statedump_start, THIS_MODULE);
+	return ret;
+}
+
+static void __exit statedump_exit(void)
+{
+	printk(KERN_DEBUG "LTT : State dump exit\n");
+	ltt_module_unregister(LTT_FUNCTION_STATEDUMP);
+}
+
+module_init(statedump_init)
+module_exit(statedump_exit)
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Jean-Hugues Deschenes");
+MODULE_DESCRIPTION("Linux Trace Toolkit Statedump");
diff --git a/stblinux-2.6.31/ltt/ltt-trace-control.c b/stblinux-2.6.31/ltt/ltt-trace-control.c
new file mode 100644
index 0000000..71f6460
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-trace-control.c
@@ -0,0 +1,1425 @@
+/*
+ * LTT trace control module over debugfs.
+ *
+ * Copyright 2008 - Zhaolei <zhaolei@cn.fujitsu.com>
+ *
+ * Copyright 2009 - Gui Jianfeng <guijianfeng@cn.fujitsu.com>
+ *                  Make mark-control work in debugfs
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+/*
+ * Todo:
+ *   Impl read operations for control file to read attributes
+ *   Create a README file in ltt control dir, for display help info
+ */
+
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/uaccess.h>
+#include <linux/debugfs.h>
+#include <linux/ltt-tracer.h>
+#include <linux/notifier.h>
+#include <linux/jiffies.h>
+#include <linux/marker.h>
+
+#define LTT_CONTROL_DIR "control"
+#define MARKERS_CONTROL_DIR "markers"
+#define LTT_SETUP_TRACE_FILE "setup_trace"
+#define LTT_DESTROY_TRACE_FILE "destroy_trace"
+
+#define LTT_WRITE_MAXLEN	(128)
+
+struct dentry *ltt_control_dir, *ltt_setup_trace_file, *ltt_destroy_trace_file,
+	*markers_control_dir;
+
+/*
+ * the traces_lock nests inside control_lock.
+ * control_lock protects the consistency of directories presented in ltt
+ * directory.
+ */
+static DEFINE_MUTEX(control_lock);
+
+/*
+ * big note about locking for marker control files :
+ * If a marker control file is added/removed manually racing with module
+ * load/unload, there may be warning messages appearing, but those two
+ * operations should be able to execute concurrently without any lock
+ * synchronizing their operation one wrt another.
+ * Locking the marker mutex, module mutex and also keeping a mutex here
+ * from mkdir/rmdir _and_ from the notifier called from module load/unload makes
+ * life miserable and just asks for deadlocks.
+ */
+
+/*
+ * lookup a file/dir in parent dir.
+ * only designed to work well for debugfs.
+ * (although it maybe ok for other fs)
+ *
+ * return:
+ *	file/dir's dentry on success
+ *	NULL on failure
+ */
+static struct dentry *dir_lookup(struct dentry *parent, const char *name)
+{
+	struct qstr q;
+	struct dentry *d;
+
+	q.name = name;
+	q.len = strlen(name);
+	q.hash = full_name_hash(q.name, q.len);
+
+	d = d_lookup(parent, &q);
+	if (d)
+		dput(d);
+
+	return d;
+}
+
+
+static ssize_t alloc_write(struct file *file, const char __user *user_buf,
+			   size_t count, loff_t *ppos)
+{
+	int err = 0;
+	int buf_size;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *cmd = (char *)__get_free_page(GFP_KERNEL);
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto err_copy_from_user;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%s", cmd) != 1) {
+		err = -EPERM;
+		goto err_get_cmd;
+	}
+
+	if ((cmd[0] != 'Y' && cmd[0] != 'y' && cmd[0] != '1') || cmd[1]) {
+		err = -EPERM;
+		goto err_bad_cmd;
+	}
+
+	err = ltt_trace_alloc(file->f_dentry->d_parent->d_name.name);
+	if (IS_ERR_VALUE(err)) {
+		printk(KERN_ERR "alloc_write: ltt_trace_alloc failed: %d\n",
+			err);
+		goto err_alloc_trace;
+	}
+
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
+	return count;
+
+err_alloc_trace:
+err_bad_cmd:
+err_get_cmd:
+err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
+	return err;
+}
+
+static const struct file_operations ltt_alloc_operations = {
+	.write = alloc_write,
+};
+
+
+static ssize_t enabled_write(struct file *file, const char __user *user_buf,
+			     size_t count, loff_t *ppos)
+{
+	int err = 0;
+	int buf_size;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *cmd = (char *)__get_free_page(GFP_KERNEL);
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto err_copy_from_user;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%s", cmd) != 1) {
+		err = -EPERM;
+		goto err_get_cmd;
+	}
+
+	if (cmd[1]) {
+		err = -EPERM;
+		goto err_bad_cmd;
+	}
+
+	switch (cmd[0]) {
+	case 'Y':
+	case 'y':
+	case '1':
+		err = ltt_trace_start(file->f_dentry->d_parent->d_name.name);
+		if (IS_ERR_VALUE(err)) {
+			printk(KERN_ERR
+			       "enabled_write: ltt_trace_start failed: %d\n",
+			       err);
+			err = -EPERM;
+			goto err_start_trace;
+		}
+		break;
+	case 'N':
+	case 'n':
+	case '0':
+		err = ltt_trace_stop(file->f_dentry->d_parent->d_name.name);
+		if (IS_ERR_VALUE(err)) {
+			printk(KERN_ERR
+			       "enabled_write: ltt_trace_stop failed: %d\n",
+			       err);
+			err = -EPERM;
+			goto err_stop_trace;
+		}
+		break;
+	default:
+		err = -EPERM;
+		goto err_bad_cmd;
+	}
+
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
+	return count;
+
+err_stop_trace:
+err_start_trace:
+err_bad_cmd:
+err_get_cmd:
+err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
+	return err;
+}
+
+static const struct file_operations ltt_enabled_operations = {
+	.write = enabled_write,
+};
+
+
+static ssize_t trans_write(struct file *file, const char __user *user_buf,
+			   size_t count, loff_t *ppos)
+{
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *trans_name = (char *)__get_free_page(GFP_KERNEL);
+	int err = 0;
+	int buf_size;
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto err_copy_from_user;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%s", trans_name) != 1) {
+		err = -EPERM;
+		goto err_get_transname;
+	}
+
+	err = ltt_trace_set_type(file->f_dentry->d_parent->d_name.name,
+				 trans_name);
+	if (IS_ERR_VALUE(err)) {
+		printk(KERN_ERR "trans_write: ltt_trace_set_type failed: %d\n",
+		       err);
+		goto err_set_trans;
+	}
+
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trans_name);
+	return count;
+
+err_set_trans:
+err_get_transname:
+err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trans_name);
+	return err;
+}
+
+static const struct file_operations ltt_trans_operations = {
+	.write = trans_write,
+};
+
+
+static ssize_t channel_subbuf_num_write(struct file *file,
+		const char __user *user_buf, size_t count, loff_t *ppos)
+{
+	int err = 0;
+	int buf_size;
+	unsigned int num;
+	const char *channel_name;
+	const char *trace_name;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto err_copy_from_user;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%u", &num) != 1) {
+		err = -EPERM;
+		goto err_get_number;
+	}
+
+	channel_name = file->f_dentry->d_parent->d_name.name;
+	trace_name = file->f_dentry->d_parent->d_parent->d_parent->d_name.name;
+
+	err = ltt_trace_set_channel_subbufcount(trace_name, channel_name, num);
+	if (IS_ERR_VALUE(err)) {
+		printk(KERN_ERR "channel_subbuf_num_write: "
+		       "ltt_trace_set_channel_subbufcount failed: %d\n", err);
+		goto err_set_subbufcount;
+	}
+
+	free_page((unsigned long)buf);
+	return count;
+
+err_set_subbufcount:
+err_get_number:
+err_copy_from_user:
+	free_page((unsigned long)buf);
+	return err;
+}
+
+static const struct file_operations ltt_channel_subbuf_num_operations = {
+	.write = channel_subbuf_num_write,
+};
+
+
+static
+ssize_t channel_subbuf_size_write(struct file *file,
+				  const char __user *user_buf,
+				  size_t count, loff_t *ppos)
+{
+	int err = 0;
+	int buf_size;
+	unsigned int num;
+	const char *channel_name;
+	const char *trace_name;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto err_copy_from_user;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%u", &num) != 1) {
+		err = -EPERM;
+		goto err_get_number;
+	}
+
+	channel_name = file->f_dentry->d_parent->d_name.name;
+	trace_name = file->f_dentry->d_parent->d_parent->d_parent->d_name.name;
+
+	err = ltt_trace_set_channel_subbufsize(trace_name, channel_name, num);
+	if (IS_ERR_VALUE(err)) {
+		printk(KERN_ERR "channel_subbuf_size_write: "
+		       "ltt_trace_set_channel_subbufsize failed: %d\n", err);
+		goto err_set_subbufsize;
+	}
+
+	free_page((unsigned long)buf);
+	return count;
+
+err_set_subbufsize:
+err_get_number:
+err_copy_from_user:
+	free_page((unsigned long)buf);
+	return err;
+}
+
+static const struct file_operations ltt_channel_subbuf_size_operations = {
+	.write = channel_subbuf_size_write,
+};
+
+static
+ssize_t channel_switch_timer_write(struct file *file,
+				   const char __user *user_buf,
+				   size_t count, loff_t *ppos)
+{
+	int err = 0;
+	int buf_size;
+	unsigned long num;
+	const char *channel_name;
+	const char *trace_name;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto err_copy_from_user;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%lu", &num) != 1) {
+		err = -EPERM;
+		goto err_get_number;
+	}
+
+	channel_name = file->f_dentry->d_parent->d_name.name;
+	trace_name = file->f_dentry->d_parent->d_parent->d_parent->d_name.name;
+
+	/* Convert from ms to jiffies */
+	num = msecs_to_jiffies(num);
+
+	err = ltt_trace_set_channel_switch_timer(trace_name, channel_name, num);
+	if (IS_ERR_VALUE(err)) {
+		printk(KERN_ERR "channel_switch_timer_write: "
+		       "ltt_trace_set_channel_switch_timer failed: %d\n", err);
+		goto err_set_switch_timer;
+	}
+
+	free_page((unsigned long)buf);
+	return count;
+
+err_set_switch_timer:
+err_get_number:
+err_copy_from_user:
+	free_page((unsigned long)buf);
+	return err;
+}
+
+static struct file_operations ltt_channel_switch_timer_operations = {
+	.write = channel_switch_timer_write,
+};
+
+static
+ssize_t channel_overwrite_write(struct file *file,
+				const char __user *user_buf, size_t count,
+				loff_t *ppos)
+{
+	int err = 0;
+	int buf_size;
+	const char *channel_name;
+	const char *trace_name;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *cmd = (char *)__get_free_page(GFP_KERNEL);
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto err_copy_from_user;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%s", cmd) != 1) {
+		err = -EPERM;
+		goto err_get_cmd;
+	}
+
+	if (cmd[1]) {
+		err = -EPERM;
+		goto err_bad_cmd;
+	}
+
+	channel_name = file->f_dentry->d_parent->d_name.name;
+	trace_name = file->f_dentry->d_parent->d_parent->d_parent->d_name.name;
+
+	switch (cmd[0]) {
+	case 'Y':
+	case 'y':
+	case '1':
+		err = ltt_trace_set_channel_overwrite(trace_name, channel_name,
+						      1);
+		if (IS_ERR_VALUE(err)) {
+			printk(KERN_ERR "channel_overwrite_write: "
+			       "ltt_trace_set_channel_overwrite failed: %d\n",
+			       err);
+			goto err_set_subbufsize;
+		}
+		break;
+	case 'N':
+	case 'n':
+	case '0':
+		err = ltt_trace_set_channel_overwrite(trace_name, channel_name,
+						      0);
+		if (IS_ERR_VALUE(err)) {
+			printk(KERN_ERR "channel_overwrite_write: "
+			       "ltt_trace_set_channel_overwrite failed: %d\n",
+			       err);
+			goto err_set_subbufsize;
+		}
+		break;
+	default:
+		err = -EPERM;
+		goto err_bad_cmd;
+	}
+
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
+	return count;
+
+err_set_subbufsize:
+err_bad_cmd:
+err_get_cmd:
+err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
+	return err;
+}
+
+static const struct file_operations ltt_channel_overwrite_operations = {
+	.write = channel_overwrite_write,
+};
+
+
+static
+ssize_t channel_enable_write(struct file *file,
+			     const char __user *user_buf, size_t count,
+			     loff_t *ppos)
+{
+	int err = 0;
+	int buf_size;
+	const char *channel_name;
+	const char *trace_name;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *cmd = (char *)__get_free_page(GFP_KERNEL);
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto err_copy_from_user;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%s", cmd) != 1) {
+		err = -EPERM;
+		goto err_get_cmd;
+	}
+
+	if (cmd[1]) {
+		err = -EPERM;
+		goto err_bad_cmd;
+	}
+
+	channel_name = file->f_dentry->d_parent->d_name.name;
+	trace_name = file->f_dentry->d_parent->d_parent->d_parent->d_name.name;
+
+	switch (cmd[0]) {
+	case 'Y':
+	case 'y':
+	case '1':
+		err = ltt_trace_set_channel_enable(trace_name, channel_name,
+						   1);
+		if (IS_ERR_VALUE(err)) {
+			printk(KERN_ERR "channel_enable_write: "
+			       "ltt_trace_set_channel_enable failed: %d\n",
+			       err);
+			goto err_set_subbufsize;
+		}
+		break;
+	case 'N':
+	case 'n':
+	case '0':
+		err = ltt_trace_set_channel_enable(trace_name, channel_name,
+						   0);
+		if (IS_ERR_VALUE(err)) {
+			printk(KERN_ERR "channel_enable_write: "
+			       "ltt_trace_set_channel_enable failed: %d\n",
+			       err);
+			goto err_set_subbufsize;
+		}
+		break;
+	default:
+		err = -EPERM;
+		goto err_bad_cmd;
+	}
+
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
+	return count;
+
+err_set_subbufsize:
+err_bad_cmd:
+err_get_cmd:
+err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
+	return err;
+}
+
+static const struct file_operations ltt_channel_enable_operations = {
+	.write = channel_enable_write,
+};
+
+
+static int _create_trace_control_dir(const char *trace_name,
+				     struct ltt_trace *trace)
+{
+	int err;
+	struct dentry *trace_root, *channel_root;
+	struct dentry *tmp_den;
+	int i;
+
+	/* debugfs/control/trace_name */
+	trace_root = debugfs_create_dir(trace_name, ltt_control_dir);
+	if (IS_ERR(trace_root) || !trace_root) {
+		printk(KERN_ERR "_create_trace_control_dir: "
+		       "create control root dir of %s failed\n", trace_name);
+		err = -ENOMEM;
+		goto err_create_trace_root;
+	}
+
+	/* debugfs/control/trace_name/alloc */
+	tmp_den = debugfs_create_file("alloc", S_IWUSR, trace_root, NULL,
+				      &ltt_alloc_operations);
+	if (IS_ERR(tmp_den) || !tmp_den) {
+		printk(KERN_ERR "_create_trace_control_dir: "
+		       "create file of alloc failed\n");
+		err = -ENOMEM;
+		goto err_create_subdir;
+	}
+
+	/* debugfs/control/trace_name/trans */
+	tmp_den = debugfs_create_file("trans", S_IWUSR, trace_root, NULL,
+				      &ltt_trans_operations);
+	if (IS_ERR(tmp_den) || !tmp_den) {
+		printk(KERN_ERR "_create_trace_control_dir: "
+		       "create file of trans failed\n");
+		err = -ENOMEM;
+		goto err_create_subdir;
+	}
+
+	/* debugfs/control/trace_name/enabled */
+	tmp_den = debugfs_create_file("enabled", S_IWUSR, trace_root, NULL,
+				      &ltt_enabled_operations);
+	if (IS_ERR(tmp_den) || !tmp_den) {
+		printk(KERN_ERR "_create_trace_control_dir: "
+		       "create file of enabled failed\n");
+		err = -ENOMEM;
+		goto err_create_subdir;
+	}
+
+	/* debugfs/control/trace_name/channel/ */
+	channel_root = debugfs_create_dir("channel", trace_root);
+	if (IS_ERR(channel_root) || !channel_root) {
+		printk(KERN_ERR "_create_trace_control_dir: "
+		       "create dir of channel failed\n");
+		err = -ENOMEM;
+		goto err_create_subdir;
+	}
+
+	/*
+	 * Create dir and files in debugfs/ltt/control/trace_name/channel/
+	 * Following things(without <>) will be created:
+	 * `-- <control>
+	 *     `-- <trace_name>
+	 *         `-- <channel>
+	 *             |-- <channel_name>
+	 *             |   |-- enable
+	 *             |   |-- overwrite
+	 *             |   |-- subbuf_num
+	 *             |   |-- subbuf_size
+	 *             |   `-- switch_timer
+	 *             `-- ...
+	 */
+
+	for (i = 0; i < trace->nr_channels; i++) {
+		struct dentry *channel_den;
+		struct ltt_chan *chan;
+
+		chan = &trace->channels[i];
+		if (!chan->active)
+			continue;
+		channel_den = debugfs_create_dir(chan->a.filename,
+						 channel_root);
+		if (IS_ERR(channel_den) || !channel_den) {
+			printk(KERN_ERR "_create_trace_control_dir: "
+			       "create channel dir of %s failed\n",
+			       chan->a.filename);
+			err = -ENOMEM;
+			goto err_create_subdir;
+		}
+
+		tmp_den = debugfs_create_file("subbuf_num", S_IWUSR,
+					      channel_den, NULL,
+					      &ltt_channel_subbuf_num_operations);
+		if (IS_ERR(tmp_den) || !tmp_den) {
+			printk(KERN_ERR "_create_trace_control_dir: "
+			       "create subbuf_num in %s failed\n",
+			       chan->a.filename);
+			err = -ENOMEM;
+			goto err_create_subdir;
+		}
+
+		tmp_den = debugfs_create_file("subbuf_size", S_IWUSR,
+					      channel_den, NULL,
+					      &ltt_channel_subbuf_size_operations);
+		if (IS_ERR(tmp_den) || !tmp_den) {
+			printk(KERN_ERR "_create_trace_control_dir: "
+			       "create subbuf_size in %s failed\n",
+			       chan->a.filename);
+			err = -ENOMEM;
+			goto err_create_subdir;
+		}
+
+		tmp_den = debugfs_create_file("enable", S_IWUSR, channel_den,
+					      NULL,
+					      &ltt_channel_enable_operations);
+		if (IS_ERR(tmp_den) || !tmp_den) {
+			printk(KERN_ERR "_create_trace_control_dir: "
+			       "create enable in %s failed\n",
+			       chan->a.filename);
+			err = -ENOMEM;
+			goto err_create_subdir;
+		}
+
+		tmp_den = debugfs_create_file("overwrite", S_IWUSR, channel_den,
+					      NULL,
+					      &ltt_channel_overwrite_operations);
+		if (IS_ERR(tmp_den) || !tmp_den) {
+			printk(KERN_ERR "_create_trace_control_dir: "
+			       "create overwrite in %s failed\n",
+			       chan->a.filename);
+			err = -ENOMEM;
+			goto err_create_subdir;
+		}
+
+		tmp_den = debugfs_create_file("switch_timer", S_IWUSR,
+					      channel_den, NULL,
+					      &ltt_channel_switch_timer_operations);
+		if (IS_ERR(tmp_den) || !tmp_den) {
+			printk(KERN_ERR "_create_trace_control_dir: "
+			       "create switch_timer in %s failed\n",
+			       chan->a.filename);
+			err = -ENOMEM;
+			goto err_create_subdir;
+		}
+	}
+
+	return 0;
+
+err_create_subdir:
+	debugfs_remove_recursive(trace_root);
+err_create_trace_root:
+	return err;
+}
+
+static
+ssize_t setup_trace_write(struct file *file, const char __user *user_buf,
+			  size_t count, loff_t *ppos)
+{
+	int err = 0;
+	int buf_size;
+	struct ltt_trace *trace;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *trace_name = (char *)__get_free_page(GFP_KERNEL);
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto err_copy_from_user;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%s", trace_name) != 1) {
+		err = -EPERM;
+		goto err_get_tracename;
+	}
+
+	mutex_lock(&control_lock);
+	ltt_lock_traces();
+
+	err = _ltt_trace_setup(trace_name);
+	if (IS_ERR_VALUE(err)) {
+		printk(KERN_ERR
+		       "setup_trace_write: ltt_trace_setup failed: %d\n", err);
+		goto err_setup_trace;
+	}
+	trace = _ltt_trace_find_setup(trace_name);
+	BUG_ON(!trace);
+	err = _create_trace_control_dir(trace_name, trace);
+	if (IS_ERR_VALUE(err)) {
+		printk(KERN_ERR "setup_trace_write: "
+		       "_create_trace_control_dir failed: %d\n", err);
+		goto err_create_trace_control_dir;
+	}
+
+	ltt_unlock_traces();
+	mutex_unlock(&control_lock);
+
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trace_name);
+	return count;
+
+err_create_trace_control_dir:
+	ltt_trace_destroy(trace_name);
+err_setup_trace:
+	ltt_unlock_traces();
+	mutex_unlock(&control_lock);
+err_get_tracename:
+err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trace_name);
+	return err;
+}
+
+static const struct file_operations ltt_setup_trace_operations = {
+	.write = setup_trace_write,
+};
+
+static
+ssize_t destroy_trace_write(struct file *file, const char __user *user_buf,
+			    size_t count, loff_t *ppos)
+{
+	struct dentry *trace_den;
+	int buf_size;
+	int err = 0;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *trace_name = (char *)__get_free_page(GFP_KERNEL);
+
+	buf_size = min_t(size_t, count, PAGE_SIZE - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto err_copy_from_user;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%s", trace_name) != 1) {
+		err = -EPERM;
+		goto err_get_tracename;
+	}
+
+	mutex_lock(&control_lock);
+
+	err = ltt_trace_destroy(trace_name);
+	if (IS_ERR_VALUE(err)) {
+		printk(KERN_ERR
+		       "destroy_trace_write: ltt_trace_destroy failed: %d\n",
+		       err);
+		err = -EPERM;
+		goto err_destroy_trace;
+	}
+
+	trace_den = dir_lookup(ltt_control_dir, trace_name);
+	if (!trace_den) {
+		printk(KERN_ERR
+		       "destroy_trace_write: lookup for %s's dentry failed\n",
+		       trace_name);
+		err = -ENOENT;
+		goto err_get_dentry;
+	}
+
+	debugfs_remove_recursive(trace_den);
+
+	mutex_unlock(&control_lock);
+
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trace_name);
+	return count;
+
+err_get_dentry:
+err_destroy_trace:
+	mutex_unlock(&control_lock);
+err_get_tracename:
+err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trace_name);
+	return err;
+}
+
+static const struct file_operations ltt_destroy_trace_operations = {
+	.write = destroy_trace_write,
+};
+
+static void init_marker_dir(struct dentry *dentry,
+			    const struct inode_operations *opt)
+{
+	dentry->d_inode->i_op = opt;
+}
+
+static
+ssize_t marker_enable_read(struct file *filp, char __user *ubuf,
+			   size_t cnt, loff_t *ppos)
+{
+	char *buf;
+	const char *channel, *marker;
+	int len, enabled, present;
+
+	marker = filp->f_dentry->d_parent->d_name.name;
+	channel = filp->f_dentry->d_parent->d_parent->d_name.name;
+
+	len = 0;
+	buf = (char *)__get_free_page(GFP_KERNEL);
+
+	/*
+	 * Note: we cannot take the marker lock to make these two checks
+	 * atomic, because the marker mutex nests inside the module mutex, taken
+	 * inside the marker present check.
+	 */
+	enabled = is_marker_enabled(channel, marker);
+	present = is_marker_present(channel, marker);
+
+	if (enabled && present)
+		len = snprintf(buf, PAGE_SIZE, "%d\n", 1);
+	else if (enabled && !present)
+		len = snprintf(buf, PAGE_SIZE, "%d\n", 2);
+	else
+		len = snprintf(buf, PAGE_SIZE, "%d\n", 0);
+
+
+	if (len >= PAGE_SIZE) {
+		len = PAGE_SIZE;
+		buf[PAGE_SIZE] = '\0';
+	}
+	len = simple_read_from_buffer(ubuf, cnt, ppos, buf, len);
+	free_page((unsigned long)buf);
+
+	return len;
+}
+
+static
+ssize_t marker_enable_write(struct file *filp, const char __user *ubuf,
+			    size_t cnt, loff_t *ppos)
+{
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	int buf_size;
+	ssize_t ret = 0;
+	const char *channel, *marker;
+
+	marker = filp->f_dentry->d_parent->d_name.name;
+	channel = filp->f_dentry->d_parent->d_parent->d_name.name;
+
+	buf_size = min_t(size_t, cnt, PAGE_SIZE - 1);
+	ret = copy_from_user(buf, ubuf, buf_size);
+	if (ret)
+		goto end;
+
+	buf[buf_size] = 0;
+
+	switch (buf[0]) {
+	case 'Y':
+	case 'y':
+	case '1':
+		ret = ltt_marker_connect(channel, marker, "default");
+		if (ret)
+			goto end;
+		break;
+	case 'N':
+	case 'n':
+	case '0':
+		ret = ltt_marker_disconnect(channel, marker, "default");
+		if (ret)
+			goto end;
+		break;
+	default:
+		ret = -EPERM;
+		goto end;
+	}
+	ret = cnt;
+end:
+	free_page((unsigned long)buf);
+	return ret;
+}
+
+static const struct file_operations enable_fops = {
+	.read = marker_enable_read,
+	.write = marker_enable_write,
+};
+
+/*
+ * In practice, the output size should never be larger than 4096 kB. If it
+ * ever happens, the output will simply be truncated.
+ */
+static
+ssize_t marker_info_read(struct file *filp, char __user *ubuf,
+			 size_t cnt, loff_t *ppos)
+{
+	char *buf;
+	const char *channel, *marker;
+	int len;
+	struct marker_iter iter;
+
+	marker = filp->f_dentry->d_parent->d_name.name;
+	channel = filp->f_dentry->d_parent->d_parent->d_name.name;
+
+	len = 0;
+	buf = (char *)__get_free_page(GFP_KERNEL);
+
+	if (is_marker_enabled(channel, marker) &&
+	    !is_marker_present(channel, marker)) {
+		len += snprintf(buf + len, PAGE_SIZE - len,
+				"Marker Pre-enabled\n");
+		goto out;
+	}
+
+	marker_iter_reset(&iter);
+	marker_iter_start(&iter);
+	for (; iter.marker != NULL; marker_iter_next(&iter)) {
+		if (!strcmp(iter.marker->channel, channel) &&
+		    !strcmp(iter.marker->name, marker))
+			len += snprintf(buf + len, PAGE_SIZE - len,
+				       "Location: %s\n"
+				       "format: \"%s\"\nstate: %d\n"
+				       "event_id: %hu\n"
+				       "call: 0x%p\n"
+				       "probe %s : 0x%p\n\n",
+#ifdef CONFIG_MODULES
+				       iter.module ? iter.module->name :
+#endif
+				       "Core Kernel",
+				       iter.marker->format,
+				       _imv_read(iter.marker->state),
+				       iter.marker->event_id,
+				       iter.marker->call,
+				       iter.marker->ptype ?
+				       "multi" : "single", iter.marker->ptype ?
+				       (void *)iter.marker->multi :
+				       (void *)iter.marker->single.func);
+			if (len >= PAGE_SIZE)
+				break;
+	}
+	marker_iter_stop(&iter);
+
+out:
+	if (len >= PAGE_SIZE) {
+		len = PAGE_SIZE;
+		buf[PAGE_SIZE] = '\0';
+	}
+
+	len = simple_read_from_buffer(ubuf, cnt, ppos, buf, len);
+	free_page((unsigned long)buf);
+
+	return len;
+}
+
+static const struct file_operations info_fops = {
+	.read = marker_info_read,
+};
+
+static int marker_mkdir(struct inode *dir, struct dentry *dentry, int mode)
+{
+	struct dentry *marker_d, *enable_d, *info_d, *channel_d;
+	int ret;
+
+	ret = 0;
+	channel_d = (struct dentry *)dir->i_private;
+	mutex_unlock(&dir->i_mutex);
+
+	marker_d = debugfs_create_dir(dentry->d_name.name,
+				      channel_d);
+	if (IS_ERR(marker_d)) {
+		ret = PTR_ERR(marker_d);
+		goto out;
+	}
+
+	enable_d = debugfs_create_file("enable", 0644, marker_d,
+				       NULL, &enable_fops);
+	if (IS_ERR(enable_d) || !enable_d) {
+		printk(KERN_ERR
+		       "%s: create file of %s failed\n",
+		       __func__, "enable");
+		ret = -ENOMEM;
+		goto remove_marker_dir;
+	}
+
+	info_d = debugfs_create_file("info", 0644, marker_d,
+				     NULL, &info_fops);
+	if (IS_ERR(info_d) || !info_d) {
+		printk(KERN_ERR
+		       "%s: create file of %s failed\n",
+		       __func__, "info");
+		ret = -ENOMEM;
+		goto remove_enable_dir;
+	}
+
+	goto out;
+
+remove_enable_dir:
+	debugfs_remove(enable_d);
+remove_marker_dir:
+	debugfs_remove(marker_d);
+out:
+	mutex_lock_nested(&dir->i_mutex, I_MUTEX_PARENT);
+	return ret;
+}
+
+static int marker_rmdir(struct inode *dir, struct dentry *dentry)
+{
+	struct dentry *marker_d, *channel_d;
+	const char *channel, *name;
+	int ret, enabled, present;
+
+	ret = 0;
+
+	channel_d = (struct dentry *)dir->i_private;
+	channel = channel_d->d_name.name;
+
+	marker_d = dir_lookup(channel_d, dentry->d_name.name);
+
+	if (!marker_d) {
+		ret = -ENOENT;
+		goto out;
+	}
+
+	name = marker_d->d_name.name;
+
+	enabled = is_marker_enabled(channel, name);
+	present = is_marker_present(channel, name);
+
+	if (present || (!present && enabled)) {
+		ret = -EPERM;
+		goto out;
+	}
+
+	mutex_unlock(&dir->i_mutex);
+	mutex_unlock(&dentry->d_inode->i_mutex);
+	debugfs_remove_recursive(marker_d);
+	mutex_lock_nested(&dir->i_mutex, I_MUTEX_PARENT);
+	mutex_lock(&dentry->d_inode->i_mutex);
+out:
+	return ret;
+}
+
+const struct inode_operations channel_dir_opt = {
+	.lookup = simple_lookup,
+	.mkdir = marker_mkdir,
+	.rmdir = marker_rmdir,
+};
+
+static int channel_mkdir(struct inode *dir, struct dentry *dentry, int mode)
+{
+	struct dentry *channel_d;
+	int ret;
+
+	ret = 0;
+	mutex_unlock(&dir->i_mutex);
+
+	channel_d = debugfs_create_dir(dentry->d_name.name,
+				       markers_control_dir);
+	if (IS_ERR(channel_d)) {
+		ret = PTR_ERR(channel_d);
+		goto out;
+	}
+
+	channel_d->d_inode->i_private = (void *)channel_d;
+	init_marker_dir(channel_d, &channel_dir_opt);
+out:
+	mutex_lock_nested(&dir->i_mutex, I_MUTEX_PARENT);
+	return ret;
+}
+
+static int channel_rmdir(struct inode *dir, struct dentry *dentry)
+{
+	struct dentry *channel_d;
+	int ret;
+
+	ret = 0;
+
+	channel_d = dir_lookup(markers_control_dir, dentry->d_name.name);
+	if (!channel_d) {
+		ret = -ENOENT;
+		goto out;
+	}
+
+	if (list_empty(&channel_d->d_subdirs)) {
+		mutex_unlock(&dir->i_mutex);
+		mutex_unlock(&dentry->d_inode->i_mutex);
+		debugfs_remove(channel_d);
+		mutex_lock_nested(&dir->i_mutex, I_MUTEX_PARENT);
+		mutex_lock(&dentry->d_inode->i_mutex);
+	} else
+		ret = -EPERM;
+
+out:
+	return ret;
+}
+
+const struct inode_operations root_dir_opt = {
+	.lookup = simple_lookup,
+	.mkdir = channel_mkdir,
+	.rmdir = channel_rmdir
+};
+
+static int build_marker_file(struct marker *marker)
+{
+	struct dentry *channel_d, *marker_d, *enable_d, *info_d;
+	int err;
+
+	channel_d = dir_lookup(markers_control_dir, marker->channel);
+	if (!channel_d) {
+		channel_d = debugfs_create_dir(marker->channel,
+					       markers_control_dir);
+		if (IS_ERR(channel_d) || !channel_d) {
+			printk(KERN_ERR
+			       "%s: build channel dir of %s failed\n",
+			       __func__, marker->channel);
+			err = -ENOMEM;
+			goto err_build_fail;
+		}
+		channel_d->d_inode->i_private = (void *)channel_d;
+		init_marker_dir(channel_d, &channel_dir_opt);
+	}
+
+	marker_d  = dir_lookup(channel_d, marker->name);
+	if (!marker_d) {
+		marker_d = debugfs_create_dir(marker->name, channel_d);
+		if (IS_ERR(marker_d) || !marker_d) {
+			printk(KERN_ERR
+			       "%s: marker dir of %s failed\n",
+			       __func__, marker->name);
+			err = -ENOMEM;
+			goto err_build_fail;
+		}
+	}
+
+	enable_d = dir_lookup(marker_d, "enable");
+	if (!enable_d) {
+		enable_d = debugfs_create_file("enable", 0644, marker_d,
+						NULL, &enable_fops);
+		if (IS_ERR(enable_d) || !enable_d) {
+			printk(KERN_ERR
+			       "%s: create file of %s failed\n",
+			       __func__, "enable");
+			err = -ENOMEM;
+			goto err_build_fail;
+		}
+	}
+
+	info_d = dir_lookup(marker_d, "info");
+	if (!info_d) {
+		info_d = debugfs_create_file("info", 0444, marker_d,
+						NULL, &info_fops);
+		if (IS_ERR(info_d) || !info_d) {
+			printk(KERN_ERR
+			       "%s: create file of %s failed\n",
+			       __func__, "enable");
+			err = -ENOMEM;
+			goto err_build_fail;
+		}
+	}
+
+	return 0;
+
+err_build_fail:
+	return err;
+}
+
+static int build_marker_control_files(void)
+{
+	struct marker_iter iter;
+	int err;
+
+	err = 0;
+	if (!markers_control_dir)
+		return -EEXIST;
+
+	marker_iter_reset(&iter);
+	marker_iter_start(&iter);
+	for (; iter.marker != NULL; marker_iter_next(&iter)) {
+		err = build_marker_file(iter.marker);
+		if (err)
+			goto out;
+	}
+	marker_iter_stop(&iter);
+
+out:
+	return err;
+}
+
+#ifdef CONFIG_MODULES
+static int remove_marker_control_dir(struct module *mod, struct marker *marker)
+{
+	struct dentry *channel_d, *marker_d;
+	const char *channel, *name;
+	int count;
+	struct marker_iter iter;
+
+	count = 0;
+
+	channel_d = dir_lookup(markers_control_dir, marker->channel);
+	if (!channel_d)
+		return -ENOENT;
+	channel = channel_d->d_name.name;
+
+	marker_d = dir_lookup(channel_d, marker->name);
+	if (!marker_d)
+		return -ENOENT;
+	name = marker_d->d_name.name;
+
+	marker_iter_reset(&iter);
+	marker_iter_start(&iter);
+	for (; iter.marker != NULL; marker_iter_next(&iter)) {
+		if (!strcmp(iter.marker->channel, channel) &&
+		    !strcmp(iter.marker->name, name) && mod != iter.module)
+			count++;
+	}
+
+	if (count > 0)
+		goto end;
+
+	debugfs_remove_recursive(marker_d);
+	if (list_empty(&channel_d->d_subdirs))
+		debugfs_remove(channel_d);
+
+end:
+	marker_iter_stop(&iter);
+	return 0;
+}
+
+static void cleanup_control_dir(struct module *mod, struct marker *begin,
+				struct marker *end)
+{
+	struct marker *iter;
+
+	if (!markers_control_dir)
+		return;
+
+	for (iter = begin; iter < end; iter++)
+		remove_marker_control_dir(mod, iter);
+
+	return;
+}
+
+static void build_control_dir(struct module *mod, struct marker *begin,
+			      struct marker *end)
+{
+	struct marker *iter;
+	int err;
+
+	err = 0;
+	if (!markers_control_dir)
+		return;
+
+	for (iter = begin; iter < end; iter++) {
+		err = build_marker_file(iter);
+		if (err)
+			goto err_build_fail;
+	}
+
+	return;
+err_build_fail:
+	cleanup_control_dir(mod, begin, end);
+}
+
+static int module_notify(struct notifier_block *self,
+		  unsigned long val, void *data)
+{
+	struct module *mod = data;
+
+	switch (val) {
+	case MODULE_STATE_COMING:
+		build_control_dir(mod, mod->markers,
+				  mod->markers + mod->num_markers);
+		break;
+	case MODULE_STATE_GOING:
+		cleanup_control_dir(mod, mod->markers,
+				    mod->markers + mod->num_markers);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+#else
+static inline int module_notify(struct notifier_block *self,
+		unsigned long val, void *data)
+{
+	return 0;
+}
+#endif
+
+static struct notifier_block module_nb = {
+	.notifier_call = module_notify,
+};
+
+static int __init ltt_trace_control_init(void)
+{
+	int err = 0;
+	struct dentry *ltt_root_dentry;
+
+	ltt_root_dentry = get_ltt_root();
+	if (!ltt_root_dentry) {
+		err = -ENOENT;
+		goto err_no_root;
+	}
+
+	ltt_control_dir = debugfs_create_dir(LTT_CONTROL_DIR, ltt_root_dentry);
+	if (IS_ERR(ltt_control_dir) || !ltt_control_dir) {
+		printk(KERN_ERR
+		       "ltt_channel_control_init: create dir of %s failed\n",
+		       LTT_CONTROL_DIR);
+		err = -ENOMEM;
+		goto err_create_control_dir;
+	}
+
+	ltt_setup_trace_file = debugfs_create_file(LTT_SETUP_TRACE_FILE,
+						   S_IWUSR, ltt_root_dentry,
+						   NULL,
+						   &ltt_setup_trace_operations);
+	if (IS_ERR(ltt_setup_trace_file) || !ltt_setup_trace_file) {
+		printk(KERN_ERR
+		       "ltt_channel_control_init: create file of %s failed\n",
+		       LTT_SETUP_TRACE_FILE);
+		err = -ENOMEM;
+		goto err_create_setup_trace_file;
+	}
+
+	ltt_destroy_trace_file = debugfs_create_file(LTT_DESTROY_TRACE_FILE,
+						     S_IWUSR, ltt_root_dentry,
+						     NULL,
+						     &ltt_destroy_trace_operations);
+	if (IS_ERR(ltt_destroy_trace_file) || !ltt_destroy_trace_file) {
+		printk(KERN_ERR
+		       "ltt_channel_control_init: create file of %s failed\n",
+		       LTT_DESTROY_TRACE_FILE);
+		err = -ENOMEM;
+		goto err_create_destroy_trace_file;
+	}
+
+	markers_control_dir = debugfs_create_dir(MARKERS_CONTROL_DIR,
+						 ltt_root_dentry);
+	if (IS_ERR(markers_control_dir) || !markers_control_dir) {
+		printk(KERN_ERR
+		       "ltt_channel_control_init: create dir of %s failed\n",
+		       MARKERS_CONTROL_DIR);
+		err = -ENOMEM;
+		goto err_create_marker_control_dir;
+	}
+
+	init_marker_dir(markers_control_dir, &root_dir_opt);
+
+	if (build_marker_control_files())
+		goto err_build_fail;
+
+	if (!register_module_notifier(&module_nb))
+		return 0;
+
+err_build_fail:
+	debugfs_remove_recursive(markers_control_dir);
+	markers_control_dir = NULL;
+err_create_marker_control_dir:
+	debugfs_remove(ltt_destroy_trace_file);
+err_create_destroy_trace_file:
+	debugfs_remove(ltt_setup_trace_file);
+err_create_setup_trace_file:
+	debugfs_remove(ltt_control_dir);
+err_create_control_dir:
+err_no_root:
+	return err;
+}
+
+static void __exit ltt_trace_control_exit(void)
+{
+	struct dentry *trace_dir;
+
+	/* destory all traces */
+	list_for_each_entry(trace_dir, &ltt_control_dir->d_subdirs,
+			    d_u.d_child) {
+		ltt_trace_stop(trace_dir->d_name.name);
+		ltt_trace_destroy(trace_dir->d_name.name);
+	}
+
+	/* clean dirs in debugfs */
+	debugfs_remove(ltt_setup_trace_file);
+	debugfs_remove(ltt_destroy_trace_file);
+	debugfs_remove_recursive(ltt_control_dir);
+	debugfs_remove_recursive(markers_control_dir);
+	unregister_module_notifier(&module_nb);
+	put_ltt_root();
+}
+
+module_init(ltt_trace_control_init);
+module_exit(ltt_trace_control_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Zhao Lei <zhaolei@cn.fujitsu.com>");
+MODULE_DESCRIPTION("Linux Trace Toolkit Trace Controller");
diff --git a/stblinux-2.6.31/ltt/ltt-tracer.c b/stblinux-2.6.31/ltt/ltt-tracer.c
new file mode 100644
index 0000000..5a587fe
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-tracer.c
@@ -0,0 +1,1348 @@
+/*
+ * ltt/ltt-tracer.c
+ *
+ * (C) Copyright	2005-2008 -
+ * 		Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Tracing management internal kernel API. Trace buffer allocation/free, tracing
+ * start/stop.
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Inspired from LTT :
+ *  Karim Yaghmour (karim@opersys.com)
+ *  Tom Zanussi (zanussi@us.ibm.com)
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ * And from K42 :
+ *  Bob Wisniewski (bob@watson.ibm.com)
+ *
+ * Changelog:
+ *  22/09/06, Move to the marker/probes mechanism.
+ *  19/10/05, Complete lockless mechanism.
+ *  27/05/05, Modular redesign and rewrite.
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/time.h>
+#include <linux/ltt-tracer.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/bitops.h>
+#include <linux/fs.h>
+#include <linux/cpu.h>
+#include <linux/kref.h>
+#include <linux/delay.h>
+#include <linux/vmalloc.h>
+#include <asm/atomic.h>
+
+static void async_wakeup(unsigned long data);
+
+static DEFINE_TIMER(ltt_async_wakeup_timer, async_wakeup, 0, 0);
+
+/* Default callbacks for modules */
+notrace
+int ltt_filter_control_default(enum ltt_filter_control_msg msg,
+			       struct ltt_trace *trace)
+{
+	return 0;
+}
+
+int ltt_statedump_default(struct ltt_trace *trace)
+{
+	return 0;
+}
+
+/* Callbacks for registered modules */
+
+int (*ltt_filter_control_functor)
+	(enum ltt_filter_control_msg msg, struct ltt_trace *trace) =
+					ltt_filter_control_default;
+struct module *ltt_filter_control_owner;
+
+/* These function pointers are protected by a trace activation check */
+struct module *ltt_run_filter_owner;
+int (*ltt_statedump_functor)(struct ltt_trace *trace) = ltt_statedump_default;
+struct module *ltt_statedump_owner;
+
+struct chan_info_struct {
+	const char *name;
+	unsigned int def_sb_size;
+	unsigned int def_n_sb;
+} chan_infos[] = {
+	[LTT_CHANNEL_METADATA] = {
+		LTT_METADATA_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_FD_STATE] = {
+		LTT_FD_STATE_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_GLOBAL_STATE] = {
+		LTT_GLOBAL_STATE_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_IRQ_STATE] = {
+		LTT_IRQ_STATE_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_MODULE_STATE] = {
+		LTT_MODULE_STATE_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_NETIF_STATE] = {
+		LTT_NETIF_STATE_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_SOFTIRQ_STATE] = {
+		LTT_SOFTIRQ_STATE_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_SWAP_STATE] = {
+		LTT_SWAP_STATE_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_SYSCALL_STATE] = {
+		LTT_SYSCALL_STATE_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_TASK_STATE] = {
+		LTT_TASK_STATE_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_VM_STATE] = {
+		LTT_VM_STATE_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_MED,
+		LTT_DEFAULT_N_SUBBUFS_MED,
+	},
+	[LTT_CHANNEL_FS] = {
+		LTT_FS_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_MED,
+		LTT_DEFAULT_N_SUBBUFS_MED,
+	},
+	[LTT_CHANNEL_INPUT] = {
+		LTT_INPUT_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_IPC] = {
+		LTT_IPC_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_LOW,
+		LTT_DEFAULT_N_SUBBUFS_LOW,
+	},
+	[LTT_CHANNEL_KERNEL] = {
+		LTT_KERNEL_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_HIGH,
+		LTT_DEFAULT_N_SUBBUFS_HIGH,
+	},
+	[LTT_CHANNEL_MM] = {
+		LTT_MM_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_MED,
+		LTT_DEFAULT_N_SUBBUFS_MED,
+	},
+	[LTT_CHANNEL_RCU] = {
+		LTT_RCU_CHANNEL,
+		LTT_DEFAULT_SUBBUF_SIZE_MED,
+		LTT_DEFAULT_N_SUBBUFS_MED,
+	},
+	[LTT_CHANNEL_DEFAULT] = {
+		NULL,
+		LTT_DEFAULT_SUBBUF_SIZE_MED,
+		LTT_DEFAULT_N_SUBBUFS_MED,
+	},
+};
+
+static enum ltt_channels get_channel_type_from_name(const char *name)
+{
+	int i;
+
+	if (!name)
+		return LTT_CHANNEL_DEFAULT;
+
+	for (i = 0; i < ARRAY_SIZE(chan_infos); i++)
+		if (chan_infos[i].name && !strcmp(name, chan_infos[i].name))
+			return (enum ltt_channels)i;
+
+	return LTT_CHANNEL_DEFAULT;
+}
+
+size_t ltt_write_event_header_slow(struct ltt_chanbuf_alloc *bufa,
+				   struct ltt_chan_alloc *chana,
+				   long buf_offset, u16 eID, u32 event_size,
+				   u64 tsc, unsigned int rflags)
+{
+	struct ltt_event_header header;
+	u16 small_size;
+
+	switch (rflags) {
+	case LTT_RFLAG_ID_SIZE_TSC:
+		header.id_time = 29 << LTT_TSC_BITS;
+		break;
+	case LTT_RFLAG_ID_SIZE:
+		header.id_time = 30 << LTT_TSC_BITS;
+		break;
+	case LTT_RFLAG_ID:
+		header.id_time = 31 << LTT_TSC_BITS;
+		break;
+	}
+
+	header.id_time |= (u32)tsc & LTT_TSC_MASK;
+	ltt_relay_write(bufa, chana, buf_offset, &header, sizeof(header));
+	buf_offset += sizeof(header);
+
+	switch (rflags) {
+	case LTT_RFLAG_ID_SIZE_TSC:
+		small_size = (u16)min_t(u32, event_size, LTT_MAX_SMALL_SIZE);
+		ltt_relay_write(bufa, chana, buf_offset,
+			&eID, sizeof(u16));
+		buf_offset += sizeof(u16);
+		ltt_relay_write(bufa, chana, buf_offset,
+			&small_size, sizeof(u16));
+		buf_offset += sizeof(u16);
+		if (small_size == LTT_MAX_SMALL_SIZE) {
+			ltt_relay_write(bufa, chana, buf_offset,
+				&event_size, sizeof(u32));
+			buf_offset += sizeof(u32);
+		}
+		buf_offset += ltt_align(buf_offset, sizeof(u64));
+		ltt_relay_write(bufa, chana, buf_offset,
+			&tsc, sizeof(u64));
+		buf_offset += sizeof(u64);
+		break;
+	case LTT_RFLAG_ID_SIZE:
+		small_size = (u16)min_t(u32, event_size, LTT_MAX_SMALL_SIZE);
+		ltt_relay_write(bufa, chana, buf_offset,
+			&eID, sizeof(u16));
+		buf_offset += sizeof(u16);
+		ltt_relay_write(bufa, chana, buf_offset,
+			&small_size, sizeof(u16));
+		buf_offset += sizeof(u16);
+		if (small_size == LTT_MAX_SMALL_SIZE) {
+			ltt_relay_write(bufa, chana, buf_offset,
+				&event_size, sizeof(u32));
+			buf_offset += sizeof(u32);
+		}
+		break;
+	case LTT_RFLAG_ID:
+		ltt_relay_write(bufa, chana, buf_offset,
+			&eID, sizeof(u16));
+		buf_offset += sizeof(u16);
+		break;
+	}
+
+	return buf_offset;
+}
+EXPORT_SYMBOL_GPL(ltt_write_event_header_slow);
+
+/**
+ * ltt_module_register - LTT module registration
+ * @name: module type
+ * @function: callback to register
+ * @owner: module which owns the callback
+ *
+ * The module calling this registration function must ensure that no
+ * trap-inducing code will be executed by "function". E.g. vmalloc_sync_all()
+ * must be called between a vmalloc and the moment the memory is made visible to
+ * "function". This registration acts as a vmalloc_sync_all. Therefore, only if
+ * the module allocates virtual memory after its registration must it
+ * synchronize the TLBs.
+ */
+int ltt_module_register(enum ltt_module_function name, void *function,
+			struct module *owner)
+{
+	int ret = 0;
+
+	/*
+	 * Make sure no page fault can be triggered by the module about to be
+	 * registered. We deal with this here so we don't have to call
+	 * vmalloc_sync_all() in each module's init.
+	 */
+	vmalloc_sync_all();
+
+	switch (name) {
+	case LTT_FUNCTION_RUN_FILTER:
+		if (ltt_run_filter_owner != NULL) {
+			ret = -EEXIST;
+			goto end;
+		}
+		ltt_filter_register((ltt_run_filter_functor)function);
+		ltt_run_filter_owner = owner;
+		break;
+	case LTT_FUNCTION_FILTER_CONTROL:
+		if (ltt_filter_control_owner != NULL) {
+			ret = -EEXIST;
+			goto end;
+		}
+		ltt_filter_control_functor =
+			(int (*)(enum ltt_filter_control_msg,
+			struct ltt_trace *))function;
+		ltt_filter_control_owner = owner;
+		break;
+	case LTT_FUNCTION_STATEDUMP:
+		if (ltt_statedump_owner != NULL) {
+			ret = -EEXIST;
+			goto end;
+		}
+		ltt_statedump_functor =
+			(int (*)(struct ltt_trace *))function;
+		ltt_statedump_owner = owner;
+		break;
+	}
+
+end:
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_module_register);
+
+/**
+ * ltt_module_unregister - LTT module unregistration
+ * @name: module type
+ */
+void ltt_module_unregister(enum ltt_module_function name)
+{
+	switch (name) {
+	case LTT_FUNCTION_RUN_FILTER:
+		ltt_filter_unregister();
+		ltt_run_filter_owner = NULL;
+		/* Wait for preempt sections to finish */
+		synchronize_sched();
+		break;
+	case LTT_FUNCTION_FILTER_CONTROL:
+		ltt_filter_control_functor = ltt_filter_control_default;
+		ltt_filter_control_owner = NULL;
+		break;
+	case LTT_FUNCTION_STATEDUMP:
+		ltt_statedump_functor = ltt_statedump_default;
+		ltt_statedump_owner = NULL;
+		break;
+	}
+
+}
+EXPORT_SYMBOL_GPL(ltt_module_unregister);
+
+static LIST_HEAD(ltt_transport_list);
+
+/**
+ * ltt_transport_register - LTT transport registration
+ * @transport: transport structure
+ *
+ * Registers a transport which can be used as output to extract the data out of
+ * LTTng. The module calling this registration function must ensure that no
+ * trap-inducing code will be executed by the transport functions. E.g.
+ * vmalloc_sync_all() must be called between a vmalloc and the moment the memory
+ * is made visible to the transport function. This registration acts as a
+ * vmalloc_sync_all. Therefore, only if the module allocates virtual memory
+ * after its registration must it synchronize the TLBs.
+ */
+void ltt_transport_register(struct ltt_transport *transport)
+{
+	/*
+	 * Make sure no page fault can be triggered by the module about to be
+	 * registered. We deal with this here so we don't have to call
+	 * vmalloc_sync_all() in each module's init.
+	 */
+	vmalloc_sync_all();
+
+	ltt_lock_traces();
+	list_add_tail(&transport->node, &ltt_transport_list);
+	ltt_unlock_traces();
+}
+EXPORT_SYMBOL_GPL(ltt_transport_register);
+
+/**
+ * ltt_transport_unregister - LTT transport unregistration
+ * @transport: transport structure
+ */
+void ltt_transport_unregister(struct ltt_transport *transport)
+{
+	ltt_lock_traces();
+	list_del(&transport->node);
+	ltt_unlock_traces();
+}
+EXPORT_SYMBOL_GPL(ltt_transport_unregister);
+
+static inline
+int is_channel_overwrite(enum ltt_channels chan, enum trace_mode mode)
+{
+	switch (mode) {
+	case LTT_TRACE_NORMAL:
+		return 0;
+	case LTT_TRACE_FLIGHT:
+		switch (chan) {
+		case LTT_CHANNEL_METADATA:
+			return 0;
+		default:
+			return 1;
+		}
+	case LTT_TRACE_HYBRID:
+		switch (chan) {
+		case LTT_CHANNEL_KERNEL:
+		case LTT_CHANNEL_FS:
+		case LTT_CHANNEL_MM:
+		case LTT_CHANNEL_RCU:
+		case LTT_CHANNEL_IPC:
+		case LTT_CHANNEL_INPUT:
+			return 1;
+		default:
+			return 0;
+		}
+	default:
+		return 0;
+	}
+}
+
+static void trace_async_wakeup(struct ltt_trace *trace)
+{
+	int i;
+	struct ltt_chan *chan;
+
+	/* Must check each channel for pending read wakeup */
+	for (i = 0; i < trace->nr_channels; i++) {
+		chan = &trace->channels[i];
+		if (chan->active)
+			trace->ops->wakeup_channel(chan);
+	}
+}
+
+/* Timer to send async wakeups to the readers */
+static void async_wakeup(unsigned long data)
+{
+	struct ltt_trace *trace;
+
+	/*
+	 * PREEMPT_RT does not allow spinlocks to be taken within preempt
+	 * disable sections (spinlock taken in wake_up). However, mainline won't
+	 * allow mutex to be taken in interrupt context. Ugly.
+	 * A proper way to do this would be to turn the timer into a
+	 * periodically woken up thread, but it adds to the footprint.
+	 */
+#ifndef CONFIG_PREEMPT_RT
+	rcu_read_lock_sched();
+#else
+	ltt_lock_traces();
+#endif
+	list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		trace_async_wakeup(trace);
+	}
+#ifndef CONFIG_PREEMPT_RT
+	rcu_read_unlock_sched();
+#else
+	ltt_unlock_traces();
+#endif
+
+	mod_timer(&ltt_async_wakeup_timer, jiffies + LTT_PERCPU_TIMER_INTERVAL);
+}
+
+/**
+ * _ltt_trace_find - find a trace by given name.
+ * trace_name: trace name
+ *
+ * Returns a pointer to the trace structure, NULL if not found.
+ */
+static struct ltt_trace *_ltt_trace_find(const char *trace_name)
+{
+	struct ltt_trace *trace;
+
+	list_for_each_entry(trace, &ltt_traces.head, list)
+		if (!strncmp(trace->trace_name, trace_name, NAME_MAX))
+			return trace;
+
+	return NULL;
+}
+
+/* _ltt_trace_find_setup :
+ * find a trace in setup list by given name.
+ *
+ * Returns a pointer to the trace structure, NULL if not found.
+ */
+struct ltt_trace *_ltt_trace_find_setup(const char *trace_name)
+{
+	struct ltt_trace *trace;
+
+	list_for_each_entry(trace, &ltt_traces.setup_head, list)
+		if (!strncmp(trace->trace_name, trace_name, NAME_MAX))
+			return trace;
+
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(_ltt_trace_find_setup);
+
+/**
+ * ltt_release_trace - Release a LTT trace
+ * @kref : reference count on the trace
+ */
+void ltt_release_trace(struct kref *kref)
+{
+	struct ltt_trace *trace = container_of(kref, struct ltt_trace, kref);
+
+	trace->ops->remove_dirs(trace);
+	module_put(trace->transport->owner);
+	ltt_channels_trace_free(trace->channels, trace->nr_channels);
+	kfree(trace);
+}
+EXPORT_SYMBOL_GPL(ltt_release_trace);
+
+static inline void prepare_chan_size_num(unsigned int *subbuf_size,
+					 unsigned int *n_subbufs)
+{
+	/* Make sure the subbuffer size is larger than a page */
+	*subbuf_size = max_t(unsigned int, *subbuf_size, PAGE_SIZE);
+
+	/* round to next power of 2 */
+	*subbuf_size = 1 << get_count_order(*subbuf_size);
+	*n_subbufs = 1 << get_count_order(*n_subbufs);
+
+	/* Subbuf size and number must both be power of two */
+	WARN_ON(hweight32(*subbuf_size) != 1);
+	WARN_ON(hweight32(*n_subbufs) != 1);
+}
+
+int _ltt_trace_setup(const char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace *new_trace = NULL;
+	int metadata_index;
+	unsigned int chan;
+	enum ltt_channels chantype;
+
+	if (_ltt_trace_find_setup(trace_name)) {
+		printk(KERN_ERR	"LTT : Trace name %s already used.\n",
+				trace_name);
+		err = -EEXIST;
+		goto traces_error;
+	}
+
+	if (_ltt_trace_find(trace_name)) {
+		printk(KERN_ERR	"LTT : Trace name %s already used.\n",
+				trace_name);
+		err = -EEXIST;
+		goto traces_error;
+	}
+
+	new_trace = kzalloc(sizeof(struct ltt_trace), GFP_KERNEL);
+	if (!new_trace) {
+		printk(KERN_ERR
+			"LTT : Unable to allocate memory for trace %s\n",
+			trace_name);
+		err = -ENOMEM;
+		goto traces_error;
+	}
+	strncpy(new_trace->trace_name, trace_name, NAME_MAX);
+	new_trace->channels = ltt_channels_trace_alloc(&new_trace->nr_channels,
+						       0, 1);
+	if (!new_trace->channels) {
+		printk(KERN_ERR
+			"LTT : Unable to allocate memory for chaninfo  %s\n",
+			trace_name);
+		err = -ENOMEM;
+		goto trace_free;
+	}
+
+	/*
+	 * Force metadata channel to active, no overwrite.
+	 */
+	metadata_index = ltt_channels_get_index_from_name("metadata");
+	WARN_ON(metadata_index < 0);
+	new_trace->channels[metadata_index].overwrite = 0;
+	new_trace->channels[metadata_index].active = 1;
+
+	/*
+	 * Set hardcoded tracer defaults for some channels
+	 */
+	for (chan = 0; chan < new_trace->nr_channels; chan++) {
+		if (!(new_trace->channels[chan].active))
+			continue;
+
+		chantype = get_channel_type_from_name(
+			ltt_channels_get_name_from_index(chan));
+		new_trace->channels[chan].a.sb_size =
+			chan_infos[chantype].def_sb_size;
+		new_trace->channels[chan].a.n_sb =
+			chan_infos[chantype].def_n_sb;
+	}
+
+	list_add(&new_trace->list, &ltt_traces.setup_head);
+	return 0;
+
+trace_free:
+	kfree(new_trace);
+traces_error:
+	return err;
+}
+EXPORT_SYMBOL_GPL(_ltt_trace_setup);
+
+
+int ltt_trace_setup(const char *trace_name)
+{
+	int ret;
+	ltt_lock_traces();
+	ret = _ltt_trace_setup(trace_name);
+	ltt_unlock_traces();
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_setup);
+
+/* must be called from within a traces lock. */
+static void _ltt_trace_free(struct ltt_trace *trace)
+{
+	list_del(&trace->list);
+	kfree(trace);
+}
+
+int ltt_trace_set_type(const char *trace_name, const char *trace_type)
+{
+	int err = 0;
+	struct ltt_trace *trace;
+	struct ltt_transport *tran_iter, *transport = NULL;
+
+	ltt_lock_traces();
+
+	trace = _ltt_trace_find_setup(trace_name);
+	if (!trace) {
+		printk(KERN_ERR "LTT : Trace not found %s\n", trace_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+
+	list_for_each_entry(tran_iter, &ltt_transport_list, node) {
+		if (!strcmp(tran_iter->name, trace_type)) {
+			transport = tran_iter;
+			break;
+		}
+	}
+	if (!transport) {
+		printk(KERN_ERR	"LTT : Transport %s is not present.\n",
+			trace_type);
+		err = -EINVAL;
+		goto traces_error;
+	}
+
+	trace->transport = transport;
+
+traces_error:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_set_type);
+
+int ltt_trace_set_channel_subbufsize(const char *trace_name,
+				     const char *channel_name,
+				     unsigned int size)
+{
+	int err = 0;
+	struct ltt_trace *trace;
+	int index;
+
+	ltt_lock_traces();
+
+	trace = _ltt_trace_find_setup(trace_name);
+	if (!trace) {
+		printk(KERN_ERR "LTT : Trace not found %s\n", trace_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+
+	index = ltt_channels_get_index_from_name(channel_name);
+	if (index < 0) {
+		printk(KERN_ERR "LTT : Channel %s not found\n", channel_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+	trace->channels[index].a.sb_size = size;
+
+traces_error:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_set_channel_subbufsize);
+
+int ltt_trace_set_channel_subbufcount(const char *trace_name,
+				      const char *channel_name,
+				      unsigned int cnt)
+{
+	int err = 0;
+	struct ltt_trace *trace;
+	int index;
+
+	ltt_lock_traces();
+
+	trace = _ltt_trace_find_setup(trace_name);
+	if (!trace) {
+		printk(KERN_ERR "LTT : Trace not found %s\n", trace_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+
+	index = ltt_channels_get_index_from_name(channel_name);
+	if (index < 0) {
+		printk(KERN_ERR "LTT : Channel %s not found\n", channel_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+	trace->channels[index].a.n_sb = cnt;
+
+traces_error:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_set_channel_subbufcount);
+
+int ltt_trace_set_channel_switch_timer(const char *trace_name,
+				       const char *channel_name,
+				       unsigned long interval)
+{
+	int err = 0;
+	struct ltt_trace *trace;
+	int index;
+
+	ltt_lock_traces();
+
+	trace = _ltt_trace_find_setup(trace_name);
+	if (!trace) {
+		printk(KERN_ERR "LTT : Trace not found %s\n", trace_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+
+	index = ltt_channels_get_index_from_name(channel_name);
+	if (index < 0) {
+		printk(KERN_ERR "LTT : Channel %s not found\n", channel_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+	ltt_channels_trace_set_timer(&trace->channels[index], interval);
+
+traces_error:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_set_channel_switch_timer);
+
+int ltt_trace_set_channel_enable(const char *trace_name,
+				 const char *channel_name, unsigned int enable)
+{
+	int err = 0;
+	struct ltt_trace *trace;
+	int index;
+
+	ltt_lock_traces();
+
+	trace = _ltt_trace_find_setup(trace_name);
+	if (!trace) {
+		printk(KERN_ERR "LTT : Trace not found %s\n", trace_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+
+	/*
+	 * Datas in metadata channel(marker info) is necessary to be able to
+	 * read the trace, we always enable this channel.
+	 */
+	if (!enable && !strcmp(channel_name, "metadata")) {
+		printk(KERN_ERR "LTT : Trying to disable metadata channel\n");
+		err = -EINVAL;
+		goto traces_error;
+	}
+
+	index = ltt_channels_get_index_from_name(channel_name);
+	if (index < 0) {
+		printk(KERN_ERR "LTT : Channel %s not found\n", channel_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+
+	trace->channels[index].active = enable;
+
+traces_error:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_set_channel_enable);
+
+int ltt_trace_set_channel_overwrite(const char *trace_name,
+				    const char *channel_name,
+				    unsigned int overwrite)
+{
+	int err = 0;
+	struct ltt_trace *trace;
+	int index;
+
+	ltt_lock_traces();
+
+	trace = _ltt_trace_find_setup(trace_name);
+	if (!trace) {
+		printk(KERN_ERR "LTT : Trace not found %s\n", trace_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+
+	/*
+	 * Always put the metadata channel in non-overwrite mode :
+	 * This is a very low traffic channel and it can't afford to have its
+	 * data overwritten : this data (marker info) is necessary to be
+	 * able to read the trace.
+	 */
+	if (overwrite && !strcmp(channel_name, "metadata")) {
+		printk(KERN_ERR "LTT : Trying to set metadata channel to "
+				"overwrite mode\n");
+		err = -EINVAL;
+		goto traces_error;
+	}
+
+	index = ltt_channels_get_index_from_name(channel_name);
+	if (index < 0) {
+		printk(KERN_ERR "LTT : Channel %s not found\n", channel_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+
+	trace->channels[index].overwrite = overwrite;
+
+traces_error:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_set_channel_overwrite);
+
+int ltt_trace_alloc(const char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace *trace;
+	int sb_size, n_sb;
+	unsigned long flags;
+	int chan;
+	const char *channel_name;
+
+	ltt_lock_traces();
+
+	trace = _ltt_trace_find_setup(trace_name);
+	if (!trace) {
+		printk(KERN_ERR "LTT : Trace not found %s\n", trace_name);
+		err = -ENOENT;
+		goto traces_error;
+	}
+
+	kref_init(&trace->kref);
+	init_waitqueue_head(&trace->kref_wq);
+	trace->active = 0;
+	get_trace_clock();
+	trace->freq_scale = trace_clock_freq_scale();
+
+	if (!trace->transport) {
+		printk(KERN_ERR "LTT : Transport is not set.\n");
+		err = -EINVAL;
+		goto transport_error;
+	}
+	if (!try_module_get(trace->transport->owner)) {
+		printk(KERN_ERR	"LTT : Can't lock transport module.\n");
+		err = -ENODEV;
+		goto transport_error;
+	}
+	trace->ops = &trace->transport->ops;
+
+	err = trace->ops->create_dirs(trace);
+	if (err) {
+		printk(KERN_ERR	"LTT : Can't create dir for trace %s.\n",
+			trace_name);
+		goto dirs_error;
+	}
+
+	local_irq_save(flags);
+	trace->start_freq = trace_clock_frequency();
+	trace->start_tsc = trace_clock_read64();
+	do_gettimeofday(&trace->start_time);
+	local_irq_restore(flags);
+
+	for (chan = 0; chan < trace->nr_channels; chan++) {
+		if (!(trace->channels[chan].active))
+			continue;
+
+		channel_name = ltt_channels_get_name_from_index(chan);
+		WARN_ON(!channel_name);
+		/*
+		 * note: sb_size and n_sb will be overwritten with updated
+		 * values by channel creation.
+		 */
+		sb_size = trace->channels[chan].a.sb_size;
+		n_sb = trace->channels[chan].a.n_sb;
+		prepare_chan_size_num(&sb_size, &n_sb);
+		err = trace->ops->create_channel(channel_name,
+				      &trace->channels[chan],
+				      trace->dentry.trace_root,
+				      sb_size, n_sb,
+				      trace->channels[chan].overwrite, trace);
+		if (err != 0) {
+			printk(KERN_ERR	"LTT : Can't create channel %s.\n",
+				channel_name);
+			goto create_channel_error;
+		}
+	}
+
+	list_del(&trace->list);
+	if (list_empty(&ltt_traces.head)) {
+		mod_timer(&ltt_async_wakeup_timer,
+				jiffies + LTT_PERCPU_TIMER_INTERVAL);
+		set_kernel_trace_flag_all_tasks();
+	}
+	list_add_rcu(&trace->list, &ltt_traces.head);
+	synchronize_sched();
+
+	ltt_unlock_traces();
+
+	return 0;
+
+create_channel_error:
+	for (chan--; chan >= 0; chan--) {
+		if (trace->channels[chan].active)
+			kref_put(&trace->channels[chan].a.kref,
+				 trace->ops->remove_channel);
+	}
+	trace->ops->remove_dirs(trace);
+
+dirs_error:
+	module_put(trace->transport->owner);
+transport_error:
+	put_trace_clock();
+traces_error:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_alloc);
+
+/*
+ * It is worked as a wrapper for current version of ltt_control.ko.
+ * We will make a new ltt_control based on debugfs, and control each channel's
+ * buffer.
+ */
+static
+int ltt_trace_create(const char *trace_name, const char *trace_type,
+		     enum trace_mode mode,
+		     unsigned int subbuf_size_low, unsigned int n_subbufs_low,
+		     unsigned int subbuf_size_med, unsigned int n_subbufs_med,
+		     unsigned int subbuf_size_high, unsigned int n_subbufs_high)
+{
+	int err = 0;
+
+	err = ltt_trace_setup(trace_name);
+	if (IS_ERR_VALUE(err))
+		return err;
+
+	err = ltt_trace_set_type(trace_name, trace_type);
+	if (IS_ERR_VALUE(err))
+		return err;
+
+	err = ltt_trace_alloc(trace_name);
+	if (IS_ERR_VALUE(err))
+		return err;
+
+	return err;
+}
+
+/* Must be called while sure that trace is in the list. */
+static int _ltt_trace_destroy(struct ltt_trace *trace)
+{
+	int err = -EPERM;
+
+	if (trace == NULL) {
+		err = -ENOENT;
+		goto traces_error;
+	}
+	if (trace->active) {
+		printk(KERN_ERR
+			"LTT : Can't destroy trace %s : tracer is active\n",
+			trace->trace_name);
+		err = -EBUSY;
+		goto active_error;
+	}
+	/* Everything went fine */
+	list_del_rcu(&trace->list);
+	synchronize_sched();
+	if (list_empty(&ltt_traces.head)) {
+		clear_kernel_trace_flag_all_tasks();
+		/*
+		 * We stop the asynchronous delivery of reader wakeup, but
+		 * we must make one last check for reader wakeups pending
+		 * later in __ltt_trace_destroy.
+		 */
+		del_timer_sync(&ltt_async_wakeup_timer);
+	}
+	return 0;
+
+	/* error handling */
+active_error:
+traces_error:
+	return err;
+}
+
+/* Sleepable part of the destroy */
+static void __ltt_trace_destroy(struct ltt_trace *trace)
+{
+	int i;
+	struct ltt_chan *chan;
+
+	for (i = 0; i < trace->nr_channels; i++) {
+		chan = &trace->channels[i];
+		if (chan->active)
+			trace->ops->finish_channel(chan);
+	}
+
+	flush_scheduled_work();
+
+	/*
+	 * The currently destroyed trace is not in the trace list anymore,
+	 * so it's safe to call the async wakeup ourself. It will deliver
+	 * the last subbuffers.
+	 */
+	trace_async_wakeup(trace);
+
+	for (i = 0; i < trace->nr_channels; i++) {
+		chan = &trace->channels[i];
+		if (chan->active)
+			kref_put(&chan->a.kref,
+				 trace->ops->remove_channel);
+	}
+
+	/*
+	 * Wait for lttd readers to release the files, therefore making sure
+	 * the last subbuffers have been read.
+	 */
+	if (atomic_read(&trace->kref.refcount) > 1) {
+		int ret = 0;
+		/*
+		 * Unlock traces and CPU hotplug while we wait for lttd to
+		 * release the files.
+		 */
+		ltt_unlock_traces();
+		__wait_event_interruptible(trace->kref_wq,
+			(atomic_read(&trace->kref.refcount) == 1), ret);
+		ltt_lock_traces();
+	}
+
+	kref_put(&trace->kref, ltt_release_trace);
+}
+
+int ltt_trace_destroy(const char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace *trace;
+
+	ltt_lock_traces();
+
+	trace = _ltt_trace_find(trace_name);
+	if (trace) {
+		err = _ltt_trace_destroy(trace);
+		if (err)
+			goto error;
+
+		__ltt_trace_destroy(trace);
+		ltt_unlock_traces();
+		put_trace_clock();
+
+		return 0;
+	}
+
+	trace = _ltt_trace_find_setup(trace_name);
+	if (trace) {
+		_ltt_trace_free(trace);
+		ltt_unlock_traces();
+		return 0;
+	}
+
+	err = -ENOENT;
+
+	/* Error handling */
+error:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_destroy);
+
+/*
+ * called with trace lock held.
+ */
+static
+void ltt_channels_trace_start_timer(struct ltt_chan *channels,
+				    unsigned int nr_channels)
+{
+	int i;
+
+	for (i = 0; i < nr_channels; i++) {
+		struct ltt_chan *chan = &channels[i];
+		chan->a.trace->ops->start_switch_timer(chan);
+	}
+}
+
+/*
+ * called with trace lock held.
+ */
+static
+void ltt_channels_trace_stop_timer(struct ltt_chan *channels,
+				   unsigned int nr_channels)
+{
+	int i;
+
+	for (i = 0; i < nr_channels; i++) {
+		struct ltt_chan *chan = &channels[i];
+		chan->a.trace->ops->stop_switch_timer(chan);
+	}
+}
+
+/* must be called from within a traces lock. */
+static int _ltt_trace_start(struct ltt_trace *trace)
+{
+	int err = 0;
+
+	if (trace == NULL) {
+		err = -ENOENT;
+		goto traces_error;
+	}
+	if (trace->active)
+		printk(KERN_INFO "LTT : Tracing already active for trace %s\n",
+				trace->trace_name);
+	if (!try_module_get(ltt_run_filter_owner)) {
+		err = -ENODEV;
+		printk(KERN_ERR "LTT : Can't lock filter module.\n");
+		goto get_ltt_run_filter_error;
+	}
+	ltt_channels_trace_start_timer(trace->channels, trace->nr_channels);
+	trace->active = 1;
+	/* Read by trace points without protection : be careful */
+	ltt_traces.num_active_traces++;
+	return err;
+
+	/* error handling */
+get_ltt_run_filter_error:
+traces_error:
+	return err;
+}
+
+int ltt_trace_start(const char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace *trace;
+
+	ltt_lock_traces();
+
+	trace = _ltt_trace_find(trace_name);
+	err = _ltt_trace_start(trace);
+	if (err)
+		goto no_trace;
+
+	ltt_unlock_traces();
+
+	/*
+	 * Call the kernel state dump.
+	 * Events will be mixed with real kernel events, it's ok.
+	 * Notice that there is no protection on the trace : that's exactly
+	 * why we iterate on the list and check for trace equality instead of
+	 * directly using this trace handle inside the logging function.
+	 */
+
+	ltt_dump_marker_state(trace);
+
+	if (!try_module_get(ltt_statedump_owner)) {
+		err = -ENODEV;
+		printk(KERN_ERR
+			"LTT : Can't lock state dump module.\n");
+	} else {
+		ltt_statedump_functor(trace);
+		module_put(ltt_statedump_owner);
+	}
+
+	return err;
+
+	/* Error handling */
+no_trace:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_start);
+
+/* must be called from within traces lock */
+static int _ltt_trace_stop(struct ltt_trace *trace)
+{
+	int err = -EPERM;
+
+	if (trace == NULL) {
+		err = -ENOENT;
+		goto traces_error;
+	}
+	if (!trace->active)
+		printk(KERN_INFO "LTT : Tracing not active for trace %s\n",
+				trace->trace_name);
+	if (trace->active) {
+		ltt_channels_trace_stop_timer(trace->channels,
+			trace->nr_channels);
+		trace->active = 0;
+		ltt_traces.num_active_traces--;
+		synchronize_sched(); /* Wait for each tracing to be finished */
+	}
+	module_put(ltt_run_filter_owner);
+	/* Everything went fine */
+	return 0;
+
+	/* Error handling */
+traces_error:
+	return err;
+}
+
+int ltt_trace_stop(const char *trace_name)
+{
+	int err = 0;
+	struct ltt_trace *trace;
+
+	ltt_lock_traces();
+	trace = _ltt_trace_find(trace_name);
+	err = _ltt_trace_stop(trace);
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_trace_stop);
+
+/**
+ * ltt_control - Trace control in-kernel API
+ * @msg: Action to perform
+ * @trace_name: Trace on which the action must be done
+ * @trace_type: Type of trace (normal, flight, hybrid)
+ * @args: Arguments specific to the action
+ */
+int ltt_control(enum ltt_control_msg msg, const char *trace_name,
+		const char *trace_type, union ltt_control_args args)
+{
+	int err = -EPERM;
+
+	printk(KERN_ALERT "ltt_control : trace %s\n", trace_name);
+	switch (msg) {
+	case LTT_CONTROL_START:
+		printk(KERN_DEBUG "Start tracing %s\n", trace_name);
+		err = ltt_trace_start(trace_name);
+		break;
+	case LTT_CONTROL_STOP:
+		printk(KERN_DEBUG "Stop tracing %s\n", trace_name);
+		err = ltt_trace_stop(trace_name);
+		break;
+	case LTT_CONTROL_CREATE_TRACE:
+		printk(KERN_DEBUG "Creating trace %s\n", trace_name);
+		err = ltt_trace_create(trace_name, trace_type,
+			args.new_trace.mode,
+			args.new_trace.subbuf_size_low,
+			args.new_trace.n_subbufs_low,
+			args.new_trace.subbuf_size_med,
+			args.new_trace.n_subbufs_med,
+			args.new_trace.subbuf_size_high,
+			args.new_trace.n_subbufs_high);
+		break;
+	case LTT_CONTROL_DESTROY_TRACE:
+		printk(KERN_DEBUG "Destroying trace %s\n", trace_name);
+		err = ltt_trace_destroy(trace_name);
+		break;
+	}
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_control);
+
+/**
+ * ltt_filter_control - Trace filter control in-kernel API
+ * @msg: Action to perform on the filter
+ * @trace_name: Trace on which the action must be done
+ */
+int ltt_filter_control(enum ltt_filter_control_msg msg, const char *trace_name)
+{
+	int err;
+	struct ltt_trace *trace;
+
+	printk(KERN_DEBUG "ltt_filter_control : trace %s\n", trace_name);
+	ltt_lock_traces();
+	trace = _ltt_trace_find(trace_name);
+	if (trace == NULL) {
+		printk(KERN_ALERT
+			"Trace does not exist. Cannot proxy control request\n");
+		err = -ENOENT;
+		goto trace_error;
+	}
+	if (!try_module_get(ltt_filter_control_owner)) {
+		err = -ENODEV;
+		goto get_module_error;
+	}
+	switch (msg) {
+	case LTT_FILTER_DEFAULT_ACCEPT:
+		printk(KERN_DEBUG
+			"Proxy filter default accept %s\n", trace_name);
+		err = (*ltt_filter_control_functor)(msg, trace);
+		break;
+	case LTT_FILTER_DEFAULT_REJECT:
+		printk(KERN_DEBUG
+			"Proxy filter default reject %s\n", trace_name);
+		err = (*ltt_filter_control_functor)(msg, trace);
+		break;
+	default:
+		err = -EPERM;
+	}
+	module_put(ltt_filter_control_owner);
+
+get_module_error:
+trace_error:
+	ltt_unlock_traces();
+	return err;
+}
+EXPORT_SYMBOL_GPL(ltt_filter_control);
+
+int __init ltt_init(void)
+{
+	/* Make sure no page fault can be triggered by this module */
+	vmalloc_sync_all();
+	init_timer_deferrable(&ltt_async_wakeup_timer);
+	return 0;
+}
+
+module_init(ltt_init)
+
+static void __exit ltt_exit(void)
+{
+	struct ltt_trace *trace;
+	struct list_head *pos, *n;
+
+	ltt_lock_traces();
+	/* Stop each trace, currently being read by RCU read-side */
+	list_for_each_entry_rcu(trace, &ltt_traces.head, list)
+		_ltt_trace_stop(trace);
+	/* Wait for quiescent state. Readers have preemption disabled. */
+	synchronize_sched();
+	/* Safe iteration is now permitted. It does not have to be RCU-safe
+	 * because no readers are left. */
+	list_for_each_safe(pos, n, &ltt_traces.head) {
+		trace = container_of(pos, struct ltt_trace, list);
+		/* _ltt_trace_destroy does a synchronize_sched() */
+		_ltt_trace_destroy(trace);
+		__ltt_trace_destroy(trace);
+	}
+	/* free traces in pre-alloc status */
+	list_for_each_safe(pos, n, &ltt_traces.setup_head) {
+		trace = container_of(pos, struct ltt_trace, list);
+		_ltt_trace_free(trace);
+	}
+
+	ltt_unlock_traces();
+}
+
+module_exit(ltt_exit)
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Linux Trace Toolkit Next Generation Tracer Kernel API");
diff --git a/stblinux-2.6.31/ltt/ltt-type-serializer.c b/stblinux-2.6.31/ltt/ltt-type-serializer.c
new file mode 100644
index 0000000..c4864ea
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-type-serializer.c
@@ -0,0 +1,98 @@
+/**
+ * ltt-type-serializer.c
+ *
+ * LTTng specialized type serializer.
+ *
+ * Copyright Mathieu Desnoyers, 2008.
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+#include <linux/module.h>
+#include <linux/ltt-type-serializer.h>
+
+#include "ltt-relay-select.h"
+
+notrace
+void _ltt_specialized_trace(const struct marker *mdata, void *probe_data,
+		void *serialize_private, unsigned int data_size,
+		unsigned int largest_align)
+{
+	int ret;
+	uint16_t eID;
+	size_t slot_size;
+	unsigned int chan_index;
+	struct ltt_chanbuf *buf;
+	struct ltt_chan *chan;
+	struct ltt_trace *trace;
+	uint64_t tsc;
+	long buf_offset;
+	int cpu;
+	unsigned int rflags;
+
+	/*
+	 * If we get here, it's probably because we have useful work to do.
+	 */
+	if (unlikely(ltt_traces.num_active_traces == 0))
+		return;
+
+	rcu_read_lock_sched_notrace();
+	cpu = smp_processor_id();
+	__get_cpu_var(ltt_nesting)++;
+
+	eID = mdata->event_id;
+	chan_index = mdata->channel_id;
+
+	/*
+	 * Iterate on each trace, typically small number of active traces,
+	 * list iteration with prefetch is usually slower.
+	 */
+	__list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		if (unlikely(!trace->active))
+			continue;
+		if (unlikely(!ltt_run_filter(trace, eID)))
+			continue;
+#ifdef CONFIG_LTT_DEBUG_EVENT_SIZE
+		rflags = LTT_RFLAG_ID_SIZE;
+#else
+		if (unlikely(eID >= LTT_FREE_EVENTS))
+			rflags = LTT_RFLAG_ID;
+		else
+			rflags = 0;
+#endif
+		/*
+		 * Skip channels added after trace creation.
+		 */
+		if (unlikely(chan_index >= trace->nr_channels))
+			continue;
+		chan = &trace->channels[chan_index];
+		if (!chan->active)
+			continue;
+
+		/* reserve space : header and data */
+		ret = ltt_reserve_slot(chan, trace, data_size, largest_align,
+				       cpu, &buf, &slot_size, &buf_offset, &tsc,
+				       &rflags);
+		if (unlikely(ret < 0))
+			continue; /* buffer full */
+
+		/* Out-of-order write : header and data */
+		buf_offset = ltt_write_event_header(&buf->a, &chan->a,
+						    buf_offset, eID, data_size,
+						    tsc, rflags);
+		if (data_size) {
+			buf_offset += ltt_align(buf_offset, largest_align);
+			ltt_relay_write(&buf->a, &chan->a, buf_offset,
+					serialize_private, data_size);
+			buf_offset += data_size;
+		}
+		/* Out-of-order commit */
+		ltt_commit_slot(buf, chan, buf_offset, data_size, slot_size);
+	}
+	__get_cpu_var(ltt_nesting)--;
+	rcu_read_unlock_sched_notrace();
+}
+EXPORT_SYMBOL_GPL(_ltt_specialized_trace);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("LTT type serializer");
diff --git a/stblinux-2.6.31/ltt/ltt-userspace-event.c b/stblinux-2.6.31/ltt/ltt-userspace-event.c
new file mode 100644
index 0000000..46edbed
--- /dev/null
+++ b/stblinux-2.6.31/ltt/ltt-userspace-event.c
@@ -0,0 +1,120 @@
+/*
+ * Copyright (C) 2008 Mathieu Desnoyers
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/marker.h>
+#include <linux/uaccess.h>
+#include <linux/gfp.h>
+#include <linux/fs.h>
+#include <linux/debugfs.h>
+#include <linux/ltt-type-serializer.h>
+
+#define LTT_WRITE_EVENT_FILE	"write_event"
+
+DEFINE_MARKER(userspace, event, "string %s");
+static struct dentry *ltt_event_file;
+
+/**
+ * write_event - write a userspace string into the trace system
+ * @file: file pointer
+ * @user_buf: user string
+ * @count: length to copy, including the final NULL
+ * @ppos: unused
+ *
+ * Copy a string into a trace event, in channel "userspace", event "event".
+ * Copies until either \n or \0 is reached.
+ * On success, returns the number of bytes copied from the source, including the
+ * \n or \0 character (if there was one in the count range). It cannot return
+ * more than count.
+ * Inspired from tracing_mark_write implementation from Steven Rostedt and
+ * Ingo Molnar.
+ */
+static
+ssize_t write_event(struct file *file, const char __user *user_buf,
+		    size_t count, loff_t *ppos)
+{
+	struct marker *marker;
+	char *buf, *end;
+	long copycount;
+	ssize_t ret;
+
+	buf = kmalloc(count + 1, GFP_KERNEL);
+	if (!buf) {
+		ret = -ENOMEM;
+		goto string_out;
+	}
+	copycount = strncpy_from_user(buf, user_buf, count);
+	if (copycount < 0) {
+		ret = -EFAULT;
+		goto string_err;
+	}
+	/* Cut from the first nil or newline. */
+	buf[copycount] = '\0';
+	end = strchr(buf, '\n');
+	if (end) {
+		*end = '\0';
+		copycount = end - buf;
+	}
+	/* Add final \0 to copycount */
+	copycount++;
+	marker = &GET_MARKER(userspace, event);
+	ltt_specialized_trace(marker, marker->single.probe_private, buf,
+			      copycount, sizeof(char));
+	/* If there is no \0 nor \n in count, do not return a larger value */
+	ret = min_t(size_t, copycount, count);
+string_err:
+	kfree(buf);
+string_out:
+	return ret;
+}
+
+static const struct file_operations ltt_userspace_operations = {
+	.write = write_event,
+};
+
+static int __init ltt_userspace_init(void)
+{
+	struct dentry *ltt_root_dentry;
+	int err = 0;
+
+	ltt_root_dentry = get_ltt_root();
+	if (!ltt_root_dentry) {
+		err = -ENOENT;
+		goto err_no_root;
+	}
+
+	ltt_event_file = debugfs_create_file(LTT_WRITE_EVENT_FILE,
+					     S_IWUGO,
+					     ltt_root_dentry,
+					     NULL,
+					     &ltt_userspace_operations);
+	if (IS_ERR(ltt_event_file) || !ltt_event_file) {
+		printk(KERN_ERR
+			"ltt_userspace_init: failed to create file %s\n",
+			LTT_WRITE_EVENT_FILE);
+		err = -EPERM;
+		goto err_no_file;
+	}
+
+	return err;
+err_no_file:
+	put_ltt_root();
+err_no_root:
+	return err;
+}
+
+static void __exit ltt_userspace_exit(void)
+{
+	debugfs_remove(ltt_event_file);
+	put_ltt_root();
+}
+
+module_init(ltt_userspace_init);
+module_exit(ltt_userspace_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>");
+MODULE_DESCRIPTION("Linux Trace Toolkit Userspace Event");
diff --git a/stblinux-2.6.31/ltt/probes/Makefile b/stblinux-2.6.31/ltt/probes/Makefile
new file mode 100644
index 0000000..56c4fff
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/Makefile
@@ -0,0 +1,48 @@
+# LTTng tracing probes
+
+ifdef CONFIG_FTRACE
+CFLAGS_REMOVE_kernel-trace.o = -pg
+CFLAGS_REMOVE_mm-trace.o = -pg
+CFLAGS_REMOVE_fs-trace.o = -pg
+CFLAGS_REMOVE_ipc-trace.o = -pg
+CFLAGS_REMOVE_lockdep-trace.o = -pg
+CFLAGS_REMOVE_rcu-trace.o = -pg
+CFLAGS_REMOVE_syscall-trace.o = -pg
+CFLAGS_REMOVE_trap-trace.o = -pg
+CFLAGS_REMOVE_pm-trace.o = -pg
+endif
+
+obj-$(CONFIG_LTT_TRACEPROBES)	+= kernel-trace.o mm-trace.o fs-trace.o \
+				ipc-trace.o lockdep-trace.o rcu-trace.o \
+				syscall-trace.o trap-trace.o pm-trace.o
+
+ifeq ($(CONFIG_NET),y)
+ifdef CONFIG_FTRACE
+CFLAGS_REMOVE_net-trace.o = -pg
+CFLAGS_REMOVE_net-extended-trace.o = -pg
+endif
+obj-$(CONFIG_LTT_TRACEPROBES)	+= net-trace.o net-extended-trace.o
+endif
+
+ifdef CONFIG_JBD2
+ifdef CONFIG_FTRACE
+CFLAGS_REMOVE_jbd2-trace.o = -pg
+endif
+obj-$(CONFIG_LTT_TRACEPROBES)	+= jbd2-trace.o
+endif
+
+ifdef CONFIG_EXT4_FS
+ifdef CONFIG_FTRACE
+CFLAGS_REMOVE_ext4-trace.o = -pg
+endif
+obj-$(CONFIG_LTT_TRACEPROBES)	+= ext4-trace.o
+endif
+
+ifdef CONFIG_BLOCK
+ifdef CONFIG_FTRACE
+CFLAGS_REMOVE_block-trace.o = -pg
+endif
+obj-$(CONFIG_LTT_TRACEPROBES)	+= block-trace.o
+endif
+
+
diff --git a/stblinux-2.6.31/ltt/probes/block-trace.c b/stblinux-2.6.31/ltt/probes/block-trace.c
new file mode 100644
index 0000000..6a90744
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/block-trace.c
@@ -0,0 +1,309 @@
+/*
+ * ltt/probes/block-trace.c
+ *
+ * block layer tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+
+#include <trace/events/block.h>
+
+/*
+ * Add rq cmd as a sequence. Needs new type. (size + binary blob)
+ */
+
+void probe_block_rq_abort(struct request_queue *q, struct request *rq)
+{
+	int rw = rq->cmd_flags & 0x03;
+
+	if (blk_discard_rq(rq))
+		rw |= (1 << BIO_RW_DISCARD);
+
+	if (blk_pc_request(rq)) {
+		trace_mark_tp(block, rq_abort_pc, block_rq_abort,
+			probe_block_rq_abort,
+			"data_len %u rw %d errors %d",
+			blk_rq_bytes(rq), rw, rq->errors);
+	} else {
+		/*
+		 * FIXME Using a simple trace_mark for the second event
+		 * possibility because tracepoints do not support multiple
+		 * connections to the same probe yet. They should have some
+		 * refcounting. Need to enable both rq_abort_pc and rq_abort_fs
+		 * markers to have the rq_abort_fs marker enabled.
+		 */
+		trace_mark(block, rq_abort_fs,
+			"hard_sector %llu "
+			"rw %d errors %d", (unsigned long long)blk_rq_pos(rq),
+			rw, rq->errors);
+	}
+}
+
+void probe_block_rq_insert(struct request_queue *q, struct request *rq)
+{
+	int rw = rq->cmd_flags & 0x03;
+
+	if (blk_discard_rq(rq))
+		rw |= (1 << BIO_RW_DISCARD);
+
+	if (blk_pc_request(rq)) {
+		trace_mark_tp(block, rq_insert_pc, block_rq_insert,
+			probe_block_rq_insert,
+			"data_len %u rw %d errors %d",
+			blk_rq_bytes(rq), rw, rq->errors);
+	} else {
+		/*
+		 * FIXME Using a simple trace_mark for the second event
+		 * possibility because tracepoints do not support multiple
+		 * connections to the same probe yet. They should have some
+		 * refcounting. Need to enable both rq_insert_pc and
+		 * rq_insert_fs markers to have the rq_insert_fs marker enabled.
+		 */
+		trace_mark(block, rq_insert_fs,
+			"hard_sector %llu "
+			"rw %d errors %d", (unsigned long long)blk_rq_pos(rq),
+			rw, rq->errors);
+	}
+}
+
+void probe_block_rq_issue(struct request_queue *q, struct request *rq)
+{
+	int rw = rq->cmd_flags & 0x03;
+
+	if (blk_discard_rq(rq))
+		rw |= (1 << BIO_RW_DISCARD);
+
+	if (blk_pc_request(rq)) {
+		trace_mark_tp(block, rq_issue_pc, block_rq_issue,
+			probe_block_rq_issue,
+			"data_len %u rw %d errors %d",
+			blk_rq_bytes(rq), rw, rq->errors);
+	} else {
+		/*
+		 * FIXME Using a simple trace_mark for the second event
+		 * possibility because tracepoints do not support multiple
+		 * connections to the same probe yet. They should have some
+		 * refcounting. Need to enable both rq_issue_pc and rq_issue_fs
+		 * markers to have the rq_issue_fs marker enabled.
+		 */
+		trace_mark(block, rq_issue_fs,
+			"hard_sector %llu "
+			"rw %d errors %d", (unsigned long long)blk_rq_pos(rq),
+			rw, rq->errors);
+	}
+}
+
+void probe_block_rq_requeue(struct request_queue *q, struct request *rq)
+{
+	int rw = rq->cmd_flags & 0x03;
+
+	if (blk_discard_rq(rq))
+		rw |= (1 << BIO_RW_DISCARD);
+
+	if (blk_pc_request(rq)) {
+		trace_mark_tp(block, rq_requeue_pc, block_rq_requeue,
+			probe_block_rq_requeue,
+			"data_len %u rw %d errors %d",
+			blk_rq_bytes(rq), rw, rq->errors);
+	} else {
+		/*
+		 * FIXME Using a simple trace_mark for the second event
+		 * possibility because tracepoints do not support multiple
+		 * connections to the same probe yet. They should have some
+		 * refcounting. Need to enable both rq_requeue_pc and
+		 * rq_requeue_fs markers to have the rq_requeue_fs marker
+		 * enabled.
+		 */
+		trace_mark(block, rq_requeue_fs,
+			"hard_sector %llu "
+			"rw %d errors %d", (unsigned long long)blk_rq_pos(rq),
+			rw, rq->errors);
+	}
+}
+
+void probe_block_rq_complete(struct request_queue *q, struct request *rq)
+{
+	int rw = rq->cmd_flags & 0x03;
+
+	if (blk_discard_rq(rq))
+		rw |= (1 << BIO_RW_DISCARD);
+
+	if (blk_pc_request(rq)) {
+		trace_mark_tp(block, rq_complete_pc, block_rq_complete,
+			probe_block_rq_complete,
+			"data_len %u rw %d errors %d",
+			blk_rq_bytes(rq), rw, rq->errors);
+	} else {
+		/*
+		 * FIXME Using a simple trace_mark for the second event
+		 * possibility because tracepoints do not support multiple
+		 * connections to the same probe yet. They should have some
+		 * refcounting. Need to enable both rq_complete_pc and
+		 * rq_complete_fs markers to have the rq_complete_fs marker
+		 * enabled.
+		 */
+		trace_mark(block, rq_complete_fs,
+			"hard_sector %llu "
+			"rw %d errors %d", (unsigned long long)blk_rq_pos(rq),
+			rw, rq->errors);
+	}
+}
+
+void probe_block_bio_bounce(struct request_queue *q, struct bio *bio)
+{
+	trace_mark_tp(block, bio_bounce, block_bio_bounce,
+		probe_block_bio_bounce,
+		"sector %llu size %u rw(FAILFAST_DRIVER,FAILFAST_TRANSPORT,"
+		"FAILFAST_DEV,DISCARD,META,SYNC,BARRIER,AHEAD,RW) %lX "
+		"not_uptodate #1u%d",
+		(unsigned long long)bio->bi_sector, bio->bi_size,
+		bio->bi_rw, !bio_flagged(bio, BIO_UPTODATE));
+}
+
+void probe_block_bio_complete(struct request_queue *q, struct bio *bio)
+{
+	trace_mark_tp(block, bio_complete, block_bio_complete,
+		probe_block_bio_complete,
+		"sector %llu size %u rw(FAILFAST_DRIVER,FAILFAST_TRANSPORT,"
+		"FAILFAST_DEV,DISCARD,META,SYNC,BARRIER,AHEAD,RW) %lX "
+		"not_uptodate #1u%d",
+		(unsigned long long)bio->bi_sector, bio->bi_size,
+		bio->bi_rw, !bio_flagged(bio, BIO_UPTODATE));
+}
+
+void probe_block_bio_backmerge(struct request_queue *q, struct bio *bio)
+{
+	trace_mark_tp(block, bio_backmerge, block_bio_backmerge,
+		probe_block_bio_backmerge,
+		"sector %llu size %u rw(FAILFAST_DRIVER,FAILFAST_TRANSPORT,"
+		"FAILFAST_DEV,DISCARD,META,SYNC,BARRIER,AHEAD,RW) %lX "
+		"not_uptodate #1u%d",
+		(unsigned long long)bio->bi_sector, bio->bi_size,
+		bio->bi_rw, !bio_flagged(bio, BIO_UPTODATE));
+}
+
+void probe_block_bio_frontmerge(struct request_queue *q, struct bio *bio)
+{
+	trace_mark_tp(block, bio_frontmerge, block_bio_frontmerge,
+		probe_block_bio_frontmerge,
+		"sector %llu size %u rw(FAILFAST_DRIVER,FAILFAST_TRANSPORT,"
+		"FAILFAST_DEV,DISCARD,META,SYNC,BARRIER,AHEAD,RW) %lX "
+		"not_uptodate #1u%d",
+		(unsigned long long)bio->bi_sector, bio->bi_size,
+		bio->bi_rw, !bio_flagged(bio, BIO_UPTODATE));
+}
+
+void probe_block_bio_queue(struct request_queue *q, struct bio *bio)
+{
+	trace_mark_tp(block, bio_queue, block_bio_queue,
+		probe_block_bio_queue,
+		"sector %llu size %u rw(FAILFAST_DRIVER,FAILFAST_TRANSPORT,"
+		"FAILFAST_DEV,DISCARD,META,SYNC,BARRIER,AHEAD,RW) %lX "
+		"not_uptodate #1u%d",
+		(unsigned long long)bio->bi_sector, bio->bi_size,
+		bio->bi_rw, !bio_flagged(bio, BIO_UPTODATE));
+}
+
+void probe_block_getrq(struct request_queue *q, struct bio *bio, int rw)
+{
+	if (bio) {
+		trace_mark_tp(block, getrq_bio, block_getrq,
+			probe_block_getrq,
+			"sector %llu size %u "
+			"rw(FAILFAST_DRIVER,FAILFAST_TRANSPORT,"
+			"FAILFAST_DEV,DISCARD,META,SYNC,BARRIER,AHEAD,RW) %lX "
+			"not_uptodate #1u%d",
+			(unsigned long long)bio->bi_sector, bio->bi_size,
+			bio->bi_rw, !bio_flagged(bio, BIO_UPTODATE));
+	} else {
+		/*
+		 * FIXME Using a simple trace_mark for the second event
+		 * possibility because tracepoints do not support multiple
+		 * connections to the same probe yet. They should have some
+		 * refcounting. Need to enable both getrq_bio and getrq markers
+		 * to have the getrq marker enabled.
+		 */
+		trace_mark(block, getrq, "rw %d", rw);
+	}
+}
+
+void probe_block_sleeprq(struct request_queue *q, struct bio *bio, int rw)
+{
+	if (bio) {
+		trace_mark_tp(block, sleeprq_bio, block_sleeprq,
+			probe_block_sleeprq,
+			"sector %llu size %u "
+			"rw(FAILFAST_DRIVER,FAILFAST_TRANSPORT,"
+			"FAILFAST_DEV,DISCARD,META,SYNC,BARRIER,AHEAD,RW) %lX "
+			"not_uptodate #1u%d",
+			(unsigned long long)bio->bi_sector, bio->bi_size,
+			bio->bi_rw, !bio_flagged(bio, BIO_UPTODATE));
+	} else {
+		/*
+		 * FIXME Using a simple trace_mark for the second event
+		 * possibility because tracepoints do not support multiple
+		 * connections to the same probe yet. They should have some
+		 * refcounting. Need to enable both sleeprq_bio and sleeprq
+		 * markers to have the sleeprq marker enabled.
+		 */
+		trace_mark(block, sleeprq, "rw %d", rw);
+	}
+}
+
+void probe_block_plug(struct request_queue *q)
+{
+	trace_mark_tp(block, plug, block_plug, probe_block_plug,
+			 MARK_NOARGS);
+}
+
+void probe_block_unplug_io(struct request_queue *q)
+{
+	unsigned int pdu = q->rq.count[READ] + q->rq.count[WRITE];
+
+	trace_mark_tp(block, unplug_io, block_unplug_io, probe_block_unplug_io,
+			"pdu %u", pdu);
+}
+
+void probe_block_unplug_timer(struct request_queue *q)
+{
+	unsigned int pdu = q->rq.count[READ] + q->rq.count[WRITE];
+
+	trace_mark_tp(block, unplug_timer, block_unplug_timer,
+			probe_block_unplug_timer,
+			"pdu %u", pdu);
+}
+
+void probe_block_split(struct request_queue *q, struct bio *bio,
+		       unsigned int pdu)
+{
+	trace_mark_tp(block, split, block_split,
+		probe_block_split,
+		"sector %llu size %u rw(FAILFAST_DRIVER,FAILFAST_TRANSPORT,"
+		"FAILFAST_DEV,DISCARD,META,SYNC,BARRIER,AHEAD,RW) %lX "
+		"not_uptodate #1u%d pdu %u",
+		(unsigned long long)bio->bi_sector, bio->bi_size,
+		bio->bi_rw, !bio_flagged(bio, BIO_UPTODATE), pdu);
+}
+
+void probe_block_remap(struct request_queue *q, struct bio *bio,
+		       dev_t dev, sector_t from)
+{
+	trace_mark_tp(block, remap, block_remap,
+		probe_block_remap,
+		"device_from %lu sector_from %llu device_to %lu "
+		"size %u rw(FAILFAST_DRIVER,FAILFAST_TRANSPORT,"
+		"FAILFAST_DEV,DISCARD,META,SYNC,BARRIER,AHEAD,RW) %lX "
+		"not_uptodate #1u%d",
+		(unsigned long)bio->bi_bdev->bd_dev,
+		(unsigned long long)from,
+		(unsigned long)dev,
+		bio->bi_size, bio->bi_rw,
+		!bio_flagged(bio, BIO_UPTODATE));
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Block Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/ext4-trace.c b/stblinux-2.6.31/ltt/probes/ext4-trace.c
new file mode 100644
index 0000000..5d397de
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/ext4-trace.c
@@ -0,0 +1,612 @@
+/*
+ * ltt/probes/ext4-trace.c
+ *
+ * ext4 tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/writeback.h>
+#include <linux/ltt-tracer.h>
+#include <linux/debugfs.h>
+#include <linux/mutex.h>
+#include <linux/rcupdate.h>
+#include <trace/events/ext4.h>
+
+#include "../../fs/ext4/mballoc.h"
+
+static struct dentry *ext4_filter_dentry, *ext4_filter_dev_dentry,
+	*ext4_filter_inode_dentry;
+static DEFINE_MUTEX(ext4_filter_mutex);
+/* Make sure we don't race between module exit and file write */
+static int module_exits;
+
+struct rcu_dev_filter {
+	struct rcu_head rcu;
+	char devname[NAME_MAX];
+};
+
+static struct rcu_dev_filter *dev_filter;
+/* ~0UL inode_filter enables all inodes */
+static unsigned long inode_filter = ~0UL;
+
+/*
+ * Probes are executed in rcu_sched read-side critical section.
+ */
+
+static int do_dev_filter(const char *dev)
+{
+	struct rcu_dev_filter *ldev_filter = rcu_dereference(dev_filter);
+
+	if (unlikely(ldev_filter))
+		if (unlikely(strcmp(ldev_filter->devname, dev)))
+			return 0;
+	return 1;
+}
+
+static int do_inode_filter(unsigned long ino)
+{
+	if (unlikely(inode_filter != ~0UL))
+		if (unlikely(inode_filter != ino))
+			return 0;
+	return 1;
+}
+
+/*
+ * Logical AND between dev and inode filter.
+ */
+static int do_filter(const char *dev, unsigned long ino)
+{
+	if (unlikely(!do_dev_filter(dev)))
+		return 0;
+	if (unlikely(!do_inode_filter(ino)))
+		return 0;
+	return 1;
+}
+
+
+void probe_ext4_free_inode(struct inode *inode)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, free_inode, ext4_free_inode,
+		probe_ext4_free_inode,
+		"dev %s ino %lu mode %d uid %lu gid %lu blocks %llu",
+		inode->i_sb->s_id, inode->i_ino, inode->i_mode,
+		(unsigned long) inode->i_uid, (unsigned long) inode->i_gid,
+		(unsigned long long) inode->i_blocks);
+}
+
+void probe_ext4_request_inode(struct inode *dir, int mode)
+{
+	if (unlikely(!do_filter(dir->i_sb->s_id, dir->i_ino)))
+		return;
+	trace_mark_tp(ext4, request_inode, ext4_request_inode,
+		probe_ext4_request_inode,
+		"dev %s dir %lu mode %d",
+		dir->i_sb->s_id, dir->i_ino, mode);
+}
+
+void probe_ext4_allocate_inode(struct inode *inode, struct inode *dir, int mode)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)
+		     && !do_filter(dir->i_sb->s_id, dir->i_ino)))
+		return;
+	trace_mark_tp(ext4, allocate_inode, ext4_allocate_inode,
+		probe_ext4_allocate_inode,
+		"dev %s ino %lu dir %lu mode %d",
+		dir->i_sb->s_id, inode->i_ino, dir->i_ino, mode);
+}
+
+void probe_ext4_write_begin(struct inode *inode, loff_t pos, unsigned int len,
+			    unsigned int flags)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, write_begin, ext4_write_begin,
+		probe_ext4_write_begin,
+		"dev %s ino %lu pos %llu len %u flags %u",
+		inode->i_sb->s_id, inode->i_ino,
+		(unsigned long long) pos, len, flags);
+}
+
+void probe_ext4_ordered_write_end(struct inode *inode, loff_t pos,
+			    unsigned int len, unsigned int copied)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, ordered_write_end, ext4_ordered_write_end,
+		probe_ext4_ordered_write_end,
+		"dev %s ino %lu pos %llu len %u copied %u",
+		inode->i_sb->s_id, inode->i_ino,
+		(unsigned long long) pos, len, copied);
+}
+
+void probe_ext4_writeback_write_end(struct inode *inode, loff_t pos,
+			    unsigned int len, unsigned int copied)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, writeback_write_end, ext4_writeback_write_end,
+		probe_ext4_writeback_write_end,
+		"dev %s ino %lu pos %llu len %u copied %u",
+		inode->i_sb->s_id, inode->i_ino,
+		(unsigned long long) pos, len, copied);
+}
+
+void probe_ext4_journalled_write_end(struct inode *inode, loff_t pos,
+			    unsigned int len, unsigned int copied)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, journalled_write_end, ext4_journalled_write_end,
+		probe_ext4_journalled_write_end,
+		"dev %s ino %lu pos %llu len %u copied %u",
+		inode->i_sb->s_id, inode->i_ino,
+		(unsigned long long) pos, len, copied);
+}
+
+/*
+ * note : wbc_flags will have to be decoded by userspace.
+ * #1x uses a single byte in the trace. Limits to 8 bits.
+ */
+void probe_ext4_da_writepages(struct inode *inode,
+			      struct writeback_control *wbc)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, da_writepages, ext4_da_writepages,
+		probe_ext4_da_writepages,
+		"dev %s ino %lu nr_to_write %ld "
+		"pages_skipped %ld range_start %llu range_end %llu "
+		"wbc_flags(nonblocking,for_kupdate,"
+		"for_reclaim,for_writepages,range_cyclic) #1x%u",
+		inode->i_sb->s_id, inode->i_ino, wbc->nr_to_write,
+		wbc->pages_skipped,
+		(unsigned long long) wbc->range_start,
+		(unsigned long long) wbc->range_end,
+		  (wbc->nonblocking << 4)
+		| (wbc->for_kupdate << 3)
+		| (wbc->for_reclaim << 2)
+		| (wbc->for_writepages << 1)
+		| wbc->range_cyclic);
+}
+
+/*
+ * note : wbc_flags will have to be decoded by userspace.
+ * #1x uses a single byte in the trace. Limits to 8 bits.
+ */
+void probe_ext4_da_writepages_result(struct inode *inode,
+				     struct writeback_control *wbc,
+				     int ret, int pages_written)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, da_writepages_result, ext4_da_writepages_result,
+		probe_ext4_da_writepages_result,
+		"dev %s ino %lu ret %d pages_written %d "
+		"pages_skipped %ld "
+		"wbc_flags(encountered_congestion,"
+		"more_io,no_nrwrite_index_update) #1x%u",
+		inode->i_sb->s_id, inode->i_ino, ret, pages_written,
+		wbc->pages_skipped,
+		  (wbc->encountered_congestion << 2)
+		| (wbc->more_io << 1)
+		| wbc->no_nrwrite_index_update);
+}
+
+void probe_ext4_da_write_begin(struct inode *inode, loff_t pos,
+			    unsigned int len, unsigned int flags)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, da_write_begin, ext4_da_write_begin,
+		probe_ext4_da_write_begin,
+		"dev %s ino %lu pos %llu len %u flags %u",
+		inode->i_sb->s_id, inode->i_ino,
+		(unsigned long long) pos, len, flags);
+}
+
+void probe_ext4_da_write_end(struct inode *inode, loff_t pos,
+			    unsigned int len, unsigned int copied)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, da_write_end, ext4_da_write_end,
+		probe_ext4_da_write_end,
+		"dev %s ino %lu pos %llu len %u copied %u",
+		inode->i_sb->s_id, inode->i_ino,
+		(unsigned long long) pos, len, copied);
+}
+
+void probe_ext4_discard_blocks(struct super_block *sb, unsigned long long blk,
+			       unsigned long long count)
+{
+	if (unlikely(!do_dev_filter(sb->s_id)))
+		return;
+	trace_mark_tp(ext4, discard_blocks, ext4_discard_blocks,
+		probe_ext4_discard_blocks,
+		"dev %s blk %llu count %llu",
+		sb->s_id, blk, count);
+}
+
+void probe_ext4_mb_new_inode_pa(struct ext4_allocation_context *ac,
+			        struct ext4_prealloc_space *pa)
+{
+	if (unlikely(!do_filter(ac->ac_sb->s_id, ac->ac_inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, mb_new_inode_pa, ext4_mb_new_inode_pa,
+		probe_ext4_mb_new_inode_pa,
+		"dev %s ino %lu pstart %llu len %u lstart %u",
+		ac->ac_sb->s_id, ac->ac_inode->i_ino, pa->pa_pstart,
+		pa->pa_len, pa->pa_lstart);
+}
+
+void probe_ext4_mb_new_group_pa(struct ext4_allocation_context *ac,
+			        struct ext4_prealloc_space *pa)
+{
+	if (unlikely(!do_dev_filter(ac->ac_sb->s_id)))
+		return;
+	trace_mark_tp(ext4, mb_new_group_pa, ext4_mb_new_group_pa,
+		probe_ext4_mb_new_group_pa,
+		"dev %s pstart %llu len %u lstart %u",
+		ac->ac_sb->s_id, pa->pa_pstart,
+		pa->pa_len, pa->pa_lstart);
+}
+
+void probe_ext4_mb_release_inode_pa(struct ext4_allocation_context *ac,
+				    struct ext4_prealloc_space *pa,
+				    unsigned long long block,
+				    unsigned int count)
+{
+	if (unlikely(!do_filter(ac->ac_sb->s_id, ac->ac_inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, mb_release_inode_pa, ext4_mb_release_inode_pa,
+		probe_ext4_mb_release_inode_pa,
+		"dev %s ino %lu block %llu count %u",
+		ac->ac_sb->s_id, pa->pa_inode->i_ino, block, count);
+}
+
+void probe_ext4_mb_release_group_pa(struct ext4_allocation_context *ac,
+				    struct ext4_prealloc_space *pa)
+{
+	if (unlikely(!do_dev_filter(ac->ac_sb->s_id)))
+		return;
+	trace_mark_tp(ext4, mb_release_group_pa, ext4_mb_release_group_pa,
+		probe_ext4_mb_release_group_pa,
+		"dev %s pstart %llu len %d",
+		ac->ac_sb->s_id, pa->pa_pstart, pa->pa_len);
+}
+
+void probe_ext4_discard_preallocations(struct inode *inode)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, discard_preallocations,
+		ext4_discard_preallocations,
+		probe_ext4_discard_preallocations,
+		"dev %s ino %lu",
+		inode->i_sb->s_id, inode->i_ino);
+}
+
+void probe_ext4_mb_discard_preallocations(struct super_block *sb, int needed)
+{
+	if (unlikely(!do_dev_filter(sb->s_id)))
+		return;
+	trace_mark_tp(ext4, mb_discard_preallocations,
+		ext4_mb_discard_preallocations,
+		probe_ext4_mb_discard_preallocations,
+		"dev %s needed %d",
+		sb->s_id, needed);
+}
+
+void probe_ext4_request_blocks(struct ext4_allocation_request *ar)
+{
+	if (ar->inode) {
+		if (unlikely(!do_filter(ar->inode->i_sb->s_id,
+					ar->inode->i_ino)))
+			return;
+	} else {
+		if (unlikely(!do_dev_filter(ar->inode->i_sb->s_id)))
+			return;
+	}
+	trace_mark_tp(ext4, request_blocks, ext4_request_blocks,
+		probe_ext4_request_blocks,
+		"dev %s flags %u len %u ino %lu "
+		"lblk %llu goal %llu lleft %llu lright %llu "
+		"pleft %llu pright %llu",
+		ar->inode->i_sb->s_id, ar->flags, ar->len,
+		ar->inode ? ar->inode->i_ino : 0,
+		(unsigned long long) ar->logical,
+		(unsigned long long) ar->goal,
+		(unsigned long long) ar->lleft,
+		(unsigned long long) ar->lright,
+		(unsigned long long) ar->pleft,
+		(unsigned long long) ar->pright);
+}
+
+void probe_ext4_allocate_blocks(struct ext4_allocation_request *ar,
+				unsigned long long block)
+{
+	if (ar->inode) {
+		if (unlikely(!do_filter(ar->inode->i_sb->s_id,
+					ar->inode->i_ino)))
+			return;
+	} else {
+		if (unlikely(!do_dev_filter(ar->inode->i_sb->s_id)))
+			return;
+	}
+	trace_mark_tp(ext4, allocate_blocks, ext4_allocate_blocks,
+		probe_ext4_allocate_blocks,
+		"dev %s block %llu flags %u len %u ino %lu "
+		"logical %llu goal %llu lleft %llu lright %llu "
+		"pleft %llu pright %llu",
+		ar->inode->i_sb->s_id, (unsigned long long) block,
+		ar->flags, ar->len, ar->inode ? ar->inode->i_ino : 0,
+		(unsigned long long) ar->logical,
+		(unsigned long long) ar->goal,
+		(unsigned long long) ar->lleft,
+		(unsigned long long) ar->lright,
+		(unsigned long long) ar->pleft,
+		(unsigned long long) ar->pright);
+}
+
+void probe_ext4_free_blocks(struct inode *inode, __u64 block,
+			    unsigned long count, int metadata)
+{
+	if (unlikely(!do_filter(inode->i_sb->s_id, inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, free_blocks, ext4_free_blocks,
+		probe_ext4_free_blocks,
+		"dev %s block %llu count %lu metadata %d ino %lu",
+		inode->i_sb->s_id, (unsigned long long)block,
+		count, metadata, inode->i_ino);
+}
+
+void probe_ext4_sync_file(struct file *file, struct dentry *dentry,
+			  int datasync)
+{
+	if (unlikely(!do_dev_filter(dentry->d_inode->i_sb->s_id)))
+		return;
+	if (unlikely(!do_inode_filter(dentry->d_inode->i_ino)
+			&& !do_inode_filter(dentry->d_parent->d_inode->i_ino)))
+		return;
+	trace_mark_tp(ext4, sync_file, ext4_sync_file,
+		probe_ext4_sync_file,
+		"dev %s datasync %d ino %ld parent %ld",
+		dentry->d_inode->i_sb->s_id, datasync, dentry->d_inode->i_ino,
+		dentry->d_parent->d_inode->i_ino);
+}
+
+void probe_ext4_sync_fs(struct super_block *sb, int wait)
+{
+	if (unlikely(!do_dev_filter(sb->s_id)))
+		return;
+	trace_mark_tp(ext4, sync_fs, ext4_sync_fs,
+		probe_ext4_sync_fs,
+		"dev %s wait %d",
+		sb->s_id, wait);
+}
+
+static void free_dev_filter(struct rcu_head *head)
+{
+	kfree(container_of(head, struct rcu_dev_filter, rcu));
+}
+
+static ssize_t dev_filter_op_write(struct file *file,
+	const char __user *user_buf, size_t count, loff_t *ppos)
+{
+	int err = 0;
+	char buf[NAME_MAX];
+	int buf_size;
+	char name[NAME_MAX];
+	struct rcu_dev_filter *new, *old;
+
+	mutex_lock(&ext4_filter_mutex);
+	if (module_exits) {
+		err = -EPERM;
+		goto error;
+	}
+	buf_size = min(count, sizeof(buf) - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto error;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%s", name) != 1) {
+		err = -EPERM;
+		goto error;
+	}
+
+	old = dev_filter;
+
+	/* Empty string or * means all active */
+	if (name[0] == '\0' || (name[0] == '*' && name[1] == '\0')) {
+		new = NULL;
+	} else {
+		new = kmalloc(sizeof(*new), GFP_KERNEL);
+		strcpy(new->devname, name);
+	}
+
+	rcu_assign_pointer(dev_filter, new);
+	if (old)
+		call_rcu_sched(&old->rcu, free_dev_filter);
+
+	mutex_unlock(&ext4_filter_mutex);
+	return count;
+
+error:
+	mutex_unlock(&ext4_filter_mutex);
+	return err;
+}
+
+static ssize_t dev_filter_op_read(struct file *filp, char __user *buffer,
+	size_t count, loff_t *ppos)
+{
+	ssize_t bcount;
+	const char *devname;
+
+	mutex_lock(&ext4_filter_mutex);
+	if (!dev_filter)
+		devname = "*";
+	else
+		devname = dev_filter->devname;
+	bcount = simple_read_from_buffer(buffer, count, ppos,
+			devname, strlen(devname));
+	mutex_unlock(&ext4_filter_mutex);
+	return bcount;
+}
+
+static struct file_operations ext4_dev_file_operations = {
+	.write = dev_filter_op_write,
+	.read = dev_filter_op_read,
+};
+
+static ssize_t inode_filter_op_write(struct file *file,
+	const char __user *user_buf, size_t count, loff_t *ppos)
+{
+	int err = 0;
+	char buf[NAME_MAX];
+	int buf_size;
+	char name[NAME_MAX];
+	unsigned long inode_num;
+
+	mutex_lock(&ext4_filter_mutex);
+	if (module_exits) {
+		err = -EPERM;
+		goto error;
+	}
+	buf_size = min(count, sizeof(buf) - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto error;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%s", name) != 1) {
+		err = -EPERM;
+		goto error;
+	}
+
+	/* Empty string or * means all active */
+	if (name[0] == '\0' || (name[0] == '*' && name[1] == '\0')) {
+		inode_filter = ~0UL;
+	} else {
+		if (sscanf(buf, "%lu", &inode_num) != 1) {
+			err = -EPERM;
+			goto error;
+		}
+		inode_filter = inode_num;
+	}
+
+	mutex_unlock(&ext4_filter_mutex);
+	return count;
+
+error:
+	mutex_unlock(&ext4_filter_mutex);
+	return err;
+}
+
+static ssize_t inode_filter_op_read(struct file *filp, char __user *buffer,
+	size_t count, loff_t *ppos)
+{
+	ssize_t bcount;
+	char inode_str[NAME_MAX];
+
+	mutex_lock(&ext4_filter_mutex);
+	if (inode_filter == ~0UL)
+		strcpy(inode_str, "*");
+	else {
+		bcount = snprintf(inode_str, sizeof(inode_str), "%lu",
+			       inode_filter);
+		if (bcount == sizeof(inode_str))
+			bcount = -ENOSPC;
+		if (bcount < 0)
+			goto end;
+	}
+	bcount = simple_read_from_buffer(buffer, count, ppos,
+			inode_str, strlen(inode_str));
+end:
+	mutex_unlock(&ext4_filter_mutex);
+	return bcount;
+}
+
+static struct file_operations ext4_inode_file_operations = {
+	.write = inode_filter_op_write,
+	.read = inode_filter_op_read,
+};
+
+static void release_filter_dev(void)
+{
+	struct rcu_dev_filter *old;
+
+	mutex_lock(&ext4_filter_mutex);
+	module_exits = 1;
+	old = dev_filter;
+	rcu_assign_pointer(dev_filter, NULL);
+	if (old)
+		call_rcu_sched(&old->rcu, free_dev_filter);
+	mutex_unlock(&ext4_filter_mutex);
+}
+
+static int __init filter_init(void)
+{
+	struct dentry *filter_root_dentry;
+	int err = 0;
+
+	filter_root_dentry = get_filter_root();
+	if (!filter_root_dentry) {
+		err = -ENOENT;
+		goto end;
+	}
+
+	ext4_filter_dentry = debugfs_create_dir("ext4", filter_root_dentry);
+
+	if (IS_ERR(ext4_filter_dentry) || !ext4_filter_dentry) {
+		printk(KERN_ERR "Failed to create ext4 filter file\n");
+		err = -ENOMEM;
+		goto end;
+	}
+
+	ext4_filter_dev_dentry = debugfs_create_file("dev", S_IWUSR,
+			ext4_filter_dentry, NULL, &ext4_dev_file_operations);
+	if (IS_ERR(ext4_filter_dev_dentry) || !ext4_filter_dev_dentry) {
+		printk(KERN_ERR "Failed to create ext4 dev filter file\n");
+		err = -ENOMEM;
+		goto release_filter_dentry;
+	}
+
+	ext4_filter_inode_dentry = debugfs_create_file("inode", S_IWUSR,
+			ext4_filter_dentry, NULL, &ext4_inode_file_operations);
+	if (IS_ERR(ext4_filter_inode_dentry) || !ext4_filter_inode_dentry) {
+		printk(KERN_ERR "Failed to create ext4 inode filter file\n");
+		err = -ENOMEM;
+		goto release_filter_dev_dentry;
+	}
+
+	goto end;
+
+release_filter_dev_dentry:
+	debugfs_remove(ext4_filter_dev_dentry);
+release_filter_dentry:
+	debugfs_remove(ext4_filter_dentry);
+	release_filter_dev();
+end:
+	return err;
+}
+
+static void __exit filter_exit(void)
+{
+	debugfs_remove(ext4_filter_dev_dentry);
+	debugfs_remove(ext4_filter_inode_dentry);
+	debugfs_remove(ext4_filter_dentry);
+	release_filter_dev();
+}
+
+module_init(filter_init);
+module_exit(filter_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("ext4 Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/fs-trace.c b/stblinux-2.6.31/ltt/probes/fs-trace.c
new file mode 100644
index 0000000..d2636c1
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/fs-trace.c
@@ -0,0 +1,157 @@
+/*
+ * ltt/probes/fs-trace.c
+ *
+ * FS tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/buffer_head.h>
+#include <linux/ltt-type-serializer.h>
+#include <trace/fs.h>
+
+void probe_fs_buffer_wait_start(struct buffer_head *bh)
+{
+	trace_mark_tp(fs, buffer_wait_start, fs_buffer_wait_start,
+		probe_fs_buffer_wait_start, "bh %p", bh);
+}
+
+void probe_fs_buffer_wait_end(struct buffer_head *bh)
+{
+	trace_mark_tp(fs, buffer_wait_end, fs_buffer_wait_end,
+		probe_fs_buffer_wait_end, "bh %p", bh);
+}
+
+void probe_fs_exec(char *filename)
+{
+	trace_mark_tp(fs, exec, fs_exec, probe_fs_exec, "filename %s",
+		filename);
+}
+
+void probe_fs_ioctl(unsigned int fd, unsigned int cmd, unsigned long arg)
+{
+	trace_mark_tp(fs, ioctl, fs_ioctl, probe_fs_ioctl,
+		"fd %u cmd %u arg %lu", fd, cmd, arg);
+}
+
+void probe_fs_open(int fd, char *filename)
+{
+	trace_mark_tp(fs, open, fs_open, probe_fs_open,
+		"fd %d filename %s", fd, filename);
+}
+
+void probe_fs_close(unsigned int fd)
+{
+	trace_mark_tp(fs, close, fs_close, probe_fs_close, "fd %u", fd);
+}
+
+void probe_fs_lseek(unsigned int fd, long offset, unsigned int origin)
+{
+	trace_mark_tp(fs, lseek, fs_lseek, probe_fs_lseek,
+		"fd %u offset %ld origin %u", fd, offset, origin);
+}
+
+void probe_fs_llseek(unsigned int fd, loff_t offset, unsigned int origin)
+{
+	trace_mark_tp(fs, llseek, fs_llseek, probe_fs_llseek,
+		"fd %u offset %lld origin %u", fd,
+		(long long)offset, origin);
+}
+
+void probe_fs_read(unsigned int fd, char __user *buf, size_t count,
+		ssize_t ret);
+
+DEFINE_MARKER_TP(fs, read, fs_read, probe_fs_read,
+	"count %zu fd %u");
+
+notrace void probe_fs_read(unsigned int fd, char __user *buf, size_t count,
+		ssize_t ret)
+{
+	struct marker *marker;
+	struct serialize_sizet_int data;
+
+	data.f1 = count;
+	data.f2 = fd;
+
+	marker = &GET_MARKER(fs, read);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(size_t));
+}
+
+void probe_fs_write(unsigned int fd, char __user *buf, size_t count,
+		ssize_t ret);
+
+DEFINE_MARKER_TP(fs, write, fs_write, probe_fs_write,
+	"count %zu fd %u");
+
+notrace void probe_fs_write(unsigned int fd, char __user *buf, size_t count,
+		ssize_t ret)
+{
+	struct marker *marker;
+	struct serialize_sizet_int data;
+
+	data.f1 = count;
+	data.f2 = fd;
+
+	marker = &GET_MARKER(fs, write);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(size_t));
+}
+
+void probe_fs_pread64(unsigned int fd, char __user *buf, size_t count,
+		loff_t pos, ssize_t ret)
+{
+	trace_mark_tp(fs, pread64, fs_pread64, probe_fs_pread64,
+		"fd %u count %zu pos %llu",
+		fd, count, (unsigned long long)pos);
+}
+
+void probe_fs_pwrite64(unsigned int fd, const char __user *buf,
+		size_t count, loff_t pos, ssize_t ret)
+{
+	trace_mark_tp(fs, pwrite64, fs_pwrite64, probe_fs_pwrite64,
+		"fd %u count %zu pos %llu",
+		fd, count, (unsigned long long)pos);
+}
+
+void probe_fs_readv(unsigned long fd, const struct iovec __user *vec,
+		unsigned long vlen, ssize_t ret)
+{
+	trace_mark_tp(fs, readv, fs_readv, probe_fs_readv,
+		"fd %lu vlen %lu", fd, vlen);
+}
+
+void probe_fs_writev(unsigned long fd, const struct iovec __user *vec,
+		unsigned long vlen, ssize_t ret)
+{
+	trace_mark_tp(fs, writev, fs_writev, probe_fs_writev,
+		"fd %lu vlen %lu", fd, vlen);
+}
+
+void probe_fs_select(int fd, struct timespec *end_time)
+{
+	struct timespec tmptime;
+
+	if (end_time) {
+		tmptime = *end_time;
+	} else {
+		tmptime.tv_sec = -1L;
+		tmptime.tv_nsec = -1L;
+	}
+
+	trace_mark_tp(fs, select, fs_select, probe_fs_select,
+		"fd %d end_time_sec %ld end_time_nsec %ld", fd,
+			tmptime.tv_sec, tmptime.tv_nsec);
+}
+
+void probe_fs_poll(int fd)
+{
+	trace_mark_tp(fs, pollfd, fs_poll, probe_fs_poll,
+		"fd %d", fd);
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("FS Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/ipc-trace.c b/stblinux-2.6.31/ltt/probes/ipc-trace.c
new file mode 100644
index 0000000..91ccb4e
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/ipc-trace.c
@@ -0,0 +1,39 @@
+/*
+ * ltt/probes/ipc-trace.c
+ *
+ * IPC tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <trace/ipc.h>
+
+void probe_ipc_msg_create(long id, int flags)
+{
+	trace_mark_tp(ipc, msg_create, ipc_msg_create, probe_ipc_msg_create,
+		"id %ld flags %d", id, flags);
+}
+
+void probe_ipc_sem_create(long id, int flags)
+{
+	trace_mark_tp(ipc, sem_create, ipc_sem_create, probe_ipc_sem_create,
+		"id %ld flags %d", id, flags);
+}
+
+void probe_ipc_shm_create(long id, int flags)
+{
+	trace_mark_tp(ipc, shm_create, ipc_shm_create, probe_ipc_shm_create,
+		"id %ld flags %d", id, flags);
+}
+
+void probe_ipc_call(unsigned int call, unsigned int first)
+{
+	trace_mark_tp(ipc, call, ipc_call, probe_ipc_call,
+		"call %u first %d", call, first);
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("IPC Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/jbd2-trace.c b/stblinux-2.6.31/ltt/probes/jbd2-trace.c
new file mode 100644
index 0000000..2a38a7b
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/jbd2-trace.c
@@ -0,0 +1,207 @@
+/*
+ * ltt/probes/jbd2-trace.c
+ *
+ * JBD2 tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/ltt-tracer.h>
+#include <linux/debugfs.h>
+#include <linux/mutex.h>
+#include <linux/rcupdate.h>
+#include <trace/events/jbd2.h>
+
+static struct dentry *jbd2_filter_dentry, *jbd2_filter_dev_dentry;
+static DEFINE_MUTEX(jbd2_filter_mutex);
+/* Make sure we don't race between module exit and file write */
+static int module_exits;
+
+struct rcu_dev_filter {
+	struct rcu_head rcu;
+	char devname[NAME_MAX];
+};
+
+static struct rcu_dev_filter *dev_filter;
+
+/*
+ * Probes are executed in rcu_sched read-side critical section.
+ */
+static int do_filter(const char *dev)
+{
+	struct rcu_dev_filter *ldev_filter = rcu_dereference(dev_filter);
+
+	if (unlikely(ldev_filter))
+		if (unlikely(strcmp(ldev_filter->devname, dev)))
+			return 0;
+	return 1;
+}
+
+void probe_jbd2_checkpoint(journal_t *journal, int result)
+{
+	if (unlikely(!do_filter(journal->j_devname)))
+		return;
+	trace_mark_tp(jbd2, checkpoint, jbd2_checkpoint,
+		probe_jbd2_checkpoint, "dev %s need_checkpoint %d",
+		journal->j_devname, result);
+}
+
+void probe_jbd2_start_commit(journal_t *journal,
+			     transaction_t *commit_transaction)
+{
+	if (unlikely(!do_filter(journal->j_devname)))
+		return;
+	trace_mark_tp(jbd2, start_commit, jbd2_start_commit,
+		probe_jbd2_start_commit, "dev %s transaction %d",
+		journal->j_devname, commit_transaction->t_tid);
+}
+
+void probe_jbd2_end_commit(journal_t *journal,
+			   transaction_t *commit_transaction)
+{
+	if (unlikely(!do_filter(journal->j_devname)))
+		return;
+	trace_mark_tp(jbd2, end_commit, jbd2_end_commit,
+		probe_jbd2_end_commit, "dev %s transaction %d head %d",
+		journal->j_devname, commit_transaction->t_tid,
+		journal->j_tail_sequence);
+}
+
+static void free_dev_filter(struct rcu_head *head)
+{
+	kfree(container_of(head, struct rcu_dev_filter, rcu));
+}
+
+static ssize_t filter_op_write(struct file *file,
+	const char __user *user_buf, size_t count, loff_t *ppos)
+{
+	int err = 0;
+	char buf[NAME_MAX];
+	int buf_size;
+	char name[NAME_MAX];
+	struct rcu_dev_filter *new, *old;
+
+	mutex_lock(&jbd2_filter_mutex);
+	if (module_exits) {
+		err = -EPERM;
+		goto error;
+	}
+	buf_size = min(count, sizeof(buf) - 1);
+	err = copy_from_user(buf, user_buf, buf_size);
+	if (err)
+		goto error;
+	buf[buf_size] = 0;
+
+	if (sscanf(buf, "%s", name) != 1) {
+		err = -EPERM;
+		goto error;
+	}
+
+	old = dev_filter;
+
+	/* Empty string or * means all active */
+	if (name[0] == '\0' || (name[0] == '*' && name[1] == '\0')) {
+		new = NULL;
+	} else {
+		new = kmalloc(sizeof(*new), GFP_KERNEL);
+		strcpy(new->devname, name);
+	}
+
+	rcu_assign_pointer(dev_filter, new);
+	if (old)
+		call_rcu_sched(&old->rcu, free_dev_filter);
+
+	mutex_unlock(&jbd2_filter_mutex);
+	return count;
+
+error:
+	mutex_unlock(&jbd2_filter_mutex);
+	return err;
+}
+
+static ssize_t filter_op_read(struct file *filp, char __user *buffer,
+	size_t count, loff_t *ppos)
+{
+	ssize_t bcount;
+	const char *devname;
+
+	mutex_lock(&jbd2_filter_mutex);
+	if (!dev_filter)
+		devname = "*";
+	else
+		devname = dev_filter->devname;
+	bcount = simple_read_from_buffer(buffer, count, ppos,
+			devname, strlen(devname));
+	mutex_unlock(&jbd2_filter_mutex);
+	return bcount;
+}
+
+static struct file_operations jbd2_file_operations = {
+	.write = filter_op_write,
+	.read = filter_op_read,
+};
+
+static void release_filter_dev(void)
+{
+	struct rcu_dev_filter *old;
+
+	mutex_lock(&jbd2_filter_mutex);
+	module_exits = 1;
+	old = dev_filter;
+	rcu_assign_pointer(dev_filter, NULL);
+	if (old)
+		call_rcu_sched(&old->rcu, free_dev_filter);
+	mutex_unlock(&jbd2_filter_mutex);
+}
+
+static int __init filter_init(void)
+{
+	struct dentry *filter_root_dentry;
+	int err = 0;
+
+	filter_root_dentry = get_filter_root();
+	if (!filter_root_dentry) {
+		err = -ENOENT;
+		goto end;
+	}
+
+	jbd2_filter_dentry = debugfs_create_dir("jbd2", filter_root_dentry);
+
+	if (IS_ERR(jbd2_filter_dentry) || !jbd2_filter_dentry) {
+		printk(KERN_ERR "Failed to create jbd2 filter file\n");
+		err = -ENOMEM;
+		goto end;
+	}
+
+	jbd2_filter_dev_dentry = debugfs_create_file("dev", S_IWUSR,
+			jbd2_filter_dentry, NULL, &jbd2_file_operations);
+	if (IS_ERR(jbd2_filter_dentry) || !jbd2_filter_dentry) {
+		printk(KERN_ERR "Failed to create jbd2 filter file\n");
+		err = -ENOMEM;
+		goto release_filter_dentry;
+	}
+
+	goto end;
+
+release_filter_dentry:
+	debugfs_remove(jbd2_filter_dentry);
+	release_filter_dev();
+end:
+	return err;
+}
+
+static void __exit filter_exit(void)
+{
+	debugfs_remove(jbd2_filter_dev_dentry);
+	debugfs_remove(jbd2_filter_dentry);
+	release_filter_dev();
+}
+
+module_init(filter_init);
+module_exit(filter_exit);
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("JBD2 Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/kernel-trace.c b/stblinux-2.6.31/ltt/probes/kernel-trace.c
new file mode 100644
index 0000000..d198bba
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/kernel-trace.c
@@ -0,0 +1,579 @@
+/*
+ * ltt/probes/kernel-trace.c
+ *
+ * kernel tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/irq.h>
+#include <linux/ltt-tracer.h>
+#include <linux/ltt-type-serializer.h>
+#include <trace/irq.h>
+#include <trace/sched.h>
+#include <trace/timer.h>
+#include <trace/kernel.h>
+#include <trace/fault.h>
+#include <trace/events/sched.h>
+
+/*
+ * This should probably be added to s390.
+ */
+#ifdef CONFIG_S390
+static struct pt_regs *get_irq_regs(void)
+{
+	return task_pt_regs(current);
+}
+#endif
+
+/*
+ * FIXME :
+ * currently, the specialized tracepoint probes cannot call into other marker
+ * probes, such as ftrace enable/disable. Given we want them to be as fast as
+ * possible, it might not be so bad to lose this flexibility. But that means
+ * such probes would have to connect to tracepoints on their own.
+ */
+
+/* kernel_irq_entry specialized tracepoint probe */
+
+void probe_irq_entry(unsigned int id, struct pt_regs *regs,
+	struct irqaction *action);
+
+DEFINE_MARKER_TP(kernel, irq_entry, irq_entry, probe_irq_entry,
+	"ip %lu handler %p irq_id #2u%u kernel_mode #1u%u");
+
+notrace void probe_irq_entry(unsigned int id, struct pt_regs *regs,
+	struct irqaction *action)
+{
+	struct marker *marker;
+	struct serialize_long_long_short_char data;
+
+	if (unlikely(!regs))
+		regs = get_irq_regs();
+	if (likely(regs)) {
+		data.f1 = instruction_pointer(regs);
+		data.f4 = !user_mode(regs);
+	} else {
+		data.f1 = 0UL;
+		data.f4 = 1;
+	}
+	data.f2 = (unsigned long) (action ? action->handler : NULL);
+	data.f3 = id;
+
+	marker = &GET_MARKER(kernel, irq_entry);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+void probe_irq_next_handler(unsigned int id, struct irqaction *action,
+		irqreturn_t prev_ret);
+
+DEFINE_MARKER_TP(kernel, irq_next_handler, irq_next_handler,
+	probe_irq_next_handler,
+	"handler %p prev_ret #1u%u");
+
+notrace void probe_irq_next_handler(unsigned int id, struct irqaction *action,
+		irqreturn_t prev_ret)
+{
+	struct marker *marker;
+	struct serialize_long_char data;
+
+	data.f1 = (unsigned long) (action ? action->handler : NULL);
+	data.f2 = prev_ret;
+
+	marker = &GET_MARKER(kernel, irq_next_handler);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+/* kernel_irq_exit specialized tracepoint probe */
+
+void probe_irq_exit(irqreturn_t retval);
+
+DEFINE_MARKER_TP(kernel, irq_exit, irq_exit, probe_irq_exit,
+	"handled #1u%u");
+
+notrace void probe_irq_exit(irqreturn_t retval)
+{
+	struct marker *marker;
+	unsigned char data;
+
+	data = IRQ_RETVAL(retval);
+
+	marker = &GET_MARKER(kernel, irq_exit);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, sizeof(data), sizeof(data));
+}
+
+/* kernel_softirq_entry specialized tracepoint probe */
+
+void probe_softirq_entry(struct softirq_action *h,
+	struct softirq_action *softirq_vec);
+
+DEFINE_MARKER_TP(kernel, softirq_entry, softirq_entry,
+	probe_softirq_entry, "softirq_id #1u%lu");
+
+notrace void probe_softirq_entry(struct softirq_action *h,
+	struct softirq_action *softirq_vec)
+{
+	struct marker *marker;
+	unsigned char data;
+
+	data = ((unsigned long)h - (unsigned long)softirq_vec) / sizeof(*h);
+
+	marker = &GET_MARKER(kernel, softirq_entry);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, sizeof(data), sizeof(data));
+}
+
+/* kernel_softirq_exit specialized tracepoint probe */
+
+void probe_softirq_exit(struct softirq_action *h,
+	struct softirq_action *softirq_vec);
+
+DEFINE_MARKER_TP(kernel, softirq_exit, softirq_exit,
+	probe_softirq_exit, "softirq_id #1u%lu");
+
+notrace void probe_softirq_exit(struct softirq_action *h,
+	struct softirq_action *softirq_vec)
+{
+	struct marker *marker;
+	unsigned char data;
+
+	data = ((unsigned long)h - (unsigned long)softirq_vec) / sizeof(*h);
+
+	marker = &GET_MARKER(kernel, softirq_exit);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, sizeof(data), sizeof(data));
+}
+
+/* kernel_softirq_raise specialized tracepoint probe */
+
+void probe_softirq_raise(unsigned int nr);
+
+DEFINE_MARKER_TP(kernel, softirq_raise, softirq_raise,
+	probe_softirq_raise, "softirq_id #1u%u");
+
+notrace void probe_softirq_raise(unsigned int nr)
+{
+	struct marker *marker;
+	unsigned char data;
+
+	data = nr;
+
+	marker = &GET_MARKER(kernel, softirq_raise);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, sizeof(data), sizeof(data));
+}
+
+/* Standard probes */
+void probe_irq_tasklet_low_entry(struct tasklet_struct *t)
+{
+	trace_mark_tp(kernel, tasklet_low_entry, irq_tasklet_low_entry,
+		probe_irq_tasklet_low_entry, "func %p data %lu",
+		t->func, t->data);
+}
+
+void probe_irq_tasklet_low_exit(struct tasklet_struct *t)
+{
+	trace_mark_tp(kernel, tasklet_low_exit, irq_tasklet_low_exit,
+		probe_irq_tasklet_low_exit, "func %p data %lu",
+		t->func, t->data);
+}
+
+void probe_irq_tasklet_high_entry(struct tasklet_struct *t)
+{
+	trace_mark_tp(kernel, tasklet_high_entry, irq_tasklet_high_entry,
+		probe_irq_tasklet_high_entry, "func %p data %lu",
+		t->func, t->data);
+}
+
+void probe_irq_tasklet_high_exit(struct tasklet_struct *t)
+{
+	trace_mark_tp(kernel, tasklet_high_exit, irq_tasklet_high_exit,
+		probe_irq_tasklet_high_exit, "func %p data %lu",
+		t->func, t->data);
+}
+
+void probe_sched_kthread_stop(struct task_struct *t)
+{
+	trace_mark_tp(kernel, kthread_stop, sched_kthread_stop,
+		probe_sched_kthread_stop, "pid %d", t->pid);
+}
+
+void probe_sched_kthread_stop_ret(int ret)
+{
+	trace_mark_tp(kernel, kthread_stop_ret, sched_kthread_stop_ret,
+		probe_sched_kthread_stop_ret, "ret %d", ret);
+}
+
+void probe_sched_wait_task(struct rq *rq, struct task_struct *p)
+{
+	trace_mark_tp(kernel, sched_wait_task, sched_wait_task,
+		probe_sched_wait_task, "pid %d state #2d%ld",
+		p->pid, p->state);
+}
+
+/* kernel_sched_try_wakeup specialized tracepoint probe */
+
+void probe_sched_wakeup(struct rq *rq, struct task_struct *p);
+
+DEFINE_MARKER_TP(kernel, sched_try_wakeup, sched_wakeup,
+	probe_sched_wakeup, "pid %d cpu_id %u state #2d%ld");
+
+notrace void probe_sched_wakeup(struct rq *rq, struct task_struct *p)
+{
+	struct marker *marker;
+	struct serialize_int_int_short data;
+
+	data.f1 = p->pid;
+	data.f2 = task_cpu(p);
+	data.f3 = p->state;
+
+	marker = &GET_MARKER(kernel, sched_try_wakeup);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(int));
+}
+
+void probe_sched_wakeup_new(struct rq *rq, struct task_struct *p, int success)
+{
+	trace_mark_tp(kernel, sched_wakeup_new_task, sched_wakeup_new,
+		probe_sched_wakeup_new, "pid %d state #2d%ld cpu_id %u",
+		p->pid, p->state, task_cpu(p));
+}
+
+/* kernel_sched_schedule specialized tracepoint probe */
+
+void probe_sched_switch(struct rq *rq, struct task_struct *prev,
+		struct task_struct *next);
+
+DEFINE_MARKER_TP(kernel, sched_schedule, sched_switch, probe_sched_switch,
+	"prev_pid %d next_pid %d prev_state #2d%ld");
+
+notrace void probe_sched_switch(struct rq *rq, struct task_struct *prev,
+		struct task_struct *next)
+{
+	struct marker *marker;
+	struct serialize_int_int_short data;
+
+	data.f1 = prev->pid;
+	data.f2 = next->pid;
+	data.f3 = prev->state;
+
+	marker = &GET_MARKER(kernel, sched_schedule);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(int));
+}
+
+void probe_sched_migrate_task(struct task_struct *p, int dest_cpu)
+{
+	trace_mark_tp(kernel, sched_migrate_task, sched_migrate_task,
+		probe_sched_migrate_task, "pid %d state #2d%ld dest_cpu %d",
+		p->pid, p->state, dest_cpu);
+}
+
+void probe_sched_signal_send(int sig, struct task_struct *p)
+{
+	trace_mark_tp(kernel, send_signal, sched_signal_send,
+		probe_sched_signal_send, "pid %d signal %d", p->pid, sig);
+}
+
+void probe_sched_process_free(struct task_struct *p)
+{
+	trace_mark_tp(kernel, process_free, sched_process_free,
+		probe_sched_process_free, "pid %d", p->pid);
+}
+
+void probe_sched_process_exit(struct task_struct *p)
+{
+	trace_mark_tp(kernel, process_exit, sched_process_exit,
+		probe_sched_process_exit, "pid %d", p->pid);
+}
+
+void probe_sched_process_wait(struct pid *pid)
+{
+	trace_mark_tp(kernel, process_wait, sched_process_wait,
+		probe_sched_process_wait, "pid %d", pid_nr(pid));
+}
+
+void probe_sched_process_fork(struct task_struct *parent,
+		struct task_struct *child)
+{
+	trace_mark_tp(kernel, process_fork, sched_process_fork,
+		probe_sched_process_fork,
+		"parent_pid %d child_pid %d child_tgid %d",
+		parent->pid, child->pid, child->tgid);
+}
+
+void probe_sched_kthread_create(void *fn, int pid)
+{
+	trace_mark_tp(kernel, kthread_create, sched_kthread_create,
+		probe_sched_kthread_create,
+		"fn %p pid %d", fn, pid);
+}
+
+void probe_timer_itimer_expired(struct signal_struct *sig)
+{
+	trace_mark_tp(kernel, timer_itimer_expired, timer_itimer_expired,
+		probe_timer_itimer_expired, "pid %d",
+		pid_nr(sig->leader_pid));
+}
+
+void probe_timer_itimer_set(int which, struct itimerval *value)
+{
+	trace_mark_tp(kernel, timer_itimer_set,
+		timer_itimer_set, probe_timer_itimer_set,
+		"which %d interval_sec %ld interval_usec %ld "
+		"value_sec %ld value_usec %ld",
+		which,
+		value->it_interval.tv_sec,
+		value->it_interval.tv_usec,
+		value->it_value.tv_sec,
+		value->it_value.tv_usec);
+}
+
+/* kernel_timer_set specialized tracepoint probe */
+
+void probe_timer_set(struct timer_list *timer);
+
+DEFINE_MARKER_TP(kernel, timer_set, timer_set, probe_timer_set,
+	"expires %lu function %p data %lu");
+
+notrace void probe_timer_set(struct timer_list *timer)
+{
+	struct marker *marker;
+	struct serialize_long_long_long data;
+
+	data.f1 = timer->expires;
+	data.f2 = (unsigned long)timer->function;
+	data.f3 = timer->data;
+
+	marker = &GET_MARKER(kernel, timer_set);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+void probe_timer_update_time(struct timespec *_xtime,
+		struct timespec *_wall_to_monotonic)
+{
+	trace_mark_tp(kernel, timer_update_time, timer_update_time,
+		probe_timer_update_time,
+		"jiffies #8u%llu xtime_sec %ld xtime_nsec %ld "
+		"walltomonotonic_sec %ld walltomonotonic_nsec %ld",
+		(unsigned long long)jiffies_64, _xtime->tv_sec, _xtime->tv_nsec,
+		_wall_to_monotonic->tv_sec, _wall_to_monotonic->tv_nsec);
+}
+
+void probe_timer_timeout(struct task_struct *p)
+{
+	trace_mark_tp(kernel, timer_timeout, timer_timeout,
+		probe_timer_timeout, "pid %d", p->pid);
+}
+
+void probe_kernel_printk(unsigned long retaddr)
+{
+	trace_mark_tp(kernel, printk, kernel_printk,
+		probe_kernel_printk, "ip 0x%lX", retaddr);
+}
+
+void probe_kernel_vprintk(unsigned long retaddr, char *buf, int len)
+{
+	if (len > 0) {
+		unsigned int loglevel;
+		int mark_len;
+		char *mark_buf;
+		char saved_char;
+
+		if (buf[0] == '<' && buf[1] >= '0' &&
+		   buf[1] <= '7' && buf[2] == '>') {
+			loglevel = buf[1] - '0';
+			mark_buf = &buf[3];
+			mark_len = len - 3;
+		} else {
+			loglevel = default_message_loglevel;
+			mark_buf = buf;
+			mark_len = len;
+		}
+		if (mark_buf[mark_len - 1] == '\n')
+			mark_len--;
+		saved_char = mark_buf[mark_len];
+		mark_buf[mark_len] = '\0';
+		trace_mark_tp(kernel, vprintk, kernel_vprintk,
+			probe_kernel_vprintk,
+			"loglevel #1u%u string %s ip 0x%lX",
+			loglevel, mark_buf, retaddr);
+		mark_buf[mark_len] = saved_char;
+	}
+}
+
+#ifdef CONFIG_MODULES
+void probe_kernel_module_free(struct module *mod)
+{
+	trace_mark_tp(kernel, module_free, kernel_module_free,
+		probe_kernel_module_free, "name %s", mod->name);
+}
+
+void probe_kernel_module_load(struct module *mod)
+{
+	trace_mark_tp(kernel, module_load, kernel_module_load,
+		probe_kernel_module_load, "name %s", mod->name);
+}
+#endif
+
+void probe_kernel_panic(const char *fmt, va_list args)
+{
+	char info[64];
+	vsnprintf(info, sizeof(info), fmt, args);
+	trace_mark_tp(kernel, panic, kernel_panic, probe_kernel_panic,
+		"info %s", info);
+}
+
+void probe_kernel_kernel_kexec(struct kimage *image)
+{
+	trace_mark_tp(kernel, kernel_kexec, kernel_kernel_kexec,
+		probe_kernel_kernel_kexec, "image %p", image);
+}
+
+void probe_kernel_crash_kexec(struct kimage *image, struct pt_regs *regs)
+{
+	trace_mark_tp(kernel, crash_kexec, kernel_crash_kexec,
+		probe_kernel_crash_kexec, "image %p ip %p", image,
+		regs ? (void *)instruction_pointer(regs) : NULL);
+}
+
+/* kernel_page_fault_entry specialized tracepoint probe */
+
+void probe_kernel_page_fault_entry(struct pt_regs *regs, int trapnr,
+	struct mm_struct *mm, struct vm_area_struct *vma,
+	unsigned long address, int write_access);
+
+DEFINE_MARKER_TP(kernel, page_fault_entry, page_fault_entry,
+	probe_kernel_page_fault_entry,
+	"ip #p%lu address #p%lu trap_id #2u%u write_access #1u%u");
+
+notrace void probe_kernel_page_fault_entry(struct pt_regs *regs, int trapnr,
+	struct mm_struct *mm, struct vm_area_struct *vma,
+	unsigned long address, int write_access)
+{
+	struct marker *marker;
+	struct serialize_long_long_short_char data;
+
+	if (likely(regs))
+		data.f1 = instruction_pointer(regs);
+	else
+		data.f1 = 0UL;
+	data.f2 = address;
+	data.f3 = (unsigned short)trapnr;
+	data.f4 = (unsigned char)!!write_access;
+
+	marker = &GET_MARKER(kernel, page_fault_entry);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+/* kernel_page_fault_exit specialized tracepoint probe */
+
+void probe_kernel_page_fault_exit(int res);
+
+DEFINE_MARKER_TP(kernel, page_fault_exit, page_fault_exit,
+	probe_kernel_page_fault_exit,
+	"res %d");
+
+notrace void probe_kernel_page_fault_exit(int res)
+{
+	struct marker *marker;
+
+	marker = &GET_MARKER(kernel, page_fault_exit);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&res, sizeof(res), sizeof(res));
+}
+
+/* kernel_page_fault_nosem_entry specialized tracepoint probe */
+
+void probe_kernel_page_fault_nosem_entry(struct pt_regs *regs,
+	int trapnr, unsigned long address);
+
+DEFINE_MARKER_TP(kernel, page_fault_nosem_entry, page_fault_nosem_entry,
+	probe_kernel_page_fault_nosem_entry,
+	"ip #p%lu address #p%lu trap_id #2u%u");
+
+notrace void probe_kernel_page_fault_nosem_entry(struct pt_regs *regs,
+	int trapnr, unsigned long address)
+{
+	struct marker *marker;
+	struct serialize_long_long_short data;
+
+	if (likely(regs))
+		data.f1 = instruction_pointer(regs);
+	else
+		data.f1 = 0UL;
+	data.f2 = address;
+	data.f3 = (unsigned short)trapnr;
+
+	marker = &GET_MARKER(kernel, page_fault_nosem_entry);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+/* kernel_page_fault_nosem_exit specialized tracepoint probe */
+
+void probe_kernel_page_fault_nosem_exit(int res);
+
+DEFINE_MARKER_TP(kernel, page_fault_nosem_exit, page_fault_nosem_exit,
+	probe_kernel_page_fault_nosem_exit,
+	MARK_NOARGS);
+
+notrace void probe_kernel_page_fault_nosem_exit(int res)
+{
+	struct marker *marker;
+
+	marker = &GET_MARKER(kernel, page_fault_nosem_exit);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		NULL, 0, 0);
+}
+
+/* kernel_page_fault_get_user_entry specialized tracepoint probe */
+
+void probe_kernel_page_fault_get_user_entry(struct mm_struct *mm,
+	struct vm_area_struct *vma, unsigned long address, int write_access);
+
+DEFINE_MARKER_TP(kernel, page_fault_get_user_entry, page_fault_get_user_entry,
+	probe_kernel_page_fault_get_user_entry,
+	"address #p%lu write_access #1u%u");
+
+notrace void probe_kernel_page_fault_get_user_entry(struct mm_struct *mm,
+	struct vm_area_struct *vma, unsigned long address, int write_access)
+{
+	struct marker *marker;
+	struct serialize_long_char data;
+
+	data.f1 = address;
+	data.f2 = (unsigned char)!!write_access;
+
+	marker = &GET_MARKER(kernel, page_fault_get_user_entry);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+/* kernel_page_fault_get_user_exit specialized tracepoint probe */
+
+void probe_kernel_page_fault_get_user_exit(int res);
+
+DEFINE_MARKER_TP(kernel, page_fault_get_user_exit, page_fault_get_user_exit,
+	probe_kernel_page_fault_get_user_exit,
+	"res %d");
+
+notrace void probe_kernel_page_fault_get_user_exit(int res)
+{
+	struct marker *marker;
+
+	marker = &GET_MARKER(kernel, page_fault_get_user_exit);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&res, sizeof(res), sizeof(res));
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("kernel Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/lockdep-trace.c b/stblinux-2.6.31/ltt/probes/lockdep-trace.c
new file mode 100644
index 0000000..61af4d3
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/lockdep-trace.c
@@ -0,0 +1,60 @@
+/*
+ * ltt/probes/lockdep-trace.c
+ *
+ * lockdep tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/lockdep.h>
+#include <trace/lockdep.h>
+
+void probe_lockdep_hardirqs_on(unsigned long retaddr)
+{
+	trace_mark_tp(lockdep, hardirqs_on, lockdep_hardirqs_on,
+		probe_lockdep_hardirqs_on, "retaddr 0x%lX", retaddr);
+}
+
+void probe_lockdep_hardirqs_off(unsigned long retaddr)
+{
+	trace_mark_tp(lockdep, hardirqs_off, lockdep_hardirqs_off,
+		probe_lockdep_hardirqs_off, "retaddr 0x%lX", retaddr);
+}
+
+void probe_lockdep_softirqs_on(unsigned long retaddr)
+{
+	trace_mark_tp(lockdep, softirqs_on, lockdep_softirqs_on,
+		probe_lockdep_softirqs_on, "retaddr 0x%lX", retaddr);
+}
+
+void probe_lockdep_softirqs_off(unsigned long retaddr)
+{
+	trace_mark_tp(lockdep, softirqs_off, lockdep_softirqs_off,
+		probe_lockdep_softirqs_off, "retaddr 0x%lX", retaddr);
+}
+
+void probe_lockdep_lock_acquire(unsigned long retaddr,
+		unsigned int subclass, struct lockdep_map *lock, int trylock,
+		int read, int hardirqs_off)
+{
+	trace_mark_tp(lockdep, lock_acquire, lockdep_lock_acquire,
+		probe_lockdep_lock_acquire,
+		"retaddr 0x%lX subclass %u lock %p trylock %d read %d "
+		"hardirqs_off %d",
+		retaddr, subclass, lock, trylock, read, hardirqs_off);
+}
+
+void probe_lockdep_lock_release(unsigned long retaddr,
+		struct lockdep_map *lock, int nested)
+{
+	trace_mark_tp(lockdep, lock_release, lockdep_lock_release,
+		probe_lockdep_lock_release,
+		"retaddr 0x%lX lock %p nested %d",
+		retaddr, lock, nested);
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("lockdep Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/mm-trace.c b/stblinux-2.6.31/ltt/probes/mm-trace.c
new file mode 100644
index 0000000..2f73d15
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/mm-trace.c
@@ -0,0 +1,145 @@
+/*
+ * ltt/probes/mm-trace.c
+ *
+ * MM tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/swap.h>
+#include <linux/ltt-type-serializer.h>
+
+#include <asm/pgtable.h>
+#include <asm/tlbflush.h>
+#include <linux/swapops.h>
+#include <trace/page_alloc.h>
+#include <trace/filemap.h>
+#include <trace/swap.h>
+#include <trace/hugetlb.h>
+
+void probe_wait_on_page_start(struct page *page, int bit_nr)
+{
+	trace_mark_tp(mm, wait_on_page_start, wait_on_page_start,
+		probe_wait_on_page_start, "pfn %lu bit_nr %d",
+		page_to_pfn(page), bit_nr);
+}
+
+void probe_wait_on_page_end(struct page *page, int bit_nr)
+{
+	trace_mark_tp(mm, wait_on_page_end, wait_on_page_end,
+		probe_wait_on_page_end, "pfn %lu bit_nr %d",
+		page_to_pfn(page), bit_nr);
+}
+
+void probe_hugetlb_page_free(struct page *page)
+{
+	trace_mark_tp(mm, huge_page_free, hugetlb_page_free,
+		probe_hugetlb_page_free, "pfn %lu", page_to_pfn(page));
+}
+
+void probe_hugetlb_page_alloc(struct page *page)
+{
+	if (page)
+		trace_mark_tp(mm, huge_page_alloc, hugetlb_page_alloc,
+			probe_hugetlb_page_alloc, "pfn %lu", page_to_pfn(page));
+}
+
+/* mm_page_free specialized tracepoint probe */
+
+void probe_page_free(struct page *page, unsigned int order);
+
+DEFINE_MARKER_TP(mm, page_free, page_free, probe_page_free,
+	"pfn %lu order %u");
+
+notrace void probe_page_free(struct page *page, unsigned int order)
+{
+	struct marker *marker;
+	struct serialize_long_int data;
+
+	data.f1 = page_to_pfn(page);
+	data.f2 = order;
+
+	marker = &GET_MARKER(mm, page_free);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+/* mm_page_alloc specialized tracepoint probe */
+
+void probe_page_alloc(struct page *page, unsigned int order);
+
+DEFINE_MARKER_TP(mm, page_alloc, page_alloc, probe_page_alloc,
+	"pfn %lu order %u");
+
+notrace void probe_page_alloc(struct page *page, unsigned int order)
+{
+	struct marker *marker;
+	struct serialize_long_int data;
+
+	if (unlikely(!page))
+		return;
+
+	data.f1 = page_to_pfn(page);
+	data.f2 = order;
+
+	marker = &GET_MARKER(mm, page_alloc);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+#ifdef CONFIG_SWAP
+void probe_swap_in(struct page *page, swp_entry_t entry)
+{
+	trace_mark_tp(mm, swap_in, swap_in, probe_swap_in,
+		"pfn %lu filp %p offset %lu",
+		page_to_pfn(page),
+		get_swap_info_struct(swp_type(entry))->swap_file,
+		swp_offset(entry));
+}
+
+void probe_swap_out(struct page *page)
+{
+	trace_mark_tp(mm, swap_out, swap_out, probe_swap_out,
+		"pfn %lu filp %p offset %lu",
+		page_to_pfn(page),
+		get_swap_info_struct(swp_type(
+			page_swp_entry(page)))->swap_file,
+		swp_offset(page_swp_entry(page)));
+}
+
+void probe_swap_file_close(struct file *file)
+{
+	trace_mark_tp(mm, swap_file_close, swap_file_close,
+		probe_swap_file_close, "filp %p", file);
+}
+
+void probe_swap_file_open(struct file *file, char *filename)
+{
+	trace_mark_tp(mm, swap_file_open, swap_file_open,
+		probe_swap_file_open, "filp %p filename %s",
+		file, filename);
+}
+#endif
+
+void probe_add_to_page_cache(struct address_space *mapping, pgoff_t offset)
+{
+	trace_mark_tp(mm, add_to_page_cache, add_to_page_cache,
+		probe_add_to_page_cache,
+		"inode %lu sdev %u",
+		mapping->host->i_ino, mapping->host->i_sb->s_dev);
+}
+
+void probe_remove_from_page_cache(struct address_space *mapping)
+{
+	trace_mark_tp(mm, remove_from_page_cache, remove_from_page_cache,
+		probe_remove_from_page_cache,
+		"inode %lu sdev %u",
+		mapping->host->i_ino, mapping->host->i_sb->s_dev);
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("MM Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/net-extended-trace.c b/stblinux-2.6.31/ltt/probes/net-extended-trace.c
new file mode 100644
index 0000000..074bfec
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/net-extended-trace.c
@@ -0,0 +1,145 @@
+/*
+ * ltt/probes/net-extended-trace.c
+ *
+ * Net tracepoint extended probes.
+ *
+ * These probes record many header fields from TCP and UDP messages. Here are
+ * the consequences of this:
+ * 1) it allows analyzing network traffic to provide some pcap-like
+ *    functionality within LTTng
+ * 2) it allows offline synchronization of a group of concurrent traces
+ *    recorded on different nodes
+ * 3) it increases tracing overhead
+ *
+ * You can leave out these probes or not activate them if you are not
+ * especially interested in the details of network traffic and do not wish to
+ * synchronize distributed traces.
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/in_route.h>
+#include <linux/ip.h>
+#include <linux/ltt-type-serializer.h>
+#include <linux/module.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+#include <net/route.h>
+#include <trace/net.h>
+
+void probe_net_dev_xmit_extended(struct sk_buff *skb);
+
+DEFINE_MARKER_TP(net, dev_xmit_extended, net_dev_xmit,
+	probe_net_dev_xmit_extended, "skb 0x%lX network_protocol #n2u%hu "
+	"transport_protocol #1u%u saddr #n4u%lu daddr #n4u%lu "
+	"tot_len #n2u%hu ihl #1u%u source #n2u%hu dest #n2u%hu seq #n4u%lu "
+	"ack_seq #n4u%lu doff #1u%u ack #1u%u rst #1u%u syn #1u%u fin #1u%u");
+
+notrace void probe_net_dev_xmit_extended(struct sk_buff *skb)
+{
+	struct marker *marker;
+	struct serialize_l214421224411111 data;
+	struct iphdr *iph = ip_hdr(skb);
+	struct tcphdr *th = tcp_hdr(skb);
+
+	data.f1 = (unsigned long)skb;
+	data.f2 = skb->protocol;
+
+	if (ntohs(skb->protocol) == ETH_P_IP) {
+		data.f3 = ip_hdr(skb)->protocol;
+		data.f4 = iph->saddr;
+		data.f5 = iph->daddr;
+		data.f6 = iph->tot_len;
+		data.f7 = iph->ihl;
+
+		if (data.f3 == IPPROTO_TCP) {
+			data.f8 = th->source;
+			data.f9 = th->dest;
+			data.f10 = th->seq;
+			data.f11 = th->ack_seq;
+			data.f12 = th->doff;
+			data.f13 = th->ack;
+			data.f14 = th->rst;
+			data.f15 = th->syn;
+			data.f16 = th->fin;
+		}
+	}
+
+	marker = &GET_MARKER(net, dev_xmit_extended);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+void probe_tcpv4_rcv_extended(struct sk_buff *skb);
+
+DEFINE_MARKER_TP(net, tcpv4_rcv_extended, net_tcpv4_rcv,
+	probe_tcpv4_rcv_extended, "skb 0x%lX saddr #n4u%lu daddr #n4u%lu "
+	"tot_len #n2u%hu ihl #1u%u source #n2u%hu dest #n2u%hu seq #n4u%lu "
+	"ack_seq #n4u%lu doff #1u%u ack #1u%u rst #1u%u syn #1u%u fin #1u%u");
+
+notrace void probe_tcpv4_rcv_extended(struct sk_buff *skb)
+{
+	struct marker *marker;
+	struct serialize_l4421224411111 data;
+	struct iphdr *iph = ip_hdr(skb);
+	struct tcphdr *th = tcp_hdr(skb);
+
+	data.f1 = (unsigned long)skb;
+	data.f2 = iph->saddr;
+	data.f3 = iph->daddr;
+	data.f4 = iph->tot_len;
+	data.f5 = iph->ihl;
+	data.f6 = th->source;
+	data.f7 = th->dest;
+	data.f8 = th->seq;
+	data.f9 = th->ack_seq;
+	data.f10 = th->doff;
+	data.f11 = th->ack;
+	data.f12 = th->rst;
+	data.f13 = th->syn;
+	data.f14 = th->fin;
+
+	marker = &GET_MARKER(net, tcpv4_rcv_extended);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+void probe_udpv4_rcv_extended(struct sk_buff *skb);
+
+DEFINE_MARKER_TP(net, udpv4_rcv_extended, net_udpv4_rcv,
+	probe_udpv4_rcv_extended, "skb 0x%lX saddr #n4u%lu daddr #n4u%lu "
+	"unicast #1u%u ulen #n2u%hu source #n2u%hu dest #n2u%hu "
+	"data_start #8u%lx");
+
+notrace void probe_udpv4_rcv_extended(struct sk_buff *skb)
+{
+	struct marker *marker;
+	struct serialize_l4412228 data;
+	struct iphdr *iph = ip_hdr(skb);
+	struct rtable *rt = skb_rtable(skb);
+	struct udphdr *uh = udp_hdr(skb);
+
+	data.f1 = (unsigned long)skb;
+	data.f2 = iph->saddr;
+	data.f3 = iph->daddr;
+	data.f4 = rt->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST) ? 0 : 1;
+	data.f5 = uh->len;
+	data.f6 = uh->source;
+	data.f7 = uh->dest;
+	/* UDP header has not been pulled from skb->data, read the first 8
+	 * bytes of UDP data if they are not in a fragment*/
+	data.f8 = 0;
+	if (skb_headlen(skb) >= sizeof(struct udphdr) + 8)
+		data.f8 = *(unsigned long long *)(skb->data + sizeof(*uh));
+	else if (skb_headlen(skb) >= sizeof(struct udphdr))
+		memcpy(&data.f8, skb->data + sizeof(struct udphdr),
+			skb_headlen(skb) - sizeof(struct udphdr));
+
+	marker = &GET_MARKER(net, udpv4_rcv_extended);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(unsigned long long));
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Benjamin Poirier");
+MODULE_DESCRIPTION("Net Tracepoint Extended Probes");
diff --git a/stblinux-2.6.31/ltt/probes/net-trace.c b/stblinux-2.6.31/ltt/probes/net-trace.c
new file mode 100644
index 0000000..e049f2b
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/net-trace.c
@@ -0,0 +1,405 @@
+/*
+ * ltt/probes/net-trace.c
+ *
+ * Net tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/ltt-type-serializer.h>
+#include <linux/string.h>
+#include <trace/net.h>
+#include <trace/ipv4.h>
+#include <trace/ipv6.h>
+#include <trace/socket.h>
+
+void probe_net_dev_xmit(struct sk_buff *skb);
+
+DEFINE_MARKER_TP(net, dev_xmit, net_dev_xmit, probe_net_dev_xmit,
+	"skb %p protocol #n2u%hu");
+
+notrace void probe_net_dev_xmit(struct sk_buff *skb)
+{
+	struct marker *marker;
+	struct serialize_long_short data;
+
+	data.f1 = (unsigned long)skb;
+	data.f2 = skb->protocol;
+
+	marker = &GET_MARKER(net, dev_xmit);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+void probe_net_dev_receive(struct sk_buff *skb);
+
+DEFINE_MARKER_TP(net, dev_receive, net_dev_receive, probe_net_dev_receive,
+	"skb %p protocol #n2u%hu");
+
+notrace void probe_net_dev_receive(struct sk_buff *skb)
+{
+	struct marker *marker;
+	struct serialize_long_short data;
+
+	data.f1 = (unsigned long)skb;
+	data.f2 = skb->protocol;
+
+	marker = &GET_MARKER(net, dev_receive);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+void probe_ipv4_addr_add(struct in_ifaddr *ifa)
+{
+	trace_mark_tp(netif_state, insert_ifa_ipv4, ipv4_addr_add,
+		probe_ipv4_addr_add, "label %s address #4u%u",
+		ifa->ifa_label, (unsigned int)ifa->ifa_address);
+}
+
+void probe_ipv4_addr_del(struct in_ifaddr *ifa)
+{
+	trace_mark_tp(netif_state, del_ifa_ipv4, ipv4_addr_del,
+		probe_ipv4_addr_del, "label %s address #4u%u",
+		ifa->ifa_label, (unsigned int)ifa->ifa_address);
+}
+
+void probe_ipv6_addr_add(struct inet6_ifaddr *ifa)
+{
+	__u8 *addr = ifa->addr.s6_addr;
+
+	trace_mark_tp(netif_state, insert_ifa_ipv6, ipv6_addr_add,
+		probe_ipv6_addr_add,
+		"label %s "
+		"a15 #1x%c a14 #1x%c a13 #1x%c a12 #1x%c "
+		"a11 #1x%c a10 #1x%c a9 #1x%c a8 #1x%c "
+		"a7 #1x%c a6 #1x%c a5 #1x%c a4 #1x%c "
+		"a3 #1x%c a2 #1x%c a1 #1x%c a0 #1x%c",
+		ifa->idev->dev->name,
+		addr[15], addr[14], addr[13], addr[12],
+		addr[11], addr[10], addr[9], addr[8],
+		addr[7], addr[6], addr[5], addr[4],
+		addr[3], addr[2], addr[1], addr[0]);
+}
+
+void probe_ipv6_addr_del(struct inet6_ifaddr *ifa)
+{
+	__u8 *addr = ifa->addr.s6_addr;
+
+	trace_mark_tp(netif_state, insert_ifa_ipv6, ipv6_addr_del,
+		probe_ipv6_addr_del,
+		"label %s "
+		"a15 #1x%c a14 #1x%c a13 #1x%c a12 #1x%c "
+		"a11 #1x%c a10 #1x%c a9 #1x%c a8 #1x%c "
+		"a7 #1x%c a6 #1x%c a5 #1x%c a4 #1x%c "
+		"a3 #1x%c a2 #1x%c a1 #1x%c a0 #1x%c",
+		ifa->idev->dev->name,
+		addr[15], addr[14], addr[13], addr[12],
+		addr[11], addr[10], addr[9], addr[8],
+		addr[7], addr[6], addr[5], addr[4],
+		addr[3], addr[2], addr[1], addr[0]);
+}
+
+void probe_socket_create(int family, int type, int protocol,
+	struct socket *sock, int ret)
+{
+	trace_mark_tp(net, socket_create, socket_create, probe_socket_create,
+		"family %d type %d protocol %d sock %p ret %d",
+		family, type, protocol, sock, ret);
+}
+
+void probe_socket_bind(int fd, struct sockaddr __user *umyaddr, int addrlen,
+	int ret)
+{
+	trace_mark_tp(net, socket_bind, socket_bind, probe_socket_bind,
+		"fd %d umyaddr %p addrlen %d ret %d",
+		fd, umyaddr, addrlen, ret);
+}
+
+void probe_socket_connect(int fd, struct sockaddr __user *uservaddr,
+	int addrlen, int ret)
+{
+	trace_mark_tp(net, socket_connect, socket_connect, probe_socket_connect,
+		"fd %d uservaddr %p addrlen %d ret %d",
+		fd, uservaddr, addrlen, ret);
+}
+
+void probe_socket_listen(int fd, int backlog, int ret)
+{
+	trace_mark_tp(net, socket_listen, socket_listen, probe_socket_listen,
+		"fd %d backlog %d ret %d",
+		fd, backlog, ret);
+}
+
+void probe_socket_accept(int fd, struct sockaddr __user *upeer_sockaddr,
+	int __user *upeer_addrlen, int flags, int ret)
+{
+	trace_mark_tp(net, socket_accept, socket_accept, probe_socket_accept,
+		"fd %d upeer_sockaddr %p upeer_addrlen %p flags %d ret %d",
+		fd, upeer_sockaddr, upeer_addrlen, flags, ret);
+}
+
+void probe_socket_getsockname(int fd, struct sockaddr __user *usockaddr,
+	int __user *usockaddr_len, int ret)
+{
+	trace_mark_tp(net, socket_getsockname, socket_getsockname,
+		probe_socket_getsockname,
+		"fd %d usockaddr %p usockaddr_len %p ret %d",
+		fd, usockaddr, usockaddr_len, ret);
+}
+
+void probe_socket_getpeername(int fd, struct sockaddr __user *usockaddr,
+	int __user *usockaddr_len, int ret)
+{
+	trace_mark_tp(net, socket_getpeername, socket_getpeername,
+		probe_socket_getpeername,
+		"fd %d usockaddr %p usockaddr_len %p ret %d",
+		fd, usockaddr, usockaddr_len, ret);
+}
+
+void probe_socket_socketpair(int family, int type, int protocol,
+	int __user *usockvec, int ret)
+{
+	trace_mark_tp(net, socket_socketpair, socket_socketpair,
+		probe_socket_socketpair,
+		"family %d type %d protocol %d usockvec %p ret %d",
+		family, type, protocol, usockvec, ret);
+}
+
+void probe_socket_sendmsg(struct socket *sock, struct msghdr *msg, size_t size,
+	int ret);
+
+DEFINE_MARKER_TP(net, socket_sendmsg, net_socket_sendmsg,
+	probe_socket_sendmsg,
+	"sock %p msg %p size %zu ret %d");
+
+notrace void probe_socket_sendmsg(struct socket *sock, struct msghdr *msg,
+	size_t size, int ret)
+{
+	struct marker *marker;
+	struct serialize_long_long_sizet_int data;
+
+	data.f1 = (unsigned long)sock;
+	data.f2 = (unsigned long)msg;
+	data.f3 = size;
+	data.f4 = ret;
+
+	marker = &GET_MARKER(net, socket_sendmsg);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(size_t));
+}
+
+void probe_socket_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,
+	int flags, int ret);
+
+DEFINE_MARKER_TP(net, socket_recvmsg, net_socket_recvmsg,
+	probe_socket_recvmsg,
+	"sock %p msg %p size %zu flags %d ret %d");
+
+notrace void probe_socket_recvmsg(struct socket *sock, struct msghdr *msg,
+	size_t size, int flags, int ret)
+{
+	struct marker *marker;
+	struct serialize_long_long_sizet_int_int data;
+
+	data.f1 = (unsigned long)sock;
+	data.f2 = (unsigned long)msg;
+	data.f3 = size;
+	data.f4 = flags;
+	data.f5 = ret;
+
+	marker = &GET_MARKER(net, socket_recvmsg);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(size_t));
+}
+
+void probe_socket_setsockopt(int fd, int level, int optname,
+	char __user *optval, int optlen, int ret)
+{
+	trace_mark_tp(net, socket_setsockopt, socket_setsockopt,
+		probe_socket_setsockopt,
+		"fd %d level %d optname %d optval %p optlen %d ret %d",
+		fd, level, optname, optval, optlen, ret);
+}
+
+void probe_socket_getsockopt(int fd, int level, int optname,
+	char __user *optval, int __user *optlen, int ret)
+{
+	trace_mark_tp(net, socket_getsockopt, socket_getsockopt,
+		probe_socket_getsockopt,
+		"fd %d level %d optname %d optval %p optlen %p ret %d",
+		fd, level, optname, optval, optlen, ret);
+}
+
+void probe_socket_shutdown(int fd, int how, int ret)
+{
+	trace_mark_tp(net, socket_shutdown, socket_shutdown,
+		probe_socket_shutdown,
+		"fd %d how %d ret %d",
+		fd, how, ret);
+}
+
+void probe_socket_call(int call, unsigned long a0)
+{
+	trace_mark_tp(net, socket_call, socket_call, probe_socket_call,
+		"call %d a0 %lu", call, a0);
+}
+
+void probe_tcpv4_rcv(struct sk_buff *skb);
+
+DEFINE_MARKER_TP(net, tcpv4_rcv, net_tcpv4_rcv, probe_tcpv4_rcv,
+	"skb %p");
+
+notrace void probe_tcpv4_rcv(struct sk_buff *skb)
+{
+	struct marker *marker;
+
+	marker = &GET_MARKER(net, tcpv4_rcv);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&skb, sizeof(skb), sizeof(skb));
+}
+
+void probe_udpv4_rcv(struct sk_buff *skb);
+
+DEFINE_MARKER_TP(net, udpv4_rcv, net_udpv4_rcv, probe_udpv4_rcv,
+	"skb %p");
+
+notrace void probe_udpv4_rcv(struct sk_buff *skb)
+{
+	struct marker *marker;
+
+	marker = &GET_MARKER(net, udpv4_rcv);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&skb, sizeof(skb), sizeof(skb));
+}
+
+#ifdef CONFIG_NETPOLL
+void probe_net_napi_schedule(struct napi_struct *n);
+
+DEFINE_MARKER_TP(net, napi_schedule, net_napi_schedule,
+	probe_net_napi_schedule,
+	"napi_struct %p name %s");
+
+notrace void probe_net_napi_schedule(struct napi_struct *n)
+{
+	struct marker *marker;
+	struct serialize_long_ifname data;
+	size_t data_len = 0;
+
+	data.f1 = (unsigned long)n;
+	data_len += sizeof(data.f1);
+	/* No need to align for strings */
+	strcpy(data.f2, n->dev ? n->dev->name : "<unk>");
+	data_len += strlen(data.f2) + 1;
+
+	marker = &GET_MARKER(net, napi_schedule);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, data_len, sizeof(long));
+}
+
+void probe_net_napi_poll(struct napi_struct *n);
+
+DEFINE_MARKER_TP(net, napi_poll, net_napi_poll,
+	probe_net_napi_poll,
+	"napi_struct %p name %s");
+
+notrace void probe_net_napi_poll(struct napi_struct *n)
+{
+	struct marker *marker;
+	struct serialize_long_ifname data;
+	size_t data_len = 0;
+
+	data.f1 = (unsigned long)n;
+	data_len += sizeof(data.f1);
+	/* No need to align for strings */
+	strcpy(data.f2, n->dev ? n->dev->name : "<unk>");
+	data_len += strlen(data.f2) + 1;
+
+	marker = &GET_MARKER(net, napi_poll);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, data_len, sizeof(long));
+}
+
+void probe_net_napi_complete(struct napi_struct *n);
+
+DEFINE_MARKER_TP(net, napi_complete, net_napi_complete,
+	probe_net_napi_complete,
+	"napi_struct %p name %s");
+
+notrace void probe_net_napi_complete(struct napi_struct *n)
+{
+	struct marker *marker;
+	struct serialize_long_ifname data;
+	size_t data_len = 0;
+
+	data.f1 = (unsigned long)n;
+	data_len += sizeof(data.f1);
+	/* No need to align for strings */
+	strcpy(data.f2, n->dev ? n->dev->name : "<unk>");
+	data_len += strlen(data.f2) + 1;
+
+	marker = &GET_MARKER(net, napi_complete);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, data_len, sizeof(long));
+}
+#else /* !CONFIG_NETPOLL */
+void probe_net_napi_schedule(struct napi_struct *n);
+
+DEFINE_MARKER_TP(net, napi_schedule, net_napi_schedule,
+	probe_net_napi_schedule,
+	"napi_struct %p");
+
+notrace void probe_net_napi_schedule(struct napi_struct *n)
+{
+	struct marker *marker;
+	unsigned long data;
+
+	data = (unsigned long)n;
+
+	marker = &GET_MARKER(net, napi_schedule);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, sizeof(data), sizeof(data));
+}
+
+void probe_net_napi_poll(struct napi_struct *n);
+
+DEFINE_MARKER_TP(net, napi_poll, net_napi_poll,
+	probe_net_napi_poll,
+	"napi_struct %p");
+
+notrace void probe_net_napi_poll(struct napi_struct *n)
+{
+	struct marker *marker;
+	unsigned long data;
+
+	data = (unsigned long)n;
+
+	marker = &GET_MARKER(net, napi_poll);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, sizeof(data), sizeof(data));
+}
+
+void probe_net_napi_complete(struct napi_struct *n);
+
+DEFINE_MARKER_TP(net, napi_complete, net_napi_complete,
+	probe_net_napi_complete,
+	"napi_struct %p");
+
+notrace void probe_net_napi_complete(struct napi_struct *n)
+{
+	struct marker *marker;
+	unsigned long data;
+
+	data = (unsigned long)n;
+
+	marker = &GET_MARKER(net, napi_complete);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, sizeof(data), sizeof(data));
+}
+#endif
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Net Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/pm-trace.c b/stblinux-2.6.31/ltt/probes/pm-trace.c
new file mode 100644
index 0000000..927a95c
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/pm-trace.c
@@ -0,0 +1,43 @@
+/*
+ * ltt/probes/pm-trace.c
+ *
+ * Power Management tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <trace/pm.h>
+
+void probe_pm_idle_entry(void)
+{
+	trace_mark_tp(pm, idle_entry, pm_idle_entry,
+		probe_pm_idle_entry, "irqstate #1%d",
+		irqs_disabled());
+}
+
+void probe_pm_idle_exit(void)
+{
+	trace_mark_tp(pm, idle_exit, pm_idle_exit,
+		probe_pm_idle_exit, "irqstate #1%d",
+		irqs_disabled());
+}
+
+void probe_pm_suspend_entry(void)
+{
+	trace_mark_tp(pm, suspend_entry, pm_suspend_entry,
+		probe_pm_suspend_entry, "irqstate #1%d",
+		irqs_disabled());
+}
+
+void probe_pm_suspend_exit(void)
+{
+	trace_mark_tp(pm, suspend_exit, pm_suspend_exit,
+		probe_pm_suspend_exit, "irqstate #1%d",
+		irqs_disabled());
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Power Management Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/rcu-trace.c b/stblinux-2.6.31/ltt/probes/rcu-trace.c
new file mode 100644
index 0000000..97ba8e1
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/rcu-trace.c
@@ -0,0 +1,78 @@
+/*
+ * ltt/probes/rcu-trace.c
+ *
+ * RCU tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <trace/rcu.h>
+
+#ifdef CONFIG_CLASSIC_RCU
+void probe_rcu_classic_callback(struct rcu_head *head)
+{
+	trace_mark_tp(rcu, classic_callback, rcu_classic_callback,
+		probe_rcu_classic_callback, "func %p", head->func);
+}
+
+void probe_rcu_classic_call_rcu(struct rcu_head *head, unsigned long ip)
+{
+	trace_mark_tp(rcu, classic_call_rcu, rcu_classic_call_rcu,
+		probe_rcu_classic_call_rcu, "func %p ip 0x%lX", head->func, ip);
+}
+
+void probe_rcu_classic_call_rcu_bh(struct rcu_head *head, unsigned long ip)
+{
+	trace_mark_tp(rcu, classic_call_rcu_bh, rcu_classic_call_rcu_bh,
+		probe_rcu_classic_call_rcu_bh, "func %p ip 0x%lX",
+		head->func, ip);
+}
+#endif
+
+#ifdef CONFIG_PREEMPT_RCU
+void probe_rcu_preempt_callback(struct rcu_head *head)
+{
+	trace_mark_tp(rcu, preempt_callback, rcu_preempt_callback,
+		probe_rcu_preempt_callback, "func %p", head->func);
+}
+
+void probe_rcu_preempt_call_rcu(struct rcu_head *head, unsigned long ip)
+{
+	trace_mark_tp(rcu, preempt_call_rcu, rcu_preempt_call_rcu,
+		probe_rcu_preempt_call_rcu, "func %p ip 0x%lX", head->func, ip);
+}
+
+void probe_rcu_preempt_call_rcu_sched(struct rcu_head *head, unsigned long ip)
+{
+	trace_mark_tp(rcu, preempt_call_rcu_sched, rcu_preempt_call_rcu_sched,
+		probe_rcu_preempt_call_rcu_sched, "func %p ip 0x%lX",
+		head->func, ip);
+}
+#endif
+
+#ifdef CONFIG_TREE_RCU
+void probe_rcu_tree_callback(struct rcu_head *head)
+{
+	trace_mark_tp(rcu, tree_callback, rcu_tree_callback,
+		probe_rcu_tree_callback, "func %p", head->func);
+}
+
+void probe_rcu_tree_call_rcu(struct rcu_head *head, unsigned long ip)
+{
+	trace_mark_tp(rcu, tree_call_rcu, rcu_tree_call_rcu,
+		probe_rcu_tree_call_rcu, "func %p ip 0x%lX", head->func, ip);
+}
+
+void probe_rcu_tree_call_rcu_bh(struct rcu_head *head, unsigned long ip)
+{
+	trace_mark_tp(rcu, tree_call_rcu_bh, rcu_tree_call_rcu_bh,
+		probe_rcu_tree_call_rcu_bh, "func %p ip 0x%lX",
+		head->func, ip);
+}
+#endif
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("RCU Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/syscall-trace.c b/stblinux-2.6.31/ltt/probes/syscall-trace.c
new file mode 100644
index 0000000..9cd42a3
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/syscall-trace.c
@@ -0,0 +1,53 @@
+/*
+ * ltt/probes/syscall-trace.c
+ *
+ * System call tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/ltt-type-serializer.h>
+#include <trace/syscall.h>
+
+
+/* kernel_syscall_entry specialized tracepoint probe */
+
+void probe_syscall_entry(struct pt_regs *regs, long id);
+
+DEFINE_MARKER_TP(kernel, syscall_entry, syscall_entry,
+	probe_syscall_entry, "ip #p%ld syscall_id #2u%u");
+
+notrace void probe_syscall_entry(struct pt_regs *regs, long id)
+{
+	struct marker *marker;
+	struct serialize_long_short data;
+
+	data.f1 = instruction_pointer(regs);
+	data.f2 = (unsigned short)id;
+
+	marker = &GET_MARKER(kernel, syscall_entry);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+/* kernel_syscall_exit specialized tracepoint probe */
+
+void probe_syscall_exit(long ret);
+
+DEFINE_MARKER_TP(kernel, syscall_exit, syscall_exit,
+	probe_syscall_exit, "ret %ld");
+
+notrace void probe_syscall_exit(long ret)
+{
+	struct marker *marker;
+
+	marker = &GET_MARKER(kernel, syscall_exit);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&ret, sizeof(ret), sizeof(ret));
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("syscall Tracepoint Probes");
diff --git a/stblinux-2.6.31/ltt/probes/trap-trace.c b/stblinux-2.6.31/ltt/probes/trap-trace.c
new file mode 100644
index 0000000..37cb6d8
--- /dev/null
+++ b/stblinux-2.6.31/ltt/probes/trap-trace.c
@@ -0,0 +1,55 @@
+/*
+ * ltt/probes/trap-trace.c
+ *
+ * Trap tracepoint probes.
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <trace/trap.h>
+#include <linux/ltt-type-serializer.h>
+
+/* kernel_trap_entry specialized tracepoint probe */
+
+void probe_trap_entry(struct pt_regs *regs, long id);
+
+DEFINE_MARKER_TP(kernel, trap_entry, trap_entry,
+	probe_trap_entry, "ip #p%ld trap_id #2u%u");
+
+notrace void probe_trap_entry(struct pt_regs *regs, long id)
+{
+	struct marker *marker;
+	struct serialize_long_short data;
+
+	if (likely(regs))
+		data.f1 = instruction_pointer(regs);
+	else
+		data.f1 = 0UL;
+	data.f2 = (unsigned short)id;
+
+	marker = &GET_MARKER(kernel, trap_entry);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		&data, serialize_sizeof(data), sizeof(long));
+}
+
+/* kernel_syscall_exit specialized tracepoint probe */
+
+void probe_trap_exit(void);
+
+DEFINE_MARKER_TP(kernel, trap_exit, trap_exit,
+	probe_trap_exit, MARK_NOARGS);
+
+notrace void probe_trap_exit(void)
+{
+	struct marker *marker;
+
+	marker = &GET_MARKER(kernel, trap_exit);
+	ltt_specialized_trace(marker, marker->single.probe_private,
+		NULL, 0, 0);
+}
+
+MODULE_LICENSE("GPL and additional rights");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("Trap Tracepoint Probes");
diff --git a/stblinux-2.6.31/mm/filemap.c b/stblinux-2.6.31/mm/filemap.c
index ccea3b6..b287306 100644
--- a/stblinux-2.6.31/mm/filemap.c
+++ b/stblinux-2.6.31/mm/filemap.c
@@ -34,6 +34,7 @@
 #include <linux/hardirq.h> /* for BUG_ON(!in_atomic()) only */
 #include <linux/memcontrol.h>
 #include <linux/mm_inline.h> /* for page_is_file_cache() */
+#include <trace/filemap.h>
 #include "internal.h"
 
 /*
@@ -43,6 +44,10 @@
 
 #include <asm/mman.h>
 
+DEFINE_TRACE(wait_on_page_start);
+DEFINE_TRACE(wait_on_page_end);
+DEFINE_TRACE(add_to_page_cache);
+DEFINE_TRACE(remove_from_page_cache);
 
 /*
  * Shared mappings implemented 30.11.1994. It's not fully working yet,
@@ -120,6 +125,7 @@ void __remove_from_page_cache(struct page *page)
 	page->mapping = NULL;
 	mapping->nrpages--;
 	__dec_zone_page_state(page, NR_FILE_PAGES);
+	trace_remove_from_page_cache(mapping);
 	BUG_ON(page_mapped(page));
 
 	/*
@@ -476,6 +482,7 @@ int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
 		if (likely(!error)) {
 			mapping->nrpages++;
 			__inc_zone_page_state(page, NR_FILE_PAGES);
+			trace_add_to_page_cache(mapping, offset);
 			spin_unlock_irq(&mapping->tree_lock);
 		} else {
 			page->mapping = NULL;
@@ -560,9 +567,11 @@ void wait_on_page_bit(struct page *page, int bit_nr)
 {
 	DEFINE_WAIT_BIT(wait, &page->flags, bit_nr);
 
+	trace_wait_on_page_start(page, bit_nr);
 	if (test_bit(bit_nr, &page->flags))
 		__wait_on_bit(page_waitqueue(page), &wait, sync_page,
 							TASK_UNINTERRUPTIBLE);
+	trace_wait_on_page_end(page, bit_nr);
 }
 EXPORT_SYMBOL(wait_on_page_bit);
 
diff --git a/stblinux-2.6.31/mm/hugetlb.c b/stblinux-2.6.31/mm/hugetlb.c
index cae0337..0e4a91e 100644
--- a/stblinux-2.6.31/mm/hugetlb.c
+++ b/stblinux-2.6.31/mm/hugetlb.c
@@ -18,6 +18,7 @@
 #include <linux/mutex.h>
 #include <linux/bootmem.h>
 #include <linux/sysfs.h>
+#include <trace/hugetlb.h>
 
 #include <asm/page.h>
 #include <asm/pgtable.h>
@@ -49,6 +50,14 @@ static unsigned long __initdata default_hstate_size;
  */
 static DEFINE_SPINLOCK(hugetlb_lock);
 
+DEFINE_TRACE(hugetlb_page_release);
+DEFINE_TRACE(hugetlb_page_grab);
+DEFINE_TRACE(hugetlb_buddy_pgalloc);
+DEFINE_TRACE(hugetlb_page_alloc);
+DEFINE_TRACE(hugetlb_page_free);
+DEFINE_TRACE(hugetlb_pages_reserve);
+DEFINE_TRACE(hugetlb_pages_unreserve);
+
 /*
  * Region tracking -- allows tracking of reservations and instantiated pages
  *                    across the pages in a mapping.
@@ -526,6 +535,7 @@ static void update_and_free_page(struct hstate *h, struct page *page)
 
 	VM_BUG_ON(h->order >= MAX_ORDER);
 
+	trace_hugetlb_page_release(page);
 	h->nr_huge_pages--;
 	h->nr_huge_pages_node[page_to_nid(page)]--;
 	for (i = 0; i < pages_per_huge_page(h); i++) {
@@ -560,6 +570,7 @@ static void free_huge_page(struct page *page)
 	int nid = page_to_nid(page);
 	struct address_space *mapping;
 
+	trace_hugetlb_page_free(page);
 	mapping = (struct address_space *) page_private(page);
 	set_page_private(page, 0);
 	BUG_ON(page_count(page));
@@ -620,8 +631,10 @@ static struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)
 {
 	struct page *page;
 
-	if (h->order >= MAX_ORDER)
-		return NULL;
+	if (h->order >= MAX_ORDER) {
+		page = NULL;
+		goto end;
+	}
 
 	page = alloc_pages_exact_node(nid,
 		htlb_alloc_mask|__GFP_COMP|__GFP_THISNODE|
@@ -630,11 +643,13 @@ static struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)
 	if (page) {
 		if (arch_prepare_hugepage(page)) {
 			__free_pages(page, huge_page_order(h));
-			return NULL;
+			page = NULL;
+			goto end;
 		}
 		prep_new_huge_page(h, page, nid);
 	}
-
+end:
+	trace_hugetlb_page_grab(page);
 	return page;
 }
 
@@ -718,7 +733,8 @@ static struct page *alloc_buddy_huge_page(struct hstate *h,
 	spin_lock(&hugetlb_lock);
 	if (h->surplus_huge_pages >= h->nr_overcommit_huge_pages) {
 		spin_unlock(&hugetlb_lock);
-		return NULL;
+		page = NULL;
+		goto end;
 	} else {
 		h->nr_huge_pages++;
 		h->surplus_huge_pages++;
@@ -756,7 +772,8 @@ static struct page *alloc_buddy_huge_page(struct hstate *h,
 		__count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);
 	}
 	spin_unlock(&hugetlb_lock);
-
+end:
+	trace_hugetlb_buddy_pgalloc(page);
 	return page;
 }
 
@@ -995,6 +1012,7 @@ static struct page *alloc_huge_page(struct vm_area_struct *vma,
 
 	vma_commit_reservation(h, vma, addr);
 
+	trace_hugetlb_page_alloc(page);
 	return page;
 }
 
@@ -2301,7 +2319,8 @@ int hugetlb_reserve_pages(struct inode *inode,
 					struct vm_area_struct *vma,
 					int acctflag)
 {
-	long ret, chg;
+	int ret = 0;
+	long chg;
 	struct hstate *h = hstate_inode(inode);
 
 	/*
@@ -2310,7 +2329,7 @@ int hugetlb_reserve_pages(struct inode *inode,
 	 * and filesystem quota without using reserves
 	 */
 	if (acctflag & VM_NORESERVE)
-		return 0;
+		goto end;
 
 	/*
 	 * Shared mappings base their reservation on the number of pages that
@@ -2322,8 +2341,10 @@ int hugetlb_reserve_pages(struct inode *inode,
 		chg = region_chg(&inode->i_mapping->private_list, from, to);
 	else {
 		struct resv_map *resv_map = resv_map_alloc();
-		if (!resv_map)
-			return -ENOMEM;
+		if (!resv_map) {
+			ret = -ENOMEM;
+			goto end;
+		}
 
 		chg = to - from;
 
@@ -2331,12 +2352,16 @@ int hugetlb_reserve_pages(struct inode *inode,
 		set_vma_resv_flags(vma, HPAGE_RESV_OWNER);
 	}
 
-	if (chg < 0)
-		return chg;
+	if (chg < 0) {
+		ret = chg;
+		goto end;
+	}
 
 	/* There must be enough filesystem quota for the mapping */
-	if (hugetlb_get_quota(inode->i_mapping, chg))
-		return -ENOSPC;
+	if (hugetlb_get_quota(inode->i_mapping, chg)) {
+		ret = -ENOSPC;
+		goto end;
+	}
 
 	/*
 	 * Check enough hugepages are available for the reservation.
@@ -2345,7 +2370,7 @@ int hugetlb_reserve_pages(struct inode *inode,
 	ret = hugetlb_acct_memory(h, chg);
 	if (ret < 0) {
 		hugetlb_put_quota(inode->i_mapping, chg);
-		return ret;
+		goto end;
 	}
 
 	/*
@@ -2361,14 +2386,18 @@ int hugetlb_reserve_pages(struct inode *inode,
 	 */
 	if (!vma || vma->vm_flags & VM_MAYSHARE)
 		region_add(&inode->i_mapping->private_list, from, to);
-	return 0;
+end:
+	trace_hugetlb_pages_reserve(inode, from, to, ret);
+	return ret;
 }
 
 void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed)
 {
 	struct hstate *h = hstate_inode(inode);
-	long chg = region_truncate(&inode->i_mapping->private_list, offset);
+	long chg;
 
+	trace_hugetlb_pages_unreserve(inode, offset, freed);
+	chg = region_truncate(&inode->i_mapping->private_list, offset);
 	spin_lock(&inode->i_lock);
 	inode->i_blocks -= (blocks_per_huge_page(h) * freed);
 	spin_unlock(&inode->i_lock);
diff --git a/stblinux-2.6.31/mm/memory.c b/stblinux-2.6.31/mm/memory.c
index d8a39d1..cab86d8 100644
--- a/stblinux-2.6.31/mm/memory.c
+++ b/stblinux-2.6.31/mm/memory.c
@@ -55,6 +55,8 @@
 #include <linux/kallsyms.h>
 #include <linux/swapops.h>
 #include <linux/elf.h>
+#include <trace/swap.h>
+#include <trace/fault.h>
 
 #include <asm/pgalloc.h>
 #include <asm/uaccess.h>
@@ -68,6 +70,10 @@
 
 #include "internal.h"
 
+DEFINE_TRACE(swap_in);
+DEFINE_TRACE(page_fault_get_user_entry);
+DEFINE_TRACE(page_fault_get_user_exit);
+
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 /* use the per-pgdat data instead for discontigmem - mbligh */
 unsigned long max_mapnr;
@@ -1326,11 +1332,15 @@ int __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 
 			cond_resched();
 			while (!(page = follow_page(vma, start, foll_flags))) {
-				int ret;
+				int ret, write_access;
 
+				write_access = foll_flags & FOLL_WRITE;
+				trace_page_fault_get_user_entry(mm,
+					vma, start, write_access);
 				ret = handle_mm_fault(mm, vma, start,
-					(foll_flags & FOLL_WRITE) ?
+					write_access ?
 					FAULT_FLAG_WRITE : 0);
+				trace_page_fault_get_user_exit(ret);
 
 				if (ret & VM_FAULT_ERROR) {
 					if (ret & VM_FAULT_OOM)
@@ -2552,6 +2562,7 @@ static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		/* Had to read the page from swap area: Major fault */
 		ret = VM_FAULT_MAJOR;
 		count_vm_event(PGMAJFAULT);
+		trace_swap_in(page, entry);
 	}
 
 	lock_page(page);
diff --git a/stblinux-2.6.31/mm/page_alloc.c b/stblinux-2.6.31/mm/page_alloc.c
index d04c842..c7bcab6 100644
--- a/stblinux-2.6.31/mm/page_alloc.c
+++ b/stblinux-2.6.31/mm/page_alloc.c
@@ -48,6 +48,7 @@
 #include <linux/page_cgroup.h>
 #include <linux/debugobjects.h>
 #include <linux/kmemleak.h>
+#include <trace/page_alloc.h>
 
 #include <asm/tlbflush.h>
 #include <asm/div64.h>
@@ -79,6 +80,9 @@ gfp_t gfp_allowed_mask __read_mostly = GFP_BOOT_MASK;
 int pageblock_order __read_mostly;
 #endif
 
+DEFINE_TRACE(page_alloc);
+DEFINE_TRACE(page_free);
+
 static void __free_pages_ok(struct page *page, unsigned int order);
 
 /*
@@ -561,6 +565,8 @@ static void __free_pages_ok(struct page *page, unsigned int order)
 
 	kmemcheck_free_shadow(page, order);
 
+	trace_page_free(page, order);
+
 	for (i = 0 ; i < (1 << order) ; ++i)
 		bad += free_pages_check(page + i);
 	if (bad)
@@ -1030,6 +1036,8 @@ static void free_hot_cold_page(struct page *page, int cold)
 
 	kmemcheck_free_shadow(page, 0);
 
+	trace_page_free(page, 0);
+
 	if (PageAnon(page))
 		page->mapping = NULL;
 	if (free_pages_check(page))
@@ -1858,6 +1866,7 @@ nopage:
 	}
 	return page;
 got_pg:
+	trace_page_alloc(page, order);
 	if (kmemcheck_enabled)
 		kmemcheck_pagealloc_alloc(page, order, gfp_mask);
 	return page;
diff --git a/stblinux-2.6.31/mm/page_io.c b/stblinux-2.6.31/mm/page_io.c
index c6f3e50..ba0a33e 100644
--- a/stblinux-2.6.31/mm/page_io.c
+++ b/stblinux-2.6.31/mm/page_io.c
@@ -17,8 +17,11 @@
 #include <linux/bio.h>
 #include <linux/swapops.h>
 #include <linux/writeback.h>
+#include <trace/swap.h>
 #include <asm/pgtable.h>
 
+DEFINE_TRACE(swap_out);
+
 static struct bio *get_swap_bio(gfp_t gfp_flags, pgoff_t index,
 				struct page *page, bio_end_io_t end_io)
 {
@@ -114,6 +117,7 @@ int swap_writepage(struct page *page, struct writeback_control *wbc)
 		rw |= (1 << BIO_RW_SYNCIO) | (1 << BIO_RW_UNPLUG);
 	count_vm_event(PSWPOUT);
 	set_page_writeback(page);
+	trace_swap_out(page);
 	unlock_page(page);
 	submit_bio(rw, bio);
 out:
diff --git a/stblinux-2.6.31/mm/swapfile.c b/stblinux-2.6.31/mm/swapfile.c
index b47ccd7..f89f13f 100644
--- a/stblinux-2.6.31/mm/swapfile.c
+++ b/stblinux-2.6.31/mm/swapfile.c
@@ -29,12 +29,16 @@
 #include <linux/capability.h>
 #include <linux/syscalls.h>
 #include <linux/memcontrol.h>
+#include <trace/swap.h>
 
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <linux/swapops.h>
 #include <linux/page_cgroup.h>
 
+DEFINE_TRACE(swap_file_open);
+DEFINE_TRACE(swap_file_close);
+
 static DEFINE_SPINLOCK(swap_lock);
 static unsigned int nr_swapfiles;
 long nr_swap_pages;
@@ -1622,6 +1626,7 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)
 	swap_map = p->swap_map;
 	p->swap_map = NULL;
 	p->flags = 0;
+	trace_swap_file_close(swap_file);
 	spin_unlock(&swap_lock);
 	mutex_unlock(&swapon_mutex);
 	vfree(swap_map);
@@ -2011,6 +2016,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)
 	} else {
 		swap_info[prev].next = p - swap_info;
 	}
+	trace_swap_file_open(swap_file, name);
 	spin_unlock(&swap_lock);
 	mutex_unlock(&swapon_mutex);
 	error = 0;
@@ -2161,6 +2167,7 @@ get_swap_info_struct(unsigned type)
 {
 	return &swap_info[type];
 }
+EXPORT_SYMBOL_GPL(get_swap_info_struct);
 
 /*
  * swap_lock prevents swap_map being freed. Don't grab an extra
@@ -2213,3 +2220,22 @@ int valid_swaphandles(swp_entry_t entry, unsigned long *offset)
 	*offset = ++toff;
 	return nr_pages? ++nr_pages: 0;
 }
+
+void ltt_dump_swap_files(void *call_data)
+{
+	int type;
+	struct swap_info_struct *p = NULL;
+
+	mutex_lock(&swapon_mutex);
+	for (type = swap_list.head; type >= 0; type = swap_info[type].next) {
+		p = swap_info + type;
+		if (!(p->flags & SWP_WRITEOK))
+			continue;
+		__trace_mark(0, swap_state, statedump_swap_files, call_data,
+			"filp %p vfsmount %p dname %s",
+			p->swap_file, p->swap_file->f_vfsmnt,
+			p->swap_file->f_dentry->d_name.name);
+	}
+	mutex_unlock(&swapon_mutex);
+}
+EXPORT_SYMBOL_GPL(ltt_dump_swap_files);
diff --git a/stblinux-2.6.31/mm/vmalloc.c b/stblinux-2.6.31/mm/vmalloc.c
index f603667..b8dd9d1 100644
--- a/stblinux-2.6.31/mm/vmalloc.c
+++ b/stblinux-2.6.31/mm/vmalloc.c
@@ -1760,7 +1760,7 @@ EXPORT_SYMBOL(remap_vmalloc_range);
 void  __attribute__((weak)) vmalloc_sync_all(void)
 {
 }
-
+EXPORT_SYMBOL_GPL(vmalloc_sync_all);
 
 static int f(pte_t *pte, pgtable_t table, unsigned long addr, void *data)
 {
diff --git a/stblinux-2.6.31/net/core/dev.c b/stblinux-2.6.31/net/core/dev.c
index f3e3319..f892dcb 100644
--- a/stblinux-2.6.31/net/core/dev.c
+++ b/stblinux-2.6.31/net/core/dev.c
@@ -127,6 +127,7 @@
 #include <linux/jhash.h>
 #include <linux/random.h>
 #include <trace/events/napi.h>
+#include <trace/net.h>
 
 #include "net-sysfs.h"
 
@@ -204,6 +205,13 @@ EXPORT_SYMBOL(dev_base_lock);
 #define NETDEV_HASHBITS	8
 #define NETDEV_HASHENTRIES (1 << NETDEV_HASHBITS)
 
+DEFINE_TRACE(net_dev_xmit);
+DEFINE_TRACE(net_dev_receive);
+DEFINE_TRACE(net_napi_schedule);
+DEFINE_TRACE(net_napi_poll);
+DEFINE_TRACE(net_napi_complete);
+EXPORT_TRACEPOINT_SYMBOL_GPL(net_napi_complete);
+
 static inline struct hlist_head *dev_name_hash(struct net *net, const char *name)
 {
 	unsigned hash = full_name_hash(name, strnlen(name, IFNAMSIZ));
@@ -1711,6 +1719,8 @@ int dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,
 		if (dev->priv_flags & IFF_XMIT_DST_RELEASE)
 			skb_dst_drop(skb);
 
+		trace_net_dev_xmit(skb);
+
 		rc = ops->ndo_start_xmit(skb, dev);
 		if (rc == 0)
 			txq_trans_update(txq);
@@ -1737,6 +1747,8 @@ gso:
 
 		skb->next = nskb->next;
 		nskb->next = NULL;
+
+		trace_net_dev_xmit(nskb);
 		rc = ops->ndo_start_xmit(nskb, dev);
 		if (unlikely(rc)) {
 			nskb->next = skb->next;
@@ -1972,6 +1984,8 @@ int netif_rx(struct sk_buff *skb)
 	if (!skb->tstamp.tv64)
 		net_timestamp(skb);
 
+	trace_net_dev_receive(skb);
+
 	/*
 	 * The code is rearranged so that the path is the most
 	 * short when CPU is congested, but is still operating.
@@ -2276,6 +2290,8 @@ int netif_receive_skb(struct sk_buff *skb)
 	if (netpoll_receive_skb(skb))
 		return NET_RX_DROP;
 
+	trace_net_dev_receive(skb);
+
 	if (!skb->iif)
 		skb->iif = skb->dev->ifindex;
 
@@ -2710,6 +2726,8 @@ void __napi_schedule(struct napi_struct *n)
 {
 	unsigned long flags;
 
+	trace_net_napi_schedule(n);
+
 	local_irq_save(flags);
 	list_add_tail(&n->poll_list, &__get_cpu_var(softnet_data).poll_list);
 	__raise_softirq_irqoff(NET_RX_SOFTIRQ);
@@ -2725,6 +2743,7 @@ void __napi_complete(struct napi_struct *n)
 	list_del(&n->poll_list);
 	smp_mb__before_clear_bit();
 	clear_bit(NAPI_STATE_SCHED, &n->state);
+	trace_net_napi_complete(n);
 }
 EXPORT_SYMBOL(__napi_complete);
 
@@ -2825,6 +2844,7 @@ static void net_rx_action(struct softirq_action *h)
 		 */
 		work = 0;
 		if (test_bit(NAPI_STATE_SCHED, &n->state)) {
+			trace_net_napi_poll(n);
 			work = n->poll(n, weight);
 			trace_napi_poll(n);
 		}
diff --git a/stblinux-2.6.31/net/ipv4/devinet.c b/stblinux-2.6.31/net/ipv4/devinet.c
index 3863c3a..49d9cdf 100644
--- a/stblinux-2.6.31/net/ipv4/devinet.c
+++ b/stblinux-2.6.31/net/ipv4/devinet.c
@@ -61,6 +61,7 @@
 #include <net/ip_fib.h>
 #include <net/rtnetlink.h>
 #include <net/net_namespace.h>
+#include <trace/ipv4.h>
 
 static struct ipv4_devconf ipv4_devconf = {
 	.data = {
@@ -91,6 +92,9 @@ static const struct nla_policy ifa_ipv4_policy[IFA_MAX+1] = {
 	[IFA_LABEL]     	= { .type = NLA_STRING, .len = IFNAMSIZ - 1 },
 };
 
+DEFINE_TRACE(ipv4_addr_add);
+DEFINE_TRACE(ipv4_addr_del);
+
 static void rtmsg_ifa(int event, struct in_ifaddr *, struct nlmsghdr *, u32);
 
 static BLOCKING_NOTIFIER_HEAD(inetaddr_chain);
@@ -250,6 +254,7 @@ static void __inet_del_ifa(struct in_device *in_dev, struct in_ifaddr **ifap,
 		struct in_ifaddr **ifap1 = &ifa1->ifa_next;
 
 		while ((ifa = *ifap1) != NULL) {
+			trace_ipv4_addr_del(ifa);
 			if (!(ifa->ifa_flags & IFA_F_SECONDARY) &&
 			    ifa1->ifa_scope <= ifa->ifa_scope)
 				last_prim = ifa;
@@ -356,6 +361,7 @@ static int __inet_insert_ifa(struct in_ifaddr *ifa, struct nlmsghdr *nlh,
 			}
 			ifa->ifa_flags |= IFA_F_SECONDARY;
 		}
+		trace_ipv4_addr_add(ifa);
 	}
 
 	if (!(ifa->ifa_flags & IFA_F_SECONDARY)) {
diff --git a/stblinux-2.6.31/net/ipv4/tcp_ipv4.c b/stblinux-2.6.31/net/ipv4/tcp_ipv4.c
index 6d88219..8d60b15 100644
--- a/stblinux-2.6.31/net/ipv4/tcp_ipv4.c
+++ b/stblinux-2.6.31/net/ipv4/tcp_ipv4.c
@@ -81,6 +81,10 @@
 #include <linux/crypto.h>
 #include <linux/scatterlist.h>
 
+#include <trace/net.h>
+
+DEFINE_TRACE(net_tcpv4_rcv);
+
 int sysctl_tcp_tw_reuse __read_mostly;
 int sysctl_tcp_low_latency __read_mostly;
 
@@ -1457,6 +1461,9 @@ static __sum16 tcp_v4_checksum_init(struct sk_buff *skb)
 int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
 {
 	struct sock *rsk;
+
+	trace_net_tcpv4_rcv(skb);
+
 #ifdef CONFIG_TCP_MD5SIG
 	/*
 	 * We really want to reject the packet as early as possible
diff --git a/stblinux-2.6.31/net/ipv4/udp.c b/stblinux-2.6.31/net/ipv4/udp.c
index 00d1f6d..7cccc22 100644
--- a/stblinux-2.6.31/net/ipv4/udp.c
+++ b/stblinux-2.6.31/net/ipv4/udp.c
@@ -104,6 +104,7 @@
 #include <net/route.h>
 #include <net/checksum.h>
 #include <net/xfrm.h>
+#include <trace/net.h>
 #include "udp_impl.h"
 
 struct udp_table udp_table;
@@ -122,6 +123,8 @@ EXPORT_SYMBOL(udp_memory_allocated);
 
 #define PORTS_PER_CHAIN (65536 / UDP_HTABLE_SIZE)
 
+DEFINE_TRACE(net_udpv4_rcv);
+
 static int udp_lib_lport_inuse(struct net *net, __u16 num,
 			       const struct udp_hslot *hslot,
 			       unsigned long *bitmap,
@@ -1292,6 +1295,8 @@ int __udp4_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,
 	if (udp4_csum_init(skb, uh, proto))
 		goto csum_error;
 
+	trace_net_udpv4_rcv(skb);
+
 	saddr = ip_hdr(skb)->saddr;
 	daddr = ip_hdr(skb)->daddr;
 
diff --git a/stblinux-2.6.31/net/ipv6/addrconf.c b/stblinux-2.6.31/net/ipv6/addrconf.c
index 43b3c9f..ef55352 100644
--- a/stblinux-2.6.31/net/ipv6/addrconf.c
+++ b/stblinux-2.6.31/net/ipv6/addrconf.c
@@ -86,6 +86,7 @@
 
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
+#include <trace/ipv6.h>
 
 /* Set to 3 to get tracing... */
 #define ACONF_DEBUG 2
@@ -99,6 +100,9 @@
 #define	INFINITY_LIFE_TIME	0xFFFFFFFF
 #define TIME_DELTA(a,b) ((unsigned long)((long)(a) - (long)(b)))
 
+DEFINE_TRACE(ipv6_addr_add);
+DEFINE_TRACE(ipv6_addr_del);
+
 #ifdef CONFIG_SYSCTL
 static void addrconf_sysctl_register(struct inet6_dev *idev);
 static void addrconf_sysctl_unregister(struct inet6_dev *idev);
@@ -662,6 +666,8 @@ ipv6_add_addr(struct inet6_dev *idev, const struct in6_addr *addr, int pfxlen,
 	/* For caller */
 	in6_ifa_hold(ifa);
 
+	trace_ipv6_addr_add(ifa);
+
 	/* Add to big hash table */
 	hash = ipv6_addr_hash(addr);
 
@@ -2175,6 +2181,7 @@ static int inet6_addr_del(struct net *net, int ifindex, struct in6_addr *pfx,
 			in6_ifa_hold(ifp);
 			read_unlock_bh(&idev->lock);
 
+			trace_ipv6_addr_del(ifp);
 			ipv6_del_addr(ifp);
 
 			/* If the last address is deleted administratively,
diff --git a/stblinux-2.6.31/net/socket.c b/stblinux-2.6.31/net/socket.c
index 6d47165..93ee6bd 100644
--- a/stblinux-2.6.31/net/socket.c
+++ b/stblinux-2.6.31/net/socket.c
@@ -95,6 +95,7 @@
 
 #include <net/sock.h>
 #include <linux/netfilter.h>
+#include <trace/socket.h>
 
 static int sock_no_open(struct inode *irrelevant, struct file *dontcare);
 static ssize_t sock_aio_read(struct kiocb *iocb, const struct iovec *iov,
@@ -155,6 +156,21 @@ static const struct net_proto_family *net_families[NPROTO] __read_mostly;
 
 static DEFINE_PER_CPU(int, sockets_in_use) = 0;
 
+DEFINE_TRACE(socket_create);
+DEFINE_TRACE(socket_bind);
+DEFINE_TRACE(socket_connect);
+DEFINE_TRACE(socket_listen);
+DEFINE_TRACE(socket_accept);
+DEFINE_TRACE(socket_getsockname);
+DEFINE_TRACE(socket_getpeername);
+DEFINE_TRACE(socket_socketpair);
+DEFINE_TRACE(socket_sendmsg);
+DEFINE_TRACE(socket_recvmsg);
+DEFINE_TRACE(socket_setsockopt);
+DEFINE_TRACE(socket_getsockopt);
+DEFINE_TRACE(socket_shutdown);
+DEFINE_TRACE(socket_call);
+
 /*
  * Support routines.
  * Move socket addresses back and forth across the kernel/user
@@ -570,7 +586,9 @@ static inline int __sock_sendmsg(struct kiocb *iocb, struct socket *sock,
 	if (err)
 		return err;
 
-	return sock->ops->sendmsg(iocb, sock, msg, size);
+	err = sock->ops->sendmsg(iocb, sock, msg, size);
+	trace_socket_sendmsg(sock, msg, size, err);
+	return err;
 }
 
 int sock_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)
@@ -684,7 +702,9 @@ static inline int __sock_recvmsg(struct kiocb *iocb, struct socket *sock,
 	if (err)
 		return err;
 
-	return sock->ops->recvmsg(iocb, sock, msg, size, flags);
+	err = sock->ops->recvmsg(iocb, sock, msg, size, flags);
+	trace_socket_recvmsg(sock, msg, size, flags, err);
+	return err;
 }
 
 int sock_recvmsg(struct socket *sock, struct msghdr *msg,
@@ -695,6 +715,7 @@ int sock_recvmsg(struct socket *sock, struct msghdr *msg,
 	int ret;
 
 	init_sync_kiocb(&iocb, NULL);
+
 	iocb.private = &siocb;
 	ret = __sock_recvmsg(&iocb, sock, msg, size, flags);
 	if (-EIOCBQUEUED == ret)
@@ -1276,8 +1297,10 @@ SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)
 	BUILD_BUG_ON(SOCK_NONBLOCK & SOCK_TYPE_MASK);
 
 	flags = type & ~SOCK_TYPE_MASK;
-	if (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))
-		return -EINVAL;
+	if (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK)) {
+		retval = -EINVAL;
+		goto out;
+	}
 	type &= SOCK_TYPE_MASK;
 
 	if (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))
@@ -1291,12 +1314,12 @@ SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)
 	if (retval < 0)
 		goto out_release;
 
-out:
-	/* It may be already another descriptor 8) Not kernel problem. */
-	return retval;
-
+	goto out;
 out_release:
 	sock_release(sock);
+out:
+	trace_socket_create(family, type, protocol, sock, retval);
+	/* It may be already another descriptor 8) Not kernel problem. */
 	return retval;
 }
 
@@ -1313,8 +1336,10 @@ SYSCALL_DEFINE4(socketpair, int, family, int, type, int, protocol,
 	int flags;
 
 	flags = type & ~SOCK_TYPE_MASK;
-	if (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))
-		return -EINVAL;
+	if (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK)) {
+		err = -EINVAL;
+		goto out;
+	}
 	type &= SOCK_TYPE_MASK;
 
 	if (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))
@@ -1373,17 +1398,18 @@ SYSCALL_DEFINE4(socketpair, int, family, int, type, int, protocol,
 	if (!err)
 		err = put_user(fd2, &usockvec[1]);
 	if (!err)
-		return 0;
+		goto out;
 
 	sys_close(fd2);
 	sys_close(fd1);
-	return err;
+	goto out;
 
 out_release_both:
 	sock_release(sock2);
 out_release_1:
 	sock_release(sock1);
 out:
+	trace_socket_socketpair(family, type, protocol, usockvec, err);
 	return err;
 
 out_fd2:
@@ -1425,6 +1451,7 @@ SYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)
 		}
 		fput_light(sock->file, fput_needed);
 	}
+	trace_socket_bind(fd, umyaddr, addrlen, err);
 	return err;
 }
 
@@ -1452,6 +1479,7 @@ SYSCALL_DEFINE2(listen, int, fd, int, backlog)
 
 		fput_light(sock->file, fput_needed);
 	}
+	trace_socket_listen(fd, backlog, err);
 	return err;
 }
 
@@ -1475,8 +1503,10 @@ SYSCALL_DEFINE4(accept4, int, fd, struct sockaddr __user *, upeer_sockaddr,
 	int err, len, newfd, fput_needed;
 	struct sockaddr_storage address;
 
-	if (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))
-		return -EINVAL;
+	if (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK)) {
+		err = -EINVAL;
+		goto out;
+	}
 
 	if (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))
 		flags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;
@@ -1537,6 +1567,7 @@ SYSCALL_DEFINE4(accept4, int, fd, struct sockaddr __user *, upeer_sockaddr,
 out_put:
 	fput_light(sock->file, fput_needed);
 out:
+	trace_socket_accept(fd, upeer_sockaddr, upeer_addrlen, flags, err);
 	return err;
 out_fd_simple:
 	sock_release(newsock);
@@ -1591,6 +1622,7 @@ SYSCALL_DEFINE3(connect, int, fd, struct sockaddr __user *, uservaddr,
 out_put:
 	fput_light(sock->file, fput_needed);
 out:
+	trace_socket_connect(fd, uservaddr, addrlen, err);
 	return err;
 }
 
@@ -1622,6 +1654,7 @@ SYSCALL_DEFINE3(getsockname, int, fd, struct sockaddr __user *, usockaddr,
 out_put:
 	fput_light(sock->file, fput_needed);
 out:
+	trace_socket_getsockname(fd, usockaddr, usockaddr_len, err);
 	return err;
 }
 
@@ -1642,7 +1675,7 @@ SYSCALL_DEFINE3(getpeername, int, fd, struct sockaddr __user *, usockaddr,
 		err = security_socket_getpeername(sock);
 		if (err) {
 			fput_light(sock->file, fput_needed);
-			return err;
+			goto out;
 		}
 
 		err =
@@ -1653,6 +1686,8 @@ SYSCALL_DEFINE3(getpeername, int, fd, struct sockaddr __user *, usockaddr,
 						usockaddr_len);
 		fput_light(sock->file, fput_needed);
 	}
+out:
+	trace_socket_getpeername(fd, usockaddr, usockaddr_len, err);
 	return err;
 }
 
@@ -1779,8 +1814,10 @@ SYSCALL_DEFINE5(setsockopt, int, fd, int, level, int, optname,
 	int err, fput_needed;
 	struct socket *sock;
 
-	if (optlen < 0)
-		return -EINVAL;
+	if (optlen < 0) {
+		err = -EINVAL;
+		goto out;
+	}
 
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
 	if (sock != NULL) {
@@ -1799,6 +1836,8 @@ SYSCALL_DEFINE5(setsockopt, int, fd, int, level, int, optname,
 out_put:
 		fput_light(sock->file, fput_needed);
 	}
+out:
+	trace_socket_setsockopt(fd, level, optname, optval, optlen, err);
 	return err;
 }
 
@@ -1830,6 +1869,7 @@ SYSCALL_DEFINE5(getsockopt, int, fd, int, level, int, optname,
 out_put:
 		fput_light(sock->file, fput_needed);
 	}
+	trace_socket_getsockopt(fd, level, optname, optval, optlen, err);
 	return err;
 }
 
@@ -1849,6 +1889,7 @@ SYSCALL_DEFINE2(shutdown, int, fd, int, how)
 			err = sock->ops->shutdown(sock, how);
 		fput_light(sock->file, fput_needed);
 	}
+	trace_socket_shutdown(fd, how, err);
 	return err;
 }
 
@@ -2111,6 +2152,8 @@ SYSCALL_DEFINE2(socketcall, int, call, unsigned long __user *, args)
 	a0 = a[0];
 	a1 = a[1];
 
+	trace_socket_call(call, a0);
+
 	switch (call) {
 	case SYS_SOCKET:
 		err = sys_socket(a0, a1, a[2]);
diff --git a/stblinux-2.6.31/samples/Kconfig b/stblinux-2.6.31/samples/Kconfig
index 428b065..a6e10cf 100644
--- a/stblinux-2.6.31/samples/Kconfig
+++ b/stblinux-2.6.31/samples/Kconfig
@@ -46,5 +46,10 @@ config SAMPLE_KRETPROBES
 	default m
 	depends on SAMPLE_KPROBES && KRETPROBES
 
+config SAMPLE_PSRWLOCK
+	tristate "Build psrwlock example -- loadable modules only"
+	default m
+	depends on m
+
 endif # SAMPLES
 
diff --git a/stblinux-2.6.31/samples/Makefile b/stblinux-2.6.31/samples/Makefile
index 13e4b47..129a6d0 100644
--- a/stblinux-2.6.31/samples/Makefile
+++ b/stblinux-2.6.31/samples/Makefile
@@ -1,3 +1,3 @@
 # Makefile for Linux samples code
 
-obj-$(CONFIG_SAMPLES)	+= markers/ kobject/ kprobes/ tracepoints/ trace_events/
+obj-$(CONFIG_SAMPLES)	+= markers/ kobject/ kprobes/ tracepoints/ trace_events/ psrwlock/
diff --git a/stblinux-2.6.31/samples/markers/Makefile b/stblinux-2.6.31/samples/markers/Makefile
index 6d72312..2244152 100644
--- a/stblinux-2.6.31/samples/markers/Makefile
+++ b/stblinux-2.6.31/samples/markers/Makefile
@@ -1,4 +1,4 @@
 # builds the kprobes example kernel modules;
 # then to use one (as root):  insmod <module_name.ko>
 
-obj-$(CONFIG_SAMPLE_MARKERS) += probe-example.o marker-example.o
+obj-$(CONFIG_SAMPLE_MARKERS) += probe-example.o marker-example.o test-multi.o
diff --git a/stblinux-2.6.31/samples/markers/marker-example.c b/stblinux-2.6.31/samples/markers/marker-example.c
index e9cd9c0..06afde4 100644
--- a/stblinux-2.6.31/samples/markers/marker-example.c
+++ b/stblinux-2.6.31/samples/markers/marker-example.c
@@ -19,10 +19,10 @@ static int my_open(struct inode *inode, struct file *file)
 {
 	int i;
 
-	trace_mark(subsystem_event, "integer %d string %s", 123,
+	trace_mark(samples, subsystem_event, "integer %d string %s", 123,
 		"example string");
 	for (i = 0; i < 10; i++)
-		trace_mark(subsystem_eventb, MARK_NOARGS);
+		trace_mark(samples, subsystem_eventb, MARK_NOARGS);
 	return -EPERM;
 }
 
diff --git a/stblinux-2.6.31/samples/markers/probe-example.c b/stblinux-2.6.31/samples/markers/probe-example.c
index 2dfb3b3..2c449c0 100644
--- a/stblinux-2.6.31/samples/markers/probe-example.c
+++ b/stblinux-2.6.31/samples/markers/probe-example.c
@@ -20,7 +20,8 @@ struct probe_data {
 	marker_probe_func *probe_func;
 };
 
-void probe_subsystem_event(void *probe_data, void *call_data,
+static void probe_subsystem_event(const struct marker *mdata,
+	void *probe_data, void *call_data,
 	const char *format, va_list *args)
 {
 	/* Declare args */
@@ -39,7 +40,8 @@ void probe_subsystem_event(void *probe_data, void *call_data,
 
 atomic_t eventb_count = ATOMIC_INIT(0);
 
-void probe_subsystem_eventb(void *probe_data, void *call_data,
+static void probe_subsystem_eventb(const struct marker *mdata,
+	void *probe_data, void *call_data,
 	const char *format, va_list *args)
 {
 	/* Increment counter */
@@ -62,7 +64,7 @@ static int __init probe_init(void)
 	int i;
 
 	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
-		result = marker_probe_register(probe_array[i].name,
+		result = marker_probe_register("samples", probe_array[i].name,
 				probe_array[i].format,
 				probe_array[i].probe_func, &probe_array[i]);
 		if (result)
@@ -77,7 +79,7 @@ static void __exit probe_fini(void)
 	int i;
 
 	for (i = 0; i < ARRAY_SIZE(probe_array); i++)
-		marker_probe_unregister(probe_array[i].name,
+		marker_probe_unregister("samples", probe_array[i].name,
 			probe_array[i].probe_func, &probe_array[i]);
 	printk(KERN_INFO "Number of event b : %u\n",
 			atomic_read(&eventb_count));
diff --git a/stblinux-2.6.31/samples/markers/test-multi.c b/stblinux-2.6.31/samples/markers/test-multi.c
new file mode 100644
index 0000000..9f62d29
--- /dev/null
+++ b/stblinux-2.6.31/samples/markers/test-multi.c
@@ -0,0 +1,116 @@
+/* test-multi.c
+ *
+ * Connects multiple callbacks.
+ *
+ * (C) Copyright 2007 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ *
+ * This file is released under the GPLv2.
+ * See the file COPYING for more details.
+ */
+
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/marker.h>
+#include <asm/atomic.h>
+
+struct probe_data {
+	const char *name;
+	const char *format;
+	marker_probe_func *probe_func;
+};
+
+atomic_t eventb_count = ATOMIC_INIT(0);
+
+void probe_subsystem_eventa(void *probe_data, void *call_data,
+	const char *format, va_list *args)
+{
+	/* Increment counter */
+	atomic_inc(&eventb_count);
+}
+
+void probe_subsystem_eventb(void *probe_data, void *call_data,
+	const char *format, va_list *args)
+{
+	/* Increment counter */
+	atomic_inc(&eventb_count);
+}
+
+void probe_subsystem_eventc(void *probe_data, void *call_data,
+	const char *format, va_list *args)
+{
+	/* Increment counter */
+	atomic_inc(&eventb_count);
+}
+
+void probe_subsystem_eventd(void *probe_data, void *call_data,
+	const char *format, va_list *args)
+{
+	/* Increment counter */
+	atomic_inc(&eventb_count);
+}
+
+static struct probe_data probe_array[] =
+{
+	{	.name = "test_multi",
+		.format = MARK_NOARGS,
+		.probe_func = (marker_probe_func*)0xa },
+	{	.name = "test_multi",
+		.format = MARK_NOARGS,
+		.probe_func = (marker_probe_func*)0xb },
+	{	.name = "test_multi",
+		.format = MARK_NOARGS,
+		.probe_func = (marker_probe_func*)0xc },
+	{	.name = "test_multi",
+		.format = MARK_NOARGS,
+		.probe_func = (marker_probe_func*)0xd },
+	{	.name = "test_multi",
+		.format = MARK_NOARGS,
+		.probe_func = (marker_probe_func*)0x10 },
+	{	.name = "test_multi",
+		.format = MARK_NOARGS,
+		.probe_func = (marker_probe_func*)0x20 },
+	{	.name = "test_multi",
+		.format = MARK_NOARGS,
+		.probe_func = (marker_probe_func*)0x30 },
+};
+
+static int __init probe_init(void)
+{
+	int result;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
+		result = marker_probe_register("samples", probe_array[i].name,
+				probe_array[i].format,
+				probe_array[i].probe_func, (void*)(long)i);
+		if (result)
+			printk(KERN_INFO "Unable to register probe %s\n",
+				probe_array[i].name);
+	}
+	return 0;
+}
+
+static void __exit probe_fini(void)
+{
+	int result;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(probe_array); i++) {
+		result = marker_probe_unregister("samples", probe_array[i].name,
+			probe_array[i].probe_func, (void*)(long)i);
+		if (result)
+			printk(KERN_INFO "Unable to unregister probe %s\n",
+				probe_array[i].name);
+	}
+	printk(KERN_INFO "Number of event b : %u\n",
+		atomic_read(&eventb_count));
+	marker_synchronize_unregister();
+}
+
+module_init(probe_init);
+module_exit(probe_fini);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("SUBSYSTEM Probe");
diff --git a/stblinux-2.6.31/samples/psrwlock/Makefile b/stblinux-2.6.31/samples/psrwlock/Makefile
new file mode 100644
index 0000000..3bdce8d
--- /dev/null
+++ b/stblinux-2.6.31/samples/psrwlock/Makefile
@@ -0,0 +1,4 @@
+# builds the writer-biased rwlock example kernel modules;
+# then to use one (as root):  insmod <module_name.ko>
+
+obj-$(CONFIG_SAMPLE_PSRWLOCK) += psrwlock_example.o
diff --git a/stblinux-2.6.31/samples/psrwlock/psrwlock_example.c b/stblinux-2.6.31/samples/psrwlock/psrwlock_example.c
new file mode 100644
index 0000000..4c3fbba
--- /dev/null
+++ b/stblinux-2.6.31/samples/psrwlock/psrwlock_example.c
@@ -0,0 +1,177 @@
+/*
+ * Priority Sifting Reader-Writer Lock Example
+ *
+ * Copyright 2008 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ */
+
+#include <linux/module.h>
+#include <linux/psrwlock.h>
+
+/*
+ * Define which execution contexts can access the lock in read or write mode.
+ * See psrwlock.h and psrwlock-types.h for details.
+ *
+ * In this example, the writer is in preemptable context and the readers either
+ * in IRQ context, softirq context, non-preemptable context or preemptable
+ * context.
+ */
+#define SAMPLE_ALL_WCTX		PSRW_PRIO_P
+#define SAMPLE_ALL_RCTX		(PSR_IRQ | PSR_BH | PSR_NPTHREAD | PSR_PTHREAD)
+
+static DEFINE_PSRWLOCK(sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+CHECK_PSRWLOCK_MAP(sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+
+/*
+ * Reader in IRQ context.
+ */
+static void executed_in_irq(void)
+{
+	psread_lock_irq(&sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+	/* read structure */
+	psread_unlock(&sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+}
+
+/*
+ * Reader in Softirq context.
+ */
+static void executed_in_bh(void)
+{
+	psread_lock_bh(&sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+	/* read structure */
+	psread_unlock(&sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+}
+
+/*
+ * Reader in non-preemptable context.
+ */
+static void executed_inatomic(void)
+{
+	psread_lock_inatomic(&sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+	/* read structure */
+	psread_unlock(&sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+}
+
+/*
+ * Reader in preemptable context.
+ */
+static void reader_executed_preemptable(void)
+{
+	psread_lock(&sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+	/* read structure */
+	psread_unlock(&sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+}
+
+/*
+ * Writer in preemptable context.
+ */
+static void writer_executed_preemptable(void)
+{
+	pswrite_lock(&sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+	/* read structure */
+	pswrite_unlock(&sample_rwlock, SAMPLE_ALL_WCTX, SAMPLE_ALL_RCTX);
+}
+
+/*
+ * Execute readers in all contexts.
+ */
+static void sample_all_context(void)
+{
+	local_irq_disable();
+	executed_in_irq();
+	local_irq_enable();
+
+	local_bh_disable();
+	executed_in_bh();
+	local_bh_enable();
+
+	preempt_disable();
+	executed_inatomic();
+	preempt_enable();
+
+	reader_executed_preemptable();
+
+	writer_executed_preemptable();
+}
+
+
+/*
+ * In this second example, the writer is in non-preemptable context and the
+ * readers either in IRQ context or softirq context only.
+ */
+static DEFINE_PSRWLOCK(sample_wnp_rbh_rirq_rwlock,
+	PSRW_PRIO_P, PSR_IRQ | PSR_BH);
+CHECK_PSRWLOCK_MAP(sample_wnp_rbh_rirq_rwlock,
+	PSRW_PRIO_P, PSR_IRQ | PSR_BH);
+
+/*
+ * Reader in IRQ context.
+ */
+static void wnp_rbh_rirq_executed_in_irq(void)
+{
+	psread_lock_irq(&sample_wnp_rbh_rirq_rwlock,
+		PSRW_PRIO_P, PSR_IRQ | PSR_BH);
+	/* read structure */
+	psread_unlock(&sample_wnp_rbh_rirq_rwlock,
+		PSRW_PRIO_P, PSR_IRQ | PSR_BH);
+}
+
+/*
+ * Reader in Softirq context.
+ */
+static void wnp_rbh_rirq_executed_in_bh(void)
+{
+	psread_lock_bh(&sample_wnp_rbh_rirq_rwlock,
+		PSRW_PRIO_P, PSR_IRQ | PSR_BH);
+	/* read structure */
+	psread_unlock(&sample_wnp_rbh_rirq_rwlock,
+		PSRW_PRIO_P, PSR_IRQ | PSR_BH);
+}
+
+/*
+ * Writer in preemptable context.
+ */
+static void wnp_rbh_rirq_writer_executed_non_preemptable(void)
+{
+	pswrite_lock(&sample_wnp_rbh_rirq_rwlock,
+				PSRW_PRIO_P, PSR_IRQ | PSR_BH);
+	/* read structure */
+	pswrite_unlock(&sample_wnp_rbh_rirq_rwlock,
+				PSRW_PRIO_P, PSR_IRQ | PSR_BH);
+}
+
+/*
+ * Execute readers in all contexts.
+ */
+static void sample_wnp_rbh_rirq_context(void)
+{
+	local_irq_disable();
+	wnp_rbh_rirq_executed_in_irq();
+	local_irq_enable();
+
+	local_bh_disable();
+	wnp_rbh_rirq_executed_in_bh();
+	local_bh_enable();
+
+	preempt_disable();
+	wnp_rbh_rirq_writer_executed_non_preemptable();
+	preempt_enable();
+}
+
+static int __init init_example(void)
+{
+	sample_all_context();
+	sample_wnp_rbh_rirq_context();
+
+	return 0;
+}
+
+static void __exit exit_example(void)
+{
+}
+
+module_init(init_example)
+module_exit(exit_example)
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mathieu Desnoyers");
+MODULE_DESCRIPTION("psrwlock example");
diff --git a/stblinux-2.6.31/scripts/mod/modpost.c b/stblinux-2.6.31/scripts/mod/modpost.c
index 4522948..d1a17b4 100644
--- a/stblinux-2.6.31/scripts/mod/modpost.c
+++ b/stblinux-2.6.31/scripts/mod/modpost.c
@@ -736,6 +736,7 @@ static const char *section_white_list[] =
 	".note*",
 	".got*",
 	".toc*",
+	"__discard",
 	NULL
 };
 
diff --git a/stblinux-2.6.31/virt/kvm/kvm_trace.c b/stblinux-2.6.31/virt/kvm/kvm_trace.c
index f598744..e360791 100644
--- a/stblinux-2.6.31/virt/kvm/kvm_trace.c
+++ b/stblinux-2.6.31/virt/kvm/kvm_trace.c
@@ -48,7 +48,8 @@ static inline int calc_rec_size(int timestamp, int extra)
 	return timestamp ? rec_size += KVM_TRC_CYCLE_SIZE : rec_size;
 }
 
-static void kvm_add_trace(void *probe_private, void *call_data,
+static void kvm_add_trace(const struct marker *mdata,
+			  void *probe_private, void *call_private,
 			  const char *format, va_list *args)
 {
 	struct kvm_trace_probe *p = probe_private;
@@ -88,8 +89,8 @@ static void kvm_add_trace(void *probe_private, void *call_data,
 }
 
 static struct kvm_trace_probe kvm_trace_probes[] = {
-	{ "kvm_trace_entryexit", "%u %p %u %u %u %u %u %u", 1, kvm_add_trace },
-	{ "kvm_trace_handler", "%u %p %u %u %u %u %u %u", 0, kvm_add_trace },
+	{ "trace_entryexit", "%u %p %u %u %u %u %u %u", 1, kvm_add_trace },
+	{ "trace_handler", "%u %p %u %u %u %u %u %u", 0, kvm_add_trace },
 };
 
 static int lost_records_get(void *data, u64 *val)
@@ -182,7 +183,8 @@ static int do_kvm_trace_enable(struct kvm_user_trace_setup *kuts)
 	for (i = 0; i < ARRAY_SIZE(kvm_trace_probes); i++) {
 		struct kvm_trace_probe *p = &kvm_trace_probes[i];
 
-		r = marker_probe_register(p->name, p->format, p->probe_func, p);
+		r = marker_probe_register("kvm", p->name, p->format,
+					  p->probe_func, p);
 		if (r)
 			printk(KERN_INFO "Unable to register probe %s\n",
 			       p->name);
@@ -250,7 +252,7 @@ void kvm_trace_cleanup(void)
 
 		for (i = 0; i < ARRAY_SIZE(kvm_trace_probes); i++) {
 			struct kvm_trace_probe *p = &kvm_trace_probes[i];
-			marker_probe_unregister(p->name, p->probe_func, p);
+			marker_probe_unregister("kvm", p->name, p->probe_func, p);
 		}
 		marker_synchronize_unregister();
 
